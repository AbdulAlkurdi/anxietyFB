{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8af7f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20604081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Link: https://github.com/WJMatthew/WESAD/blob/master/data_wrangling.py\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import heartpy as hp\n",
    "import biosppy\n",
    "import neurokit2 as nk\n",
    "from heartpy.datautils import *\n",
    "from heartpy.peakdetection import *\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "import csv \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed758cf9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b4bf2",
   "metadata": {},
   "source": [
    "Features coded:\n",
    "\n",
    "ECG: mean, std, min, max, bpm, ibi, sdnn, sdsd, rmssd, pnn20, pnn50, \n",
    "\n",
    "\n",
    "PPG/BVP: mean, std, min, max, peak_freq\n",
    "\n",
    "TEMP:mean, std, min, max, drange, slope\n",
    "\n",
    "RESP: mean, std, min, max, rate; Inh: mean, std; Exh: mean, std, I/E\n",
    "\n",
    "\n",
    "EDA: mean, std, min, max, slipe, drange; SCR: mean, std, min, max; SCL: mean, std, min, max\n",
    "\n",
    "ACC x,y,z; chest, wrist:  mean, std, min, max, abs_integral, peak_freq\n",
    "Acc net: mean, std, min, max, abs_integral, peak_freq\n",
    "\n",
    "\n",
    "Features not coded yet:\n",
    "\n",
    "\n",
    "To replicate this study with similar modalities to RADWear, I will drop the following signals: EDA_c, EMG, TEMP_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24314e2f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cd49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700, 'ECG': 700, \n",
    "           'chest': 700}\n",
    "# Window size\n",
    "WINDOW_IN_SECONDS = 60\n",
    "stride = 0.25\n",
    "\n",
    "# Labels\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "# Int to label mappings\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "# Feature names\n",
    "feat_names = None\n",
    "# Where to save the data\n",
    "savePath = 'data/WESAD'\n",
    "# Where to get the data\n",
    "subject_feature_path = '/subject_feats'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c12d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "os.listdir(savePath)\n",
    "df = pd.read_csv(savePath +'/oct5_feats4.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8c968",
   "metadata": {},
   "source": [
    "We want to drop columns in df that are not in RADWear to match modalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop _c columns\n",
    "columns_list = df.columns.tolist()\n",
    "drop_list = []\n",
    "#df.drop(columns=['Resp_C'])\n",
    "for column in columns_list:\n",
    "    if 'EMG' in column or 'EDA_C' in column or 'Temp_C' in column or 'TEMP_C' in column or 'SCR_C' in column or 'SCL_C' in column:\n",
    "        drop_list.append(column)\n",
    "\n",
    "reduced_df = df.drop(columns=drop_list)\n",
    "df = reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47677b",
   "metadata": {},
   "source": [
    "## Generate correlation between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(savePath +'/oct5_feats4.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = abs(df.corr()['label']).sort_values(ascending=False)\n",
    "if False:\n",
    "    for i in range(len(vals)):\n",
    "        print(vals.index[i], vals[i])\n",
    "\n",
    "corr = df.corr()\n",
    "plot_corr = True\n",
    "if plot_corr:\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.heatmap(corr,xticklabels=True, yticklabels=True,square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[:, df.columns != 'Resp_C_rate'] #I don't know why this is here. \n",
    "\n",
    "features = df.loc[:, df.columns != 'label'].columns\n",
    "print_feats_list = False\n",
    "if print_feats_list:\n",
    "    for ft_idx in range(len(features)):\n",
    "        print(features[ft_idx], ft_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2293566",
   "metadata": {},
   "source": [
    "## split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6898be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ee52",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0b4a2",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f7df5",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10675fc1",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e911602",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3591f",
   "metadata": {},
   "source": [
    "The models will be included in this study are: \n",
    "\n",
    "DT, RF, SVM, AB, LDA and kNN. \n",
    "\n",
    "Completed: LDA, RF, SVM, AB, DT, kNN \n",
    "\n",
    "Incomplete: NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_model_list = ['DT', 'RF', 'SVM', 'LDA', 'KNN', 'AdaBoost']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9b03e",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cf325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LDA(X_train, X_test, y_train, y_test):\n",
    "    sc = StandardScaler()  \n",
    "    X_train = sc.fit_transform(X_train)  \n",
    "    X_test = sc.transform(X_test)  \n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)  \n",
    "    print(cm)  \n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "    lda_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    return lda_baseline_acc\n",
    "lda_baseline_acc = run_LDA(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0637984",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58bc21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF(X_train, X_test, y_train, y_test, max_depth=4, random_state=0):\n",
    "    classifier = RandomForestClassifier(max_depth=max_depth, random_state=random_state)\n",
    "    classifier.fit(X_train, y_train)  \n",
    "    y_pred = classifier.predict(X_test)  \n",
    "    cm = confusion_matrix(y_test, y_pred)  \n",
    "    print(cm)  \n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "    rf_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    importances = classifier.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "    return rf_baseline_acc, forest_importances\n",
    "rf_baseline_acc , forest_importances= run_RF(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44af091",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d21bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af563",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5e3e0",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm( X_train, X_test, y_train, y_test, C=1, random_state=0, kernel='linear'):\n",
    "    # Create a SVC classifier using a linear kernel\n",
    "    clf = SVC(kernel=kernel, C=C, random_state=random_state)\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    lm_svc=(classification_report(y_test, y_pred, digits=4))\n",
    "    print(lm_svc)\n",
    "    svm_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    pd.Series(abs(clf.coef_[0]), index=features).nlargest(10).plot(kind='barh') # Feature Importance (Top 20)\n",
    "    return svm_baseline_acc\n",
    "svm_baseline_acc = run_svm(X_train, X_test, y_train, y_test)\n",
    "svm2_baseline_acc = run_svm(X_train, X_test, y_train, y_test, C=0.9) # linear svm lambda 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434fc1f",
   "metadata": {},
   "source": [
    "## Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd38445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ab( X_train, X_test, y_train, y_test, n_estimators=100, random_state=0):\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    ab_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    ab2_baseline_acc = clf.score(X_test, y_test)\n",
    "    ab_imp = clf.feature_importances_\n",
    "    ab_imp = pd.Series(ab_imp, index=features).sort_values(ascending=False)\n",
    "    # plot importances\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ab_imp[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    return ab_baseline_acc, ab2_baseline_acc\n",
    "ab_baseline_acc, ab2_baseline_acc = run_ab(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f305",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dt(X_train, X_test, y_train, y_test):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    dt_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # plot importances\n",
    "    dt_importances = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dt_importances[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return dt_baseline_acc\n",
    "dt_baseline_acc = run_dt(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print('decision tree baseline accuracy: ' + str(dt_baseline_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b373f",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c30a330",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn(X_train, X_test, y_train, y_test, n_neighbors=3):\n",
    "    nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    #knn.fit(X_train, y_train)\n",
    "    #y_pred = knn.predict(X_test)\n",
    "    #knn_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    ####################\n",
    "    #  Feature importance cannot be discovered for the kNN model. \n",
    "    #####################\n",
    "    #y_pred = knn.predict(X_test)\n",
    "    #knn_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "    nca_pipe.fit(X_train, y_train)\n",
    "    knn_baseline_acc = nca_pipe.score(X_test, y_test)\n",
    "    #print('knn baseline accuracy: ' + str(knn_baseline_acc))\n",
    "    return knn_baseline_acc\n",
    "knn_baseline_acc = run_knn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b3084",
   "metadata": {},
   "source": [
    "# Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737390",
   "metadata": {},
   "source": [
    "## Signal to Noise Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099bbe0",
   "metadata": {},
   "source": [
    "For a non-constant signal $S$ and noise $N$, the signal to noise ratio is defined as the following:\n",
    "$$ SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70b232",
   "metadata": {},
   "source": [
    "The expected value $\\mathbb{E}[X]$ of any continuous random variable $X$ is $\\int_{-\\infty}^{\\infty} x p(x) dx $, where $p(x)$ is its associated probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d01d",
   "metadata": {},
   "source": [
    "For homoskedastic noise, we can use closed form expressions to compute $E[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$.\n",
    "\n",
    "- For uniformly distributed noise $N$ ~ $u(\\alpha, \\beta)$, by the same logic as above $\\mathbb{E}[N^2] = \\left(\\frac{\\alpha - \\beta}{2}\\right)^2$.\n",
    "\n",
    "- For frequency-domain noise $N$ of the form $A\\sin(2\\pi x \\frac{1}{f}) + y, \\mathbb{E}[N^2] \\approx y^2 + \\frac{A^2}{2}$. Note the $\\approx$ since we cannot guarantee that the signal will end precisely on the end of the sin wave.\n",
    "\n",
    "For heteroskedastic noise, because there is no closed form expression, we simply take `N.mean()` where $N$ is our noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_noise_ratio(signal, noise_type, noise_dist, noise=None, sigma=None, alpha=None, beta=None, \n",
    "                          vertical_shift=None, amplitude=None):\n",
    "    \"\"\"\n",
    "    Function: Computes the signal to noise ratio for a given signal and its corresponding noise.\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we are evaluating\n",
    "        noise_type (string): 'Heteroskedastic' or 'Homoskedastic'\n",
    "        noise_dist (string): 'Uniform', 'Gaussian', or 'Frequency' for now\n",
    "        noise (array or ndarray): Only passed in if we have heteroskedastic noise\n",
    "        sigma (float): Sigma parameter of the gaussian\n",
    "        alpha (float): Alpha parameter of the uniform\n",
    "        beta (float): Beta parameter of the uniform\n",
    "        vertical_shift (float): Vertical shift parameter of the frequency\n",
    "        amplitude (float): Amplitude parameter of the frequency\n",
    "        \n",
    "    :return\n",
    "        signal_to_noise_ratio (float): Signal to noise ratio... E[S^2]/E[N^2]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate E[S^2]\n",
    "    e_s2 = (signal**2).mean()\n",
    "    e_n2 = None\n",
    "    \n",
    "    if noise_type == 'Homoskedastic':\n",
    "        # Calculate E[N^2] for the pertinent case\n",
    "        if noise_dist == 'Uniform':\n",
    "            e_n2 = (0.5*(alpha - beta))**2\n",
    "        elif noise_dist == 'Gaussian':\n",
    "            e_n2 = sigma**2\n",
    "        elif noise_dist == 'Frequency':\n",
    "            e_n2 = vertical_shift**2 + (amplitude**2)/2\n",
    "    elif noise_type == 'Heteroskedastic':\n",
    "        e_n2 = (noise**2).mean()\n",
    "    \n",
    "    # Return the signal to noise ratio\n",
    "    return e_s2/e_n2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0cc25",
   "metadata": {},
   "source": [
    "## Denoising Using Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92c4d9",
   "metadata": {},
   "source": [
    "Let $X$ represent our set of physiological signals and $\\textbf{x}_i$ denote the *i*-th column of $X$. In our case, $\\textbf{x}_i$ is one of the ECG, BVP, EDA, ACC, etc. The post-noise signal we observe $\\textbf{x}_i = \\widetilde{\\textbf{x}}_i + \\xi_i$ is composed of the original raw signal and Gaussian distributed noise with $\\mathbb{E}[\\xi] = 0$ and $V[\\xi] = E[N^2] = \\sigma^2 = \\frac{\\mathbb{E}[S^2]}{SNR}$. Literature has indicated that a principal component analysis of $\\textbf{x}_i$ can produce an estimate of $\\widetilde{\\textbf{x}}_i$ that is closer than the noisy measurements are (citation needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542fdb4",
   "metadata": {},
   "source": [
    "## Calculate Distribution Parameters from SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb7d2c",
   "metadata": {},
   "source": [
    "Given a signal $S$, we can specify a signal to noise ratio $SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]}$ and use this to calculate $\\mathbb{E}[N^2]$ because $SNR$ and $\\mathbb{E}[S^2]$ are known. So $\\mathbb{E}[N^2] = \\frac{\\mathbb{E}[S^2]}{SNR}$.\n",
    "\n",
    "Then, for any homoskedastic noise following a well-defined probability density function (PDF), we can solve for the parameters of the PDF using the known value $\\mathbb{E}[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$. Thus, $\\sigma^2 = \\frac{\\mathbb{E}[S^2]}{SNR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42057019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_param(signal, noise_type, signal_to_noise_ratio):\n",
    "    \"\"\"\n",
    "    Function: Calculates the parameters of a continuous probability density function given\n",
    "    our desired signal to noise ratio and signal.\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): Our signal\n",
    "        noise_type (string): Probability distribution of our noise (i.e., Gaussian)\n",
    "        signal_to_noise_ratio (float): Desired signal to noise ratio\n",
    "    \n",
    "    :return\n",
    "    (for now, just)\n",
    "        sigma (float): sigma of the gaussian\n",
    "    \"\"\"\n",
    "    \n",
    "    return (signal**2).mean()/signal_to_noise_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819be01",
   "metadata": {},
   "source": [
    "## Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99319416",
   "metadata": {},
   "source": [
    "The Gaussian probability density function is of the following form:\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4198cb0",
   "metadata": {},
   "source": [
    "### Estimating $\\mu$ and $\\sigma$ of the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330a231",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential with Homoskedasticity Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4116e",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with homoskedasticity approach constructs a Gaussian distribution such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_i - S_{i+n}|)$, where $max(|S_i - S_{i+n}|)$ denotes the maximum absolute difference of the signal between index $i$ and $i+n$ in the entire signal, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "In conclusion, we randomly sample from the following probability density function:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\alpha \\cdot max(|S_i - S_{i+n}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_i - S_{i+n}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "This noise exhbits homoskedasticity because it does not vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_homoskedastic(signal_name, signal, signal_to_noise_ratio=None):\n",
    "    \"\"\"\n",
    "    Constructs a homoskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal_name (string): The name of the signal (i.e., ECG)\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        signal_to_noise_ratio (float): [default: None] If specified, our desired SNR.\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    x_new = None\n",
    "\n",
    "    if signal_name == 'ACC':\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        # Noise X Axis\n",
    "        x_axis = signal[:,0]\n",
    "        sigma = calculate_param(x_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        x_axis_new = np.copy(x_axis)\n",
    "        for i in range(len(x_axis_new)):\n",
    "            x_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Y Axis\n",
    "        y_axis = signal[:,1]\n",
    "        sigma = calculate_param(y_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        y_axis_new = np.copy(y_axis)\n",
    "        for i in range(len(y_axis_new)):\n",
    "            y_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Z Axis\n",
    "        z_axis = signal[:,2]\n",
    "        sigma = calculate_param(z_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        z_axis_new = np.copy(z_axis)\n",
    "        for i in range(len(z_axis_new)):\n",
    "            z_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "\n",
    "        # Put together noisy signal\n",
    "        x_new = np.zeros((len(signal), 3))\n",
    "        x_new[:,0] = x_axis_new\n",
    "        x_new[:,1] = y_axis_new\n",
    "        x_new[:,2] = z_axis_new\n",
    "\n",
    "        return (x_new, sigma)\n",
    "    else: \n",
    "        # Store original shape\n",
    "        original_shape = signal.shape\n",
    "\n",
    "        # Caveat: some signals like ACC have three axes\n",
    "        # Flatten signal to be 1d\n",
    "        x = np.ravel(signal)\n",
    "\n",
    "        # Calculate mean and Standard deviation\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        sigma = calculate_param(x, 'Gaussian', signal_to_noise_ratio)\n",
    "\n",
    "        # test that this works\n",
    "        x_new = x + np.random.normal(mu, sigma, (len(x),))\n",
    "\n",
    "#         for i in range(len(x_new)):\n",
    "#             x_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "\n",
    "        return (np.array(x_new).reshape(original_shape), sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_new_noise1 = []\n",
    "patients_new_noise2 = []\n",
    "patients_new_noise3 = []\n",
    "patients_new_noise4 = []\n",
    "patients_new_noise5 = []\n",
    "# Iterate through each patient's folder and construct a\n",
    "# df with all patient data\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        # If this is a .pkl file then it's the synchronized features/labels\n",
    "        # and we want to serialize the file\n",
    "        if '.pkl' in file and 'gauss' in file and 'snr_0.01' in file:\n",
    "            patients_new_noise1.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.05' in file:\n",
    "            patients_new_noise2.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.1' in file:\n",
    "            patients_new_noise3.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.2' in file:\n",
    "            patients_new_noise4.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.5' in file:\n",
    "            patients_new_noise5.append(pd.read_pickle(subdir + '/' + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5a055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(16,4))\n",
    "axs[0].plot(np.ravel(patients_new_noise1[0]['signal']['chest']['Resp']))\n",
    "axs[0].set_title('SNR: 0.01')\n",
    "axs[1].plot(np.ravel(patients_new_noise2[0]['signal']['chest']['Resp']))\n",
    "axs[1].set_title('SNR: 0.05')\n",
    "axs[2].plot(np.ravel(patients_new_noise3[0]['signal']['chest']['Resp']))\n",
    "axs[2].set_title('SNR: 0.1')\n",
    "axs[3].plot(np.ravel(patients_new_noise4[0]['signal']['chest']['Resp']))\n",
    "axs[3].set_title('SNR: 0.2')\n",
    "axs[4].plot(np.ravel(patients_new_noise5[0]['signal']['chest']['Resp']))\n",
    "axs[4].set_title('SNR: 0.5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022334",
   "metadata": {},
   "source": [
    "# Data Preparation (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce48d1c",
   "metadata": {},
   "source": [
    "Prepare the data again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snrs = [0.001, 0.15, 0.3, 0.4, 0.6] # this is the starting list\n",
    "#snrs = [0.00001, 0.0001,  0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6] # this is biggest list we'd need\n",
    "snrs = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 0.6] # this is what we ran\n",
    "n_samples = 10 # number of samples taken per SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rparser_with_noise:\n",
    "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
    "    VALUE_EXTRACT_KEYS = {\n",
    "        \"age\": {\n",
    "            'search_key': 'Age',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"height\": {\n",
    "            'search_key': 'Height',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"weight\": {\n",
    "            'search_key': 'Weight',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"gender\": {\n",
    "            'search_key': 'Gender',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"dominant_hand\": {\n",
    "            'search_key': 'Dominant',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"coffee_today\": {\n",
    "            'search_key': 'Did you drink coffee today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"coffee_last_hour\": {\n",
    "            'search_key': 'Did you drink coffee within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"sport_today\": {\n",
    "            'search_key': 'Did you do any sports today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoker\": {\n",
    "            'search_key': 'Are you a smoker',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoke_last_hour\": {\n",
    "            'search_key': 'Did you smoke within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"feel_ill_today\": {\n",
    "            'search_key': 'Do you feel ill today',\n",
    "            'delimiter': '? '\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DATA_PATH = 'data/WESAD/'\n",
    "    parse_file_suffix = '_readme.txt'\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
    "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
    "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
    "        \n",
    "        # Check if parsed readme file is available ( should be as it is saved above )\n",
    "        if not os.path.isfile('data/readmes.csv'):\n",
    "            print('Parsing Readme files')\n",
    "            self.parse_all_readmes()\n",
    "        else:\n",
    "            print('Files already parsed.')\n",
    "            \n",
    "        self.merge_with_feature_data_with_noise()\n",
    "        \n",
    "        \n",
    "    def parse_readme(self, subject_id):\n",
    "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
    "\n",
    "            x = f.read().split('\\n')\n",
    "\n",
    "        readme_dict = {}\n",
    "\n",
    "        for item in x:\n",
    "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
    "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
    "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
    "                if item.startswith(search_key):\n",
    "                    d, v = item.split(delimiter)\n",
    "                    readme_dict.update({key: v})\n",
    "                    break\n",
    "        return readme_dict\n",
    "\n",
    "\n",
    "    def parse_all_readmes(self):\n",
    "        \n",
    "        dframes = []\n",
    "\n",
    "        for subject_id, path in self.readme_locations.items():\n",
    "            readme_dict = self.parse_readme(subject_id)\n",
    "            df = pd.DataFrame(readme_dict, index=[subject_id])\n",
    "            dframes.append(df)\n",
    "\n",
    "        df = pd.concat(dframes)\n",
    "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
    "\n",
    "        \n",
    "    def merge_with_feature_data_with_noise(self):\n",
    "        # Confirm feature files are available\n",
    "        if os.path.isfile('data/may14_feats4_with_noise.csv'):\n",
    "            feat_df = pd.read_csv('data/may14_feats4_with_noise.csv', index_col=0)\n",
    "            print(feat_df.info())\n",
    "        else:\n",
    "            print('No feature data available. Exiting...')\n",
    "            return\n",
    "           \n",
    "        # Combine data and save\n",
    "        df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
    "\n",
    "        dummy_df = pd.get_dummies(df)\n",
    "        \n",
    "        dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
    "\n",
    "        dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
    "                           'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
    "                           'feel_ill_today_YES', 'subject']]\n",
    "\n",
    "        merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
    "\n",
    "        merged_df.to_csv('data/noise_snr_0.6.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_with_noise = rparser_with_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb3726",
   "metadata": {},
   "source": [
    "# Modeling (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99893b",
   "metadata": {},
   "source": [
    "Model again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f997c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/noise_snr_0.15.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.isna().sum()\n",
    "for i in range(len(s)):\n",
    "    if s[i] > 0:\n",
    "        print(s.index[i], s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns != 'Resp_C_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:, df.columns != 'label'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6559b2c",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63bf21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8aa91f",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = classifier.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d44d3",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973d694",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4551",
   "metadata": {},
   "source": [
    "Compare the results of the noisy data models and the clean data models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb53c5",
   "metadata": {},
   "source": [
    "- Plots of SNR (x-axis) vs. accuracy (y-axis)\n",
    "- Compare feature importances across different noise regimes\n",
    "    - Develop dynamic evaluation method based on original feature importance / added noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0de9",
   "metadata": {},
   "source": [
    "## Test Each Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10e7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snrs = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "lda_accuracy = []\n",
    "rf_accuracy = []\n",
    "svm_accuracy = []\n",
    "ft_imp_matrix = []\n",
    "# For each signal to noise ratio\n",
    "for i in range(len(snrs)):\n",
    "    # Get data\n",
    "    df = pd.read_csv('data/noise_snr_'+str(snrs[i])+'.csv', index_col=0)\n",
    "    # Since Resp_C_rate is null for the first four, simply get rid of it\n",
    "    df = df.loc[:, df.columns != 'Resp_C_rate']\n",
    "    \n",
    "    # Get features, label\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values\n",
    "\n",
    "    # Get train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "    \n",
    "    # Scale the data\n",
    "    sc = StandardScaler()  \n",
    "    X_train = sc.fit_transform(X_train)  \n",
    "    X_test = sc.transform(X_test)  \n",
    "    \n",
    "    # Test LDA\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    lda_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Test RF\n",
    "    classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "    classifier.fit(X_train, y_train)  \n",
    "    y_pred = classifier.predict(X_test)  \n",
    "    rf_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    importances = classifier.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "    ft_imp_matrix.append(importances)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    forest_importances[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI with SNR: \" + str(snrs[i]))\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "    # Test SVM\n",
    "    clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_out = clf.predict(X_test)    \n",
    "    svm_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # test kNN\n",
    "\n",
    "    # test DT\n",
    "\n",
    "    # test AdaBoost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "for ft in range(0, len(ft_imp_matrix)):\n",
    "    ft_imp_matrix[ft] = [sorted(ft_imp_matrix[ft], reverse=True).index(x) for x in ft_imp_matrix[ft]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ebc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df = pd.DataFrame(ft_imp_matrix, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d157b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.reset_index().pivot(index=ft_imp_df, columns=ft_imp_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WESAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WESAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SNR Accuracy F1-Score dataset\n",
       "SVM   1        5      NaN   WESAD\n",
       "RF    1        5      NaN   WESAD"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_table = pd.DataFrame(columns=['SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "results_table.loc[str('SVM')] = pd.Series({'SNR':1, 'Accuracy':5, 'F1 Score':2, 'dataset':'WESAD'})\n",
    "results_table.loc[str('RF')] = pd.Series({'SNR':1, 'Accuracy':5, 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "fb_model_list = ['SVM', 'RF']\n",
    "for model in fb_model_list:\n",
    "    for i in range(len(snrs)):\n",
    "        results_table.loc[str(model) + str(snrs[i])] = pd.Series({'SNR':snrs[i], 'Accuracy':svm_accuracy[i], 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "display(results_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742139d",
   "metadata": {},
   "source": [
    "## Plot SNR vs. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,7))\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "min_accuracy = min(min(lda_accuracy), min(rf_accuracy), min(svm_accuracy)) - 0.01\n",
    "max_accuracy = max(lda_baseline_acc, rf_baseline_acc, svm_baseline_acc) + 0.01\n",
    "\n",
    "axs[0].plot(snrs, lda_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[0].plot(snrs, [lda_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[0].set_title('Linear Discriminant Analysis', fontsize=20);\n",
    "axs[0].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[0].set_ylabel('Classification Accuracy', fontsize=15);\n",
    "axs[0].legend();\n",
    "axs[0].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[1].plot(snrs, rf_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[1].plot(snrs, [rf_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[1].set_title('Random Forest', fontsize=20);\n",
    "axs[1].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[1].legend();\n",
    "axs[1].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[2].plot(snrs, svm_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[2].plot(snrs, [svm_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[2].set_title('Support Vector Machine', fontsize=20);\n",
    "axs[2].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[2].legend();\n",
    "axs[2].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "# add knn, dt and adaboost\n",
    "\n",
    "axs[3].plot(snrs, knn_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[3].plot(snrs, [knn_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[3].set_title('kNN', fontsize=20);\n",
    "axs[3].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[3].legend();\n",
    "axs[3].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[4].plot(snrs, dt_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[4].plot(snrs, [dt_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[4].set_title('Decision Tree', fontsize=20);\n",
    "axs[4].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[4].legend();\n",
    "axs[4].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[5].plot(snrs, adaboost_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[5].plot(snrs, [adaboost_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[5].set_title('AdaBoost', fontsize=20);\n",
    "axs[5].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[5].legend();\n",
    "axs[5].set_ylim([min_accuracy, max_accuracy]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Feature extraction for ecg failed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.warning(\"Feature extraction for ecg failed. \\n This happened for participant \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1055px",
    "left": "216px",
    "top": "111.12px",
    "width": "343.299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
