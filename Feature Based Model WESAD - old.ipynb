{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8af7f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20604081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Link: https://github.com/WJMatthew/WESAD/blob/master/data_wrangling.py\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import heartpy as hp\n",
    "import biosppy\n",
    "import neurokit2 as nk\n",
    "from heartpy.datautils import *\n",
    "from heartpy.peakdetection import *\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "import csv \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed758cf9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b4bf2",
   "metadata": {},
   "source": [
    "Features coded:\n",
    "\n",
    "ECG: mean, std, min, max, bpm, ibi, sdnn, sdsd, rmssd, pnn20, pnn50, \n",
    "\n",
    "\n",
    "PPG/BVP: mean, std, min, max, peak_freq\n",
    "\n",
    "TEMP:mean, std, min, max, drange, slope\n",
    "\n",
    "RESP: mean, std, min, max, rate; Inh: mean, std; Exh: mean, std, I/E\n",
    "\n",
    "\n",
    "EDA: mean, std, min, max, slipe, drange; SCR: mean, std, min, max; SCL: mean, std, min, max\n",
    "\n",
    "ACC x,y,z; chest, wrist:  mean, std, min, max, abs_integral, peak_freq\n",
    "Acc net: mean, std, min, max, abs_integral, peak_freq\n",
    "\n",
    "\n",
    "Features not coded yet:\n",
    "\n",
    "\n",
    "To replicate this study with similar modalities to RADWear, I will drop the following signals: EDA_c, EMG, TEMP_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24314e2f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cd49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700, 'ECG': 700, \n",
    "           'chest': 700}\n",
    "# Window size\n",
    "WINDOW_IN_SECONDS = 60\n",
    "stride = 1\n",
    "\n",
    "# Labels\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "# Int to label mappings\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "# Feature names\n",
    "feat_names = None\n",
    "# Where to save the data\n",
    "savePath = 'data/WESAD'\n",
    "loadPath_GN = 'data/GN-WESAD'\n",
    "# Where to get the data\n",
    "subject_feature_path = '/subject_feats'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c12d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "os.listdir(savePath)\n",
    "df = pd.read_csv(savePath +'/oct5_feats4.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8c968",
   "metadata": {},
   "source": [
    "We want to drop columns in df that are not in RADWear to match modalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a56690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop _c columns\n",
    "columns_list = df.columns.tolist()\n",
    "drop_list = []\n",
    "#df.drop(columns=['Resp_C'])\n",
    "for column in columns_list:\n",
    "    if 'EMG' in column or 'EDA_C' in column or 'Temp_C' in column or 'TEMP_C' in column or 'SCR_C' in column or 'SCL_C' in column:\n",
    "        drop_list.append(column)\n",
    "\n",
    "reduced_df = df.drop(columns=drop_list)\n",
    "df = reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47677b",
   "metadata": {},
   "source": [
    "## Generate correlation between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2dd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(savePath +'/oct5_feats4.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['net_acc_C_mean', 'net_acc_C_std', 'net_acc_C_min', 'net_acc_C_max',\n",
       "       'net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max', 'EDA_mean',\n",
       "       'EDA_std',\n",
       "       ...\n",
       "       'Resp_C_Inhal_std', 'Resp_C_Exhal_mean', 'Resp_C_Exhal_std',\n",
       "       'Resp_C_I/E', 'TEMP_drange', 'TEMP_C_drange', 'TEMP_slope',\n",
       "       'TEMP_C_slope', 'subject', 'label'],\n",
       "      dtype='object', length=120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = abs(df.corr()['label']).sort_values(ascending=False)\n",
    "if False:\n",
    "    for i in range(len(vals)):\n",
    "        print(vals.index[i], vals[i])\n",
    "\n",
    "corr = df.corr()\n",
    "plot_corr = True\n",
    "if plot_corr:\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.heatmap(corr,xticklabels=True, yticklabels=True,square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[:, df.columns != 'Resp_C_rate'] #I don't know why this is here. \n",
    "\n",
    "features = df.loc[:, df.columns != 'label'].columns\n",
    "print_feats_list = False\n",
    "if print_feats_list:\n",
    "    for ft_idx in range(len(features)):\n",
    "        print(features[ft_idx], ft_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2293566",
   "metadata": {},
   "source": [
    "## split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6898be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ee52",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0b4a2",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f7df5",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10675fc1",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e911602",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3591f",
   "metadata": {},
   "source": [
    "The models will be included in this study are: \n",
    "\n",
    "DT, RF, SVM, AB, LDA and kNN. \n",
    "\n",
    "Completed: LDA, RF, SVM, AB, DT, kNN \n",
    "\n",
    "Incomplete: NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_model_list = ['DT', 'RF', 'SVM', 'LDA', 'KNN', 'AdaBoost']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9b03e",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "471cf325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 11  1]\n",
      " [ 4 63  1]\n",
      " [ 0  0 28]]\n",
      "Accuracy: 0.853448275862069\n",
      "[[ 8 11  1]\n",
      " [ 4 63  1]\n",
      " [ 0  0 28]]\n",
      "Accuracy: 0.853448275862069\n",
      "lda accuracy:  0.853448275862069\n",
      "shit lda accuracy:  0.853448275862069\n"
     ]
    }
   ],
   "source": [
    "def run_LDA(X_train, X_test, y_train, y_test):\n",
    "    sc = StandardScaler()  \n",
    "    X_train = sc.fit_transform(X_train)  \n",
    "    X_test = sc.transform(X_test)  \n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)  \n",
    "    print(cm)  \n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "    lda_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    return lda_baseline_acc\n",
    "\n",
    "\n",
    "lda_baseline_acc = run_LDA(X_train, X_test, y_train, y_test)\n",
    "print('lda accuracy: ', lda_baseline_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0637984",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58bc21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2410057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  6  1]\n",
      " [ 0 67  1]\n",
      " [ 0  0 28]]\n",
      "Accuracy: 0.9310344827586207\n",
      "[[13  6  1]\n",
      " [ 0 67  1]\n",
      " [ 0  0 28]]\n",
      "Accuracy: 0.9310344827586207\n",
      "rf accuracy:  0.9310344827586207\n",
      "rf accuracy:  0.9310344827586207\n"
     ]
    }
   ],
   "source": [
    "def run_RF(X_train, X_test, y_train, y_test, max_depth=4, random_state=0):\n",
    "    classifier = RandomForestClassifier(max_depth=max_depth, random_state=random_state)\n",
    "    classifier.fit(X_train, y_train)  \n",
    "    y_pred = classifier.predict(X_test)  \n",
    "    cm = confusion_matrix(y_test, y_pred)  \n",
    "    print(cm)  \n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "    rf_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    importances = classifier.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "    return rf_baseline_acc, forest_importances\n",
    "rf_baseline_acc , forest_importances= run_RF(X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44af091",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d21bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af563",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5e3e0",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm( X_train, X_test, y_train, y_test, C=1, random_state=0, kernel='linear'):\n",
    "    # Create a SVC classifier using a linear kernel\n",
    "    clf = SVC(kernel=kernel, C=C, random_state=random_state)\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    lm_svc=(classification_report(y_test, y_pred, digits=4))\n",
    "    print(lm_svc)\n",
    "    svm_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    pd.Series(abs(clf.coef_[0]), index=features).nlargest(10).plot(kind='barh') # Feature Importance (Top 20)\n",
    "    return svm_baseline_acc\n",
    "svm_baseline_acc = run_svm(X_train, X_test, y_train, y_test)\n",
    "svm2_baseline_acc = run_svm(X_train, X_test, y_train, y_test, C=0.9) # linear svm lambda 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434fc1f",
   "metadata": {},
   "source": [
    "## Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd38445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ab( X_train, X_test, y_train, y_test, n_estimators=100, random_state=0):\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    ab_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    ab2_baseline_acc = clf.score(X_test, y_test)\n",
    "    ab_imp = clf.feature_importances_\n",
    "    ab_imp = pd.Series(ab_imp, index=features).sort_values(ascending=False)\n",
    "    # plot importances\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ab_imp[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    return ab_baseline_acc, ab2_baseline_acc\n",
    "ab_baseline_acc, ab2_baseline_acc = run_ab(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f305",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dt(X_train, X_test, y_train, y_test):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    dt_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # plot importances\n",
    "    dt_importances = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dt_importances[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return dt_baseline_acc\n",
    "dt_baseline_acc = run_dt(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print('decision tree baseline accuracy: ' + str(dt_baseline_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b373f",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c30a330",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn(X_train, X_test, y_train, y_test, n_neighbors=3):\n",
    "    nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    #knn.fit(X_train, y_train)\n",
    "    #y_pred = knn.predict(X_test)\n",
    "    #knn_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    ####################\n",
    "    #  Feature importance cannot be discovered for the kNN model. \n",
    "    #####################\n",
    "    #y_pred = knn.predict(X_test)\n",
    "    #knn_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "    nca_pipe.fit(X_train, y_train)\n",
    "    knn_baseline_acc = nca_pipe.score(X_test, y_test)\n",
    "    #print('knn baseline accuracy: ' + str(knn_baseline_acc))\n",
    "    return knn_baseline_acc\n",
    "knn_baseline_acc = run_knn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b3084",
   "metadata": {},
   "source": [
    "# Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737390",
   "metadata": {},
   "source": [
    "## Signal to Noise Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099bbe0",
   "metadata": {},
   "source": [
    "For a non-constant signal $S$ and noise $N$, the signal to noise ratio is defined as the following:\n",
    "$$ SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70b232",
   "metadata": {},
   "source": [
    "The expected value $\\mathbb{E}[X]$ of any continuous random variable $X$ is $\\int_{-\\infty}^{\\infty} x p(x) dx $, where $p(x)$ is its associated probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d01d",
   "metadata": {},
   "source": [
    "For homoskedastic noise, we can use closed form expressions to compute $E[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$.\n",
    "\n",
    "- For uniformly distributed noise $N$ ~ $u(\\alpha, \\beta)$, by the same logic as above $\\mathbb{E}[N^2] = \\left(\\frac{\\alpha - \\beta}{2}\\right)^2$.\n",
    "\n",
    "- For frequency-domain noise $N$ of the form $A\\sin(2\\pi x \\frac{1}{f}) + y, \\mathbb{E}[N^2] \\approx y^2 + \\frac{A^2}{2}$. Note the $\\approx$ since we cannot guarantee that the signal will end precisely on the end of the sin wave.\n",
    "\n",
    "For heteroskedastic noise, because there is no closed form expression, we simply take `N.mean()` where $N$ is our noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0cc25",
   "metadata": {},
   "source": [
    "## Denoising Using Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92c4d9",
   "metadata": {},
   "source": [
    "Let $X$ represent our set of physiological signals and $\\textbf{x}_i$ denote the *i*-th column of $X$. In our case, $\\textbf{x}_i$ is one of the ECG, BVP, EDA, ACC, etc. The post-noise signal we observe $\\textbf{x}_i = \\widetilde{\\textbf{x}}_i + \\xi_i$ is composed of the original raw signal and Gaussian distributed noise with $\\mathbb{E}[\\xi] = 0$ and $V[\\xi] = E[N^2] = \\sigma^2 = \\frac{\\mathbb{E}[S^2]}{SNR}$. Literature has indicated that a principal component analysis of $\\textbf{x}_i$ can produce an estimate of $\\widetilde{\\textbf{x}}_i$ that is closer than the noisy measurements are (citation needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542fdb4",
   "metadata": {},
   "source": [
    "## Calculate Distribution Parameters from SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb7d2c",
   "metadata": {},
   "source": [
    "Given a signal $S$, we can specify a signal to noise ratio $SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]}$ and use this to calculate $\\mathbb{E}[N^2]$ because $SNR$ and $\\mathbb{E}[S^2]$ are known. So $\\mathbb{E}[N^2] = \\frac{\\mathbb{E}[S^2]}{SNR}$.\n",
    "\n",
    "Then, for any homoskedastic noise following a well-defined probability density function (PDF), we can solve for the parameters of the PDF using the known value $\\mathbb{E}[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$. Thus, $\\sigma^2 = \\frac{\\mathbb{E}[S^2]}{SNR}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819be01",
   "metadata": {},
   "source": [
    "## Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99319416",
   "metadata": {},
   "source": [
    "The Gaussian probability density function is of the following form:\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4198cb0",
   "metadata": {},
   "source": [
    "### Estimating $\\mu$ and $\\sigma$ of the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330a231",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential with Homoskedasticity Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4116e",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with homoskedasticity approach constructs a Gaussian distribution such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_i - S_{i+n}|)$, where $max(|S_i - S_{i+n}|)$ denotes the maximum absolute difference of the signal between index $i$ and $i+n$ in the entire signal, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "In conclusion, we randomly sample from the following probability density function:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\alpha \\cdot max(|S_i - S_{i+n}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_i - S_{i+n}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "This noise exhbits homoskedasticity because it does not vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5a055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(16,4))\n",
    "axs[0].plot(np.ravel(patients_new_noise1[0]['signal']['chest']['Resp']))\n",
    "axs[0].set_title('SNR: 0.01')\n",
    "axs[1].plot(np.ravel(patients_new_noise2[0]['signal']['chest']['Resp']))\n",
    "axs[1].set_title('SNR: 0.05')\n",
    "axs[2].plot(np.ravel(patients_new_noise3[0]['signal']['chest']['Resp']))\n",
    "axs[2].set_title('SNR: 0.1')\n",
    "axs[3].plot(np.ravel(patients_new_noise4[0]['signal']['chest']['Resp']))\n",
    "axs[3].set_title('SNR: 0.2')\n",
    "axs[4].plot(np.ravel(patients_new_noise5[0]['signal']['chest']['Resp']))\n",
    "axs[4].set_title('SNR: 0.5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022334",
   "metadata": {},
   "source": [
    "# Data Preparation (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce48d1c",
   "metadata": {},
   "source": [
    "Prepare the data again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b8073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snrs = [0.00001, 0.0001,  0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6] # this is biggest list we'd need\n",
    "snrs = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "n_samples = 10 # number of samples taken per SNR\n",
    "loadPath = '../data/GN-WESAD'\n",
    "#savePath = '../data/GN-WESAD'\n",
    "subject_feature_path = '/subject_feats'\n",
    "n_samples = 10 \n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "fb_model_list = ['DT', 'RF', 'SVM', 'LDA', 'KNN', 'AdaBoost']\n",
    "cases = ['WESAD', 'GN-WESAD', 'PR-WESAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rparser_with_noise:\n",
    "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
    "    VALUE_EXTRACT_KEYS = {\n",
    "        \"age\": {\n",
    "            'search_key': 'Age',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"height\": {\n",
    "            'search_key': 'Height',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"weight\": {\n",
    "            'search_key': 'Weight',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"gender\": {\n",
    "            'search_key': 'Gender',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"dominant_hand\": {\n",
    "            'search_key': 'Dominant',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"coffee_today\": {\n",
    "            'search_key': 'Did you drink coffee today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"coffee_last_hour\": {\n",
    "            'search_key': 'Did you drink coffee within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"sport_today\": {\n",
    "            'search_key': 'Did you do any sports today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoker\": {\n",
    "            'search_key': 'Are you a smoker',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoke_last_hour\": {\n",
    "            'search_key': 'Did you smoke within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"feel_ill_today\": {\n",
    "            'search_key': 'Do you feel ill today',\n",
    "            'delimiter': '? '\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DATA_PATH = 'data/WESAD/'\n",
    "    parse_file_suffix = '_readme.txt'\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
    "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
    "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
    "        \n",
    "        # Check if parsed readme file is available ( should be as it is saved above )\n",
    "        if not os.path.isfile('data/readmes.csv'):\n",
    "            print('Parsing Readme files')\n",
    "            self.parse_all_readmes()\n",
    "        else:\n",
    "            print('Files already parsed.')\n",
    "            \n",
    "        self.merge_with_feature_data_with_noise()\n",
    "        \n",
    "        \n",
    "    def parse_readme(self, subject_id):\n",
    "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
    "\n",
    "            x = f.read().split('\\n')\n",
    "\n",
    "        readme_dict = {}\n",
    "\n",
    "        for item in x:\n",
    "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
    "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
    "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
    "                if item.startswith(search_key):\n",
    "                    d, v = item.split(delimiter)\n",
    "                    readme_dict.update({key: v})\n",
    "                    break\n",
    "        return readme_dict\n",
    "\n",
    "\n",
    "    def parse_all_readmes(self):\n",
    "        \n",
    "        dframes = []\n",
    "\n",
    "        for subject_id, path in self.readme_locations.items():\n",
    "            readme_dict = self.parse_readme(subject_id)\n",
    "            df = pd.DataFrame(readme_dict, index=[subject_id])\n",
    "            dframes.append(df)\n",
    "\n",
    "        df = pd.concat(dframes)\n",
    "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
    "\n",
    "        \n",
    "    def merge_with_feature_data_with_noise(self):\n",
    "        # Confirm feature files are available\n",
    "        if os.path.isfile('data/may14_feats4_with_noise.csv'):\n",
    "            feat_df = pd.read_csv('data/may14_feats4_with_noise.csv', index_col=0)\n",
    "            print(feat_df.info())\n",
    "        else:\n",
    "            print('No feature data available. Exiting...')\n",
    "            return\n",
    "           \n",
    "        # Combine data and save\n",
    "        df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
    "\n",
    "        dummy_df = pd.get_dummies(df)\n",
    "        \n",
    "        dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
    "\n",
    "        dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
    "                           'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
    "                           'feel_ill_today_YES', 'subject']]\n",
    "\n",
    "        merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
    "\n",
    "        merged_df.to_csv('data/noise_snr_0.6.csv')\n",
    "rp_with_noise = rparser_with_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb3726",
   "metadata": {},
   "source": [
    "# Modeling (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99893b",
   "metadata": {},
   "source": [
    "Model again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f997c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/noise_snr_0.15.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.isna().sum()\n",
    "for i in range(len(s)):\n",
    "    if s[i] > 0:\n",
    "        print(s.index[i], s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns != 'Resp_C_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:, df.columns != 'label'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6559b2c",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63bf21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8aa91f",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = classifier.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d44d3",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973d694",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4551",
   "metadata": {},
   "source": [
    "Compare the results of the noisy data models and the clean data models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb53c5",
   "metadata": {},
   "source": [
    "- Plots of SNR (x-axis) vs. accuracy (y-axis)\n",
    "- Compare feature importances across different noise regimes\n",
    "    - Develop dynamic evaluation method based on original feature importance / added noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0de9",
   "metadata": {},
   "source": [
    "## Test Each Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_table = pd.DataFrame(columns=[\n",
    "    'SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "    snr     %           F1           WESAD      Noise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10e7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "gn_wesad_day = '2023-11-12'\n",
    "model = {type}\n",
    "fb_model_list = ['DT', 'RF', 'SVM', 'LDA', 'KNN', 'AdaBoost'] #redudant\n",
    "e2e_model_list = ['LSTM', 'CNN', 'GRU', 'RNN'] # placeholder for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gn_wesad_path(n_i, snr):\n",
    "    return f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats.csv'\n",
    "def fetch_data(n_i, snr):\n",
    "    file_path = gn_wesad_path(n_i, snr)\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    return df\n",
    "def e2e_model(df, model):\n",
    "\n",
    "    #this is mostly going to be fetching the data. \n",
    "    #maybe check if results are already there. if yes, return them.\n",
    "    #if not, run the model via sbatch script and return the results.\n",
    "    e2e_results_path = '../data/e2e_results' # this is just a placeholder for now\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "def fb_model(df, model):\n",
    "    #unlike e2e, this is going to be a bit more complicated.\n",
    "    # it will run the model instead of just fetching the results.\n",
    "\n",
    "    fb_results_path = gn_wesad_path(n_i, snr) # this is just a placeholder for now\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "\n",
    "def get_model(df, model):\n",
    "\n",
    "    if model in e2e_model_list:\n",
    "        return e2e_model(df, model)\n",
    "    elif model in fb_model_list:\n",
    "        return fb_model(df, model)\n",
    "    \n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "\n",
    "def run_wesad_models(loadPath, fb_model_list):\n",
    "\n",
    "    results_table = pd.DataFrame()\n",
    "    for n_i in range(n_samples):\n",
    "        \n",
    "        for model in fb_model_list:\n",
    "            \n",
    "            \n",
    "            model_output = get_model(df, model)\n",
    "        \n",
    "            results_table.loc[model] = pd.Series({'SNR':model_output[0], 'Accuracy':,\n",
    "                                           'F1 Score':, 'dataset':})\n",
    "    return results_table\n",
    "def run_GN_models(snrs, n_samples, loadPath, fb_model_list):\n",
    "    \n",
    "    # the difference mainly between WESAD and GN-WESAD is that GN-WESAD has multiple snrs. \n",
    "    results_table = pd.DataFrame()    \n",
    "    \n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            \n",
    "            df = fetch_data(n_i, snr)\n",
    "            \n",
    "            for model in fb_model_list:\n",
    "                \n",
    "                model_output = get_model(df, model)\n",
    "        \n",
    "                results_table.loc[model] = pd.Series({'SNR':model_output[0], 'Accuracy':,\n",
    "                                           'F1 Score':, 'dataset':})\n",
    "    \n",
    "    date = \n",
    "    file_name = \n",
    "    df = pd.read_csv(loadPath+ )\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "    # split data into features and labels\n",
    "\n",
    "\n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            for model in fb_model_list:\n",
    "                \n",
    "                model_output = get_model(df, model)\n",
    "\n",
    "                results_table.loc[str(model)]['Accuracy'] = model_output[0]\n",
    "                results_table.loc[str(model)]['Precision'] = model_output[1]\n",
    "                results_table.loc[str(model)]['dataset'] = model_output[2]\n",
    "                results_table.loc[str(model)]['snr'] = model_output[3]\n",
    "                results_table.loc[str(model)]['n_i'] = model_output[4]\n",
    "                   \n",
    "    return results_table\n",
    "\n",
    "\n",
    "print(fb_model_list)\n",
    "lda_accuracy = []\n",
    "rf_accuracy = []\n",
    "svm_accuracy = []\n",
    "ft_imp_matrix = []\n",
    "# For each signal to noise ratio\n",
    "for case in cases:\n",
    "    if case == 'GN-WESAD':\n",
    "        loadPath = '../data/GN-WESAD'\n",
    "        GN_model_results = run_GN_models(snrs, subject_ids, n_samples, loadPath, fb_model_list, feature_list)\n",
    "    if case == 'WESAD':\n",
    "        loadPath = '../data/WESAD'\n",
    "    if case == 'PR-WESAD':\n",
    "        loadPath = '../data/PR-WESAD'\n",
    "\n",
    "for i in range(len(snrs)):\n",
    "    # Get data\n",
    "    df = pd.read_csv('data/noise_snr_'+str(snrs[i])+'.csv', index_col=0)\n",
    "    # Since Resp_C_rate is null for the first four, simply get rid of it\n",
    "    df = df.loc[:, df.columns != 'Resp_C_rate']\n",
    "    \n",
    "    # Get features, label\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values\n",
    "\n",
    "    # Get train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "    \n",
    "    # Scale the data\n",
    "    sc = StandardScaler()  \n",
    "    X_train = sc.fit_transform(X_train)  \n",
    "    X_test = sc.transform(X_test)  \n",
    "    \n",
    "    # Test LDA\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    lda_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Test RF\n",
    "    classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "    classifier.fit(X_train, y_train)  \n",
    "    y_pred = classifier.predict(X_test)  \n",
    "    rf_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    importances = classifier.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "    ft_imp_matrix.append(importances)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    forest_importances[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI with SNR: \" + str(snrs[i]))\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "    # Test SVM\n",
    "    clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_out = clf.predict(X_test)    \n",
    "    svm_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # test kNN\n",
    "\n",
    "    # test DT\n",
    "\n",
    "    # test AdaBoost\n",
    "# list of all models\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/noise_snr_0.15.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.isna().sum()\n",
    "for i in range(len(s)):\n",
    "    if s[i] > 0:\n",
    "        print(s.index[i], s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns != 'Resp_C_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:, df.columns != 'label'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gn_wesad_path(n_i, snr):\n",
    "    return f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats.csv'\n",
    "\n",
    "def e2e_model(df, model):\n",
    "\n",
    "    #this is mostly going to be fetching the data. \n",
    "    #maybe check if results are already there. if yes, return them.\n",
    "    #if not, run the model via sbatch script and return the results.\n",
    "    e2e_results_path = '../data/e2e_results' # this is just a placeholder for now\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "def fb_model(df, model):\n",
    "    #unlike e2e, this is going to be a bit more complicated.\n",
    "    # it will run the model instead of just fetching the results.\n",
    "\n",
    "    fb_results_path = gn_wesad_path(n_i, snr) # this is just a placeholder for now\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "\n",
    "def get_model(df, model):\n",
    "\n",
    "    if model in e2e_model_list:\n",
    "        return e2e_model(df, model)\n",
    "    elif model in fb_model_list:\n",
    "        return fb_model(df, model)\n",
    "    \n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    dataset = []\n",
    "    snr = []\n",
    "    n_i = []\n",
    "    noise_fcn = []\n",
    "\n",
    "    return [accuracy, precision, dataset, snr, n_i]\n",
    "\n",
    "def run_wesad_models(loadPath, fb_model_list):\n",
    "\n",
    "    results_table = pd.DataFrame()\n",
    "    for n_i in range(n_samples):\n",
    "        \n",
    "        for model in fb_model_list:\n",
    "            \n",
    "            \n",
    "            model_output = get_model(df, model)\n",
    "        \n",
    "            results_table.loc[model] = pd.Series({'SNR':model_output[0], 'Accuracy':,\n",
    "                                           'F1 Score':, 'dataset':})\n",
    "    return results_table\n",
    "def run_GN_models(snrs, n_samples, loadPath, fb_model_list):\n",
    "    \n",
    "    # the difference mainly between WESAD and GN-WESAD is that GN-WESAD has multiple snrs. \n",
    "    results_table = pd.DataFrame()    \n",
    "    \n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            file_path = gn_wesad_path(n_i, snr)\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            \n",
    "            for model in fb_model_list:\n",
    "                \n",
    "                model_output = get_model(df, model)\n",
    "        \n",
    "                results_table.loc[model] = pd.Series({'SNR':model_output[0], 'Accuracy':,\n",
    "                                           'F1 Score':, 'dataset':})\n",
    "    \n",
    "    date = \n",
    "    file_name = \n",
    "    df = pd.read_csv(loadPath+ )\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "    # split data into features and labels\n",
    "\n",
    "\n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            for model in fb_model_list:\n",
    "                \n",
    "                model_output = run_FB_models(df, model)\n",
    "\n",
    "                results_table.loc[str(model)]['Accuracy'] = model_output[0]\n",
    "                results_table.loc[str(model)]['Precision'] = model_output[1]\n",
    "                results_table.loc[str(model)]['dataset'] = model_output[2]\n",
    "                results_table.loc[str(model)]['snr'] = model_output[3]\n",
    "                results_table.loc[str(model)]['n_i'] = model_output[4]\n",
    "                   \n",
    "    return results_table\n",
    "\n",
    "\n",
    "print(fb_model_list)\n",
    "lda_accuracy = []\n",
    "rf_accuracy = []\n",
    "svm_accuracy = []\n",
    "ft_imp_matrix = []\n",
    "# For each signal to noise ratio\n",
    "for case in cases:\n",
    "    if case == 'GN-WESAD':\n",
    "        loadPath = '../data/GN-WESAD'\n",
    "        GN_model_results = run_GN_models(snrs, subject_ids, n_samples, loadPath, fb_model_list, feature_list)\n",
    "    if case == 'WESAD':\n",
    "        loadPath = '../data/WESAD'\n",
    "    if case == 'PR-WESAD':\n",
    "        loadPath = '../data/PR-WESAD'\n",
    "\n",
    "for i in range(len(snrs)):\n",
    "    # Get data\n",
    "    df = pd.read_csv('data/noise_snr_'+str(snrs[i])+'.csv', index_col=0)\n",
    "    # Since Resp_C_rate is null for the first four, simply get rid of it\n",
    "    df = df.loc[:, df.columns != 'Resp_C_rate']\n",
    "    \n",
    "    # Get features, label\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values\n",
    "\n",
    "    # Get train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "    \n",
    "    # Scale the data\n",
    "    sc = StandardScaler()  \n",
    "    X_train = sc.fit_transform(X_train)  \n",
    "    X_test = sc.transform(X_test)  \n",
    "    \n",
    "    # Test LDA\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    lda_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Test RF\n",
    "    classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "    classifier.fit(X_train, y_train)  \n",
    "    y_pred = classifier.predict(X_test)  \n",
    "    rf_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    importances = classifier.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "    ft_imp_matrix.append(importances)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    forest_importances[:20].plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI with SNR: \" + str(snrs[i]))\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "    # Test SVM\n",
    "    clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_out = clf.predict(X_test)    \n",
    "    svm_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # test kNN\n",
    "\n",
    "    # test DT\n",
    "\n",
    "    # test AdaBoost\n",
    "# list of all models\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "for ft in range(0, len(ft_imp_matrix)):\n",
    "    ft_imp_matrix[ft] = [sorted(ft_imp_matrix[ft], reverse=True).index(x) for x in ft_imp_matrix[ft]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ebc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df = pd.DataFrame(ft_imp_matrix, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d157b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.reset_index().pivot(index=ft_imp_df, columns=ft_imp_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WESAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WESAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SNR Accuracy F1-Score dataset\n",
       "SVM   1        5      NaN   WESAD\n",
       "RF    1        5      NaN   WESAD"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_table = pd.DataFrame(columns=['SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "results_table.loc[str('SVM')] = pd.Series({'SNR':1, 'Accuracy':5,\n",
    "                                           'F1 Score':2, 'dataset':'WESAD'})\n",
    "results_table.loc[str('RF')] = pd.Series({'SNR':1, 'Accuracy':5,\n",
    "                                          'F1 Score':2, 'dataset':'WESAD'})\n",
    "\n",
    "'''\n",
    "fb_model_list = ['DT', 'RF', 'SVM', 'LDA', 'KNN', 'AdaBoost']\n",
    "\n",
    "for model in fb_model_list:\n",
    "    for i in range(len(snrs)):\n",
    "        results_table.loc[str(model) + str(snrs[i])] = pd.Series({'SNR':snrs[i], 'Accuracy':svm_accuracy[i], 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "display(results_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742139d",
   "metadata": {},
   "source": [
    "## Plot SNR vs. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,7))\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "min_accuracy = min(min(lda_accuracy), min(rf_accuracy), min(svm_accuracy)) - 0.01\n",
    "max_accuracy = max(lda_baseline_acc, rf_baseline_acc, svm_baseline_acc) + 0.01\n",
    "\n",
    "axs[0].plot(snrs, lda_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[0].plot(snrs, [lda_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[0].set_title('Linear Discriminant Analysis', fontsize=20);\n",
    "axs[0].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[0].set_ylabel('Classification Accuracy', fontsize=15);\n",
    "axs[0].legend();\n",
    "axs[0].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[1].plot(snrs, rf_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[1].plot(snrs, [rf_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[1].set_title('Random Forest', fontsize=20);\n",
    "axs[1].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[1].legend();\n",
    "axs[1].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[2].plot(snrs, svm_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[2].plot(snrs, [svm_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[2].set_title('Support Vector Machine', fontsize=20);\n",
    "axs[2].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[2].legend();\n",
    "axs[2].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "# add knn, dt and adaboost\n",
    "\n",
    "axs[3].plot(snrs, knn_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[3].plot(snrs, [knn_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[3].set_title('kNN', fontsize=20);\n",
    "axs[3].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[3].legend();\n",
    "axs[3].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[4].plot(snrs, dt_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[4].plot(snrs, [dt_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[4].set_title('Decision Tree', fontsize=20);\n",
    "axs[4].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[4].legend();\n",
    "axs[4].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[5].plot(snrs, ab_baseline_acc, label='Accuracy (with noise)', marker='o');\n",
    "axs[5].plot(snrs, [ab_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[5].set_title('AdaBoost', fontsize=20);\n",
    "axs[5].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[5].legend();\n",
    "axs[5].set_ylim([min_accuracy, max_accuracy]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Feature extraction for ecg failed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.warning(\"Feature extraction for ecg failed. \\n This happened for participant \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1055px",
    "left": "216px",
    "top": "111.12px",
    "width": "343.299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
