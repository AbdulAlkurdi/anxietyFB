{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fb testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import dask as pd\n",
    "import pandas as pd_old\n",
    "import warnings\n",
    "import utils\n",
    "import heartpy as hp\n",
    "from ECG_feature_extractor_1001 import *\n",
    "# import time \n",
    "import time\n",
    "from datetime import datetime\n",
    "from biosppy.signals import ecg\n",
    "from feature_extraction import SubjectData, compute_features, get_samples, combine_files\n",
    "\n",
    "# To ignore all warnings:\n",
    "warnings.filterwarnings(\"ignore\", module=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_IN_SECONDS = 60\n",
    "stride = 1\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "feat_names = None\n",
    "loadPath = '../data/WESAD'\n",
    "savePath = '../data/GN-WESAD'\n",
    "subject_feature_path = '/subject_feats'\n",
    "onedrive = '/mnt/d/Users/alkurdi/OneDrive - University of Illinois - Urbana/data/GN-WESAD'\n",
    "n_samples = 10 \n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [ 0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6] #0.00001,\n",
    "fb_model_list = ['DT', 'RF', 'LDA', 'KNN', 'AdaBoost', 'SVM']\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_status(snrs, subject_ids, onedrive, n_samples= 10):\n",
    "    #bads are the ones that do not have the gaussian-modified data.  \n",
    "    bads = []\n",
    "    bad_snrs = []\n",
    "    bad_subjects = []\n",
    "    bad_ns = []\n",
    "    completed_snrs = []\n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            for subject_id in subject_ids:\n",
    "                #print(snr)\n",
    "                \n",
    "                #print(f'{onedrive}/n_{n_i}/snr_{snr}/S{subject_id}/{a}')\n",
    "                try: \n",
    "                    a = os.listdir(f'{onedrive}/n_{n_i}/snr_{str(snr)}/S{subject_id}')\n",
    "                    a[0]\n",
    "                    completed_snrs.append(snr)\n",
    "                except:\n",
    "                    bads.append(f'n_{n_i}/snr_{snr}/S{subject_id}')\n",
    "                    bad_snrs.append(snr)\n",
    "                    bad_subjects.append(subject_id)\n",
    "                    bad_ns.append(n_i)\n",
    "\n",
    "    bad_snrs = sorted(set(bad_snrs))\n",
    "    bad_subjects = sorted(set(bad_subjects))\n",
    "    bad_ns = sorted(set(bad_ns))\n",
    "    completed_snrs = sorted(set(completed_snrs))\n",
    "    #printing after checking\n",
    "    print(f'completed snrs :{completed_snrs}')\n",
    "    print(f'incomplete snrs :{bad_snrs}')\n",
    "get_processing_status(snrs, subject_ids, onedrive, n_samples= n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "for i in os.listdir(f'{loadPath}/subject_feats'):\n",
    "    #print (i)\n",
    "    if 'S' not in i[0]:\n",
    "        dataset_list.append(i)\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "\n",
    "for each_dataset in dataset_list:\n",
    "    for snr in snrs:\n",
    "        for model in fb_model_list:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "savePath = 'C:/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "n_samples = [9]\n",
    "subject_ids = [2]\n",
    "snrs = [0.01]\n",
    "for n_i in n_samples:\n",
    "    if not os.path.exists(savePath + '/n_'+str(n_i)):\n",
    "        os.makedirs(savePath + '/n_'+str(n_i))\n",
    "    for snr in snrs:\n",
    "        if not os.path.exists(savePath + '/n_'+str(n_i)+'/snr_'+str(snr)):\n",
    "            os.makedirs(savePath + '/n_'+str(n_i)+'/snr_'+str(snr))\n",
    "        for subject_id in subject_ids:\n",
    "            if not os.path.exists(savePath + '/n_'+str(n_i)+'/snr_'+str(snr)+ '/S'+str(subject_id)):\n",
    "                os.makedirs(savePath + '/n_'+str(n_i)+'/snr_'+str(snr)+ '/S'+str(subject_id))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_9\n",
      "True\n",
      "['poop.txt', 'snr_0.01']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(f'{savePath}/n_{n_i}')\n",
    "print(os.path.isdir(f'{savePath}/n_{n_i}'))\n",
    "print(os.listdir(f'{savePath}/n_{n_i}'))\n",
    "print(os.path.isdir(f'{savePath}/n_{n_i}/snr_{snr}/fixed_resampled140hz_S{subject_id}.pkl'))\n",
    "\n",
    "with open(f'{savePath}/n_{n_i}/poop.txt', 'w') as f:\n",
    "    f.write('poop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd_old.DataFrame(columns=['SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "results_table.loc[str('SVM')] = pd_old.Series({'SNR':1, 'Accuracy':5, 'F1 Score':2, 'dataset':'WESAD'})\n",
    "results_table.loc[str('RF')] = pd_old.Series({'SNR':1, 'Accuracy':5, 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "fb_model_list = ['SVM', 'RF']\n",
    "for model in fb_model_list:\n",
    "    for i in range(len(snrs)):\n",
    "        results_table.loc[str(model) + str(snrs[i])] = pd.Series({'SNR':snrs[i], 'Accuracy':svm_accuracy[i], 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "ecg_biosppy = ecg\n",
    "ecg = None\n",
    "fs_ecg = 700\n",
    "fs_ppg = 64\n",
    "subject_id=2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = SubjectData(main_path=loadPath, subject_number=subject_id)\n",
    "data_dict = subject.get_wrist_and_chest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = data_dict['ECG'][40000:60000].flatten()\n",
    "ppg = data_dict['BVP'][int(40000/700*64):int(60000/700*64)].flatten()\n",
    "print('ecg len', len(ecg))\n",
    "print('ppg len', len(ppg))\n",
    "\n",
    "\n",
    "#testing for ECG\n",
    "\n",
    "now = time.time()\n",
    "#wd4, m4 = hp.process(ecg, fs_ecg)\n",
    "#print('hp.process execution time is %5.2fs' % (time.time()-now))\n",
    "\n",
    "now = time.time()\n",
    "#pack2, ecgout2, time_dict2 = freq_ratio_hybrid(ecg, fs_ecg, RR1, method='welch', factor = 1)\n",
    "#print('freq_ratio_hybrid execution time is %5.2fs' % (time.time()-now))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "BS_signal_analysis3, pack3, ecg_out3 = analyze_ecg(ecg, fs_ecg)\n",
    "print('analyze_ecg execution time is ', now-time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = data_dict['ECG'][10000:80000].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for PPG\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR, time_dict = freq_ratio( ppg, fs_ppg, method='welch', factor = 1)\n",
    "print('freq_ratio execution time is ', now-time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, time_dict = freq_ratio_hybrid(ppg, fs_ppg, method='welch', factor = 1)\n",
    "print('freq_ratio_hybrid execution time is ', now-time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "BS_signal_analysis, pack, ppg_out = analyze_ecg(ppg, fs_ppg)\n",
    "print('analyze_ecg execution time is ', now-time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "wd, m = hp.process(ppg, fs_ppg)\n",
    "print('hp.process execution time is ', now-time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR = freq_ratio_fast( ppg, fs_ppg, method='welch', factor = 1)\n",
    "print('freq_ratio_fast execution time is ', now-time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "import logging\n",
    "\n",
    "\n",
    "def combine_noiZ_files(subject_ids):\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    logging.basicConfig(level=logging.DEBUG, filename=today+'-combine.log', filemode='w', force=True)\n",
    "    logging.info('Started')\n",
    "    print('total number of combines: ', len(snrs)*n_samples)\n",
    "    df_list = []\n",
    "    i = 0\n",
    "    for snr in snrs:\n",
    "        for n_i in range(n_samples):\n",
    "            for s in subject_ids:\n",
    "                df = pd_old.read_csv(f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{s}_feats.csv', index_col=0)\n",
    "                df['subject'] = s\n",
    "                df_list.append(df)\n",
    "            df = pd_old.concat(df_list)\n",
    "            df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "            df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            now = datetime.now().strftime('%Y-%m-%d')\n",
    "            df.to_csv(f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv')\n",
    "            i+=1\n",
    "            logging.info(f'Saved file to: {savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{today}_feats_filt.csv  {i}/{len(snrs)*n_samples}')\n",
    "            print('Saved file to: ',f'.../n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv   {i}/{len(snrs)*n_samples}')\n",
    "            counts = df['label'].value_counts()\n",
    "            logging.info('Number of samples per class:')\n",
    "            logging.info('baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(*list(zip(counts.index, counts.values))))\n",
    "    logging.info('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_noiZ_files(subject_ids)\n",
    "# took 19m 30s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "now = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def process_subject_file(snr, n_i, s):\n",
    "    file_path = f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{s}_feats.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df['subject'] = s\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def combine_noiZ_files(subjects):\n",
    "    now = datetime.now().strftime('%Y-%m-%d')\n",
    "    logging.basicConfig(level=logging.INFO, filename=now+'-combine.log', filemode='w', force=True)\n",
    "\n",
    "    for snr in snrs:\n",
    "        for n_i in range(n_samples):\n",
    "            # Parallelize file reading\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = [executor.submit(process_subject_file, snr, n_i, s) for s in subjects]\n",
    "                df_list = [future.result() for future in futures if future.result() is not None]\n",
    "\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['label'] = df[['0', '1', '2']].idxmax(axis=1)\n",
    "                df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                df.to_csv(f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv')\n",
    "                logging.info('-' * 20)\n",
    "                logging.info(f'Saved file to: {savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv')\n",
    "                \n",
    "                counts = df['label'].value_counts()\n",
    "                logging.info('Number of samples per class:')\n",
    "                logging.info('baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(*list(zip(counts.index, counts.values))))\n",
    "    logging.info('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i = 5\n",
    "s = 2\n",
    "snr = 0.6\n",
    "df = pd_old.read_csv(f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{subject_id}_feats.csv', index_col=0)\n",
    "df['subject'] = s\n",
    "#df_list.append(df)\n",
    "#f = pd_old.concat(df_list)\n",
    "df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "counts = df['label'].value_counts()\n",
    "print('Number of samples per class:')\n",
    "print('baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(*list(zip(counts.index, counts.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_noiZ_files(subject_ids)\n",
    "#this paralellized took 34seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, filemode='w', force=True)\n",
    "logging.info('GN-WESAD models ran and results generated and saved in: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    #print(os.listdir(path))\n",
    "    df = pd_old.read_csv(path, index_col=0)\n",
    "    print('len of df ',len(df))\n",
    "    pd_old.set_option('display.max_columns', None) \n",
    "    #We want to drop columns in df that are not in RADWear to match modalities. \n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    #df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if 'EMG' in column or 'EDA_C' in column or 'Temp_C' in column or 'TEMP_C' in column or 'SCR_C' in column or 'SCL_C' in column:\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    print('len of reduced df ',len(df))\n",
    "    return df \n",
    "def gn_wesad_path(n_i, snr):\n",
    "    loadPath = '../data/GN-WESAD'\n",
    "    return f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "gn_wesad_day = '2023-11-13'\n",
    "matrix = np.zeros((len(snrs), n_samples))\n",
    "\n",
    "for n_i in range(n_samples):\n",
    "    for i, snr in enumerate(snrs):\n",
    "        print(f'for {n_i} and {snr} number {i}: ')\n",
    "        file_path = gn_wesad_path(n_i, snr)\n",
    "        df = read_n_reduce(file_path)\n",
    "        matrix[i][n_i] = len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "'data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "loadPath = '../data/GN-WESAD'\n",
    "#display(os.listdir(f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}'))\n",
    "snr0001 = pd_old.read_csv('../data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-13_feats2.csv', index_col=0)\n",
    "snr001 = pd_old.read_csv('../data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-13_feats2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'SNR': [0.6]*10,\n",
    "    'Accuracy': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'F1 Score': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'dataset': ['GN-WESAD']*10,\n",
    "    'n_i': list(range(10)),\n",
    "    'n': [10]*10,\n",
    "    'noise gen function': ['Gaussian Noise']*10,\n",
    "    'Precision': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'Recall': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'Model': ['DT']*10\n",
    "})\n",
    "\n",
    "# Example Series\n",
    "series = pd.Series({\n",
    "    'SNR': 0.600000,\n",
    "    'Accuracy': 0.945747,\n",
    "    'F1 Score': 0.945747,\n",
    "    'n_i': 4.500000,\n",
    "    'n': 10.000000,\n",
    "    'Precision': 0.945747,\n",
    "    'Recall': 0.945747\n",
    "}, name='SNR 0.6 Model DT mean')\n",
    "\n",
    "# Convert the series to a DataFrame\n",
    "series_df = series.to_frame().T\n",
    "\n",
    "# Append the new DataFrame to the existing one\n",
    "df_combined = pd.concat([df, series_df], ignore_index=True)\n",
    "\n",
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'SNR': [0.6]*10,\n",
    "    'Accuracy': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'F1 Score': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'dataset': ['GN-WESAD']*10,\n",
    "    'n_i': list(range(10)),\n",
    "    'n': [10]*10,\n",
    "    'noise gen function': ['Gaussian Noise']*10,\n",
    "    'Precision': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'Recall': [0.940639, 0.968134, 0.964939, 0.940909, 0.937785, 0.923780, 0.942337, 0.960486, 0.952816, 0.925645],\n",
    "    'Model': ['DT']*10\n",
    "})\n",
    "series = pd.Series({\n",
    "    'SNR': 0.600000,\n",
    "    'Accuracy': 0.945747,\n",
    "    'F1 Score': 0.945747,\n",
    "    'n_i': 4.500000,\n",
    "    'n': 10.000000,\n",
    "    'Precision': 0.945747,\n",
    "    'Recall': 0.945747\n",
    "}, name='SNR 0.6 Model DT mean')\n",
    "\n",
    "print(set(df.columns))\n",
    "print(set(series.index))\n",
    "missing_columns = set(df.columns) - set(series.index)\n",
    "print(missing_columns)\n",
    "for col in missing_columns:\n",
    "    # You can assign a specific value or use a value from the DataFrame\n",
    "    # Here, I'm using the first row's value as an example\n",
    "    #print(col)\n",
    "    #print(df[col].iloc[0])\n",
    "    \n",
    "    series[col] = df[col].iloc[0]\n",
    "    #print(series[col])\n",
    "    \n",
    "#print(series)\n",
    "\n",
    "series_df2 = series.to_frame().T\n",
    "display(series_df2)\n",
    "df_combined2 = pd.concat([df, series_df2])\n",
    "display(df_combined2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/GN-WESAD/cm_cr_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#tabulate and plot\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../data/GN-WESAD/cm_cr_dict.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     GN_cm_cr_dict \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(handle)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/WESAD/cm_cr_dict.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n",
      "File \u001b[0;32m~/anaconda3/envs/fb_code/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/GN-WESAD/cm_cr_dict.pickle'"
     ]
    }
   ],
   "source": [
    "#tabulate and plot\n",
    "\n",
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "with open('../data/WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    WESAD_cm_cr_dict = pickle.load(handle)\n",
    "wesad_acc = pd.read_csv('../data/WESAD/wesad_models_results-win60stride1_wcm_wcr.csv', index_col=0)\n",
    "display(wesad_acc)\n",
    "#combined_results = pd.concat([WESAD_model_results, GN_model_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wbinaryf1.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "#gn_wesad_acc['Binary F1'] = None\n",
    "print(gn_wesad_acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_wesad_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wcm_wcr.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "gn_wesad_acc['Binary F1'] = None\n",
    "\n",
    "for i, classification_report in enumerate(GN_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    print(classification_report['id'][2], fb_model_list[i%6])\n",
    "    #print(cr)\n",
    "    #print(i//5)\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    #print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        #print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i%6]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i%6]\n",
    "        secret_df = pd.concat([secret_df, pd.Series({'Model': fb_model_list[i%6], 'Binary F1 Score': binary_f1_score,\n",
    "                                                    'SNR': classification_report['id'][0], 'n':classification_report['id'][1],\n",
    "                                                    'Model': classification_report['id'][2]}).to_frame().T], ignore_index=True)\n",
    "        #print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        #display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        #wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        #wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        #print(fb_model_list[i])\n",
    "        gn_wesad_acc.loc[gn_wesad_acc['Model'] == fb_model_list[i%6], 'Binary f1'] = binary_f1_score\n",
    "        #print(gn_wesad_acc.loc[gn_wesad_acc['Model'] == fb_model_list[i], 'binary f1']) \n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(GN_cm_cr_dict['cr']))\n",
    "print(len(WESAD_cm_cr_dict['cr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc_test = wesad_acc.copy()\n",
    "display(wesad_acc_test)\n",
    "#wesad_acc_test.loc['Model'] = 'DT'\n",
    "model = 'DT'\n",
    "#display (wesad_acc_test[wesad_acc_test['Model'] == model]['binary f1'] = 4)#[wesad_acc_test['SNR'] == np.nan] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in wesad_acc_test.iterrows():\n",
    "    #if row[1]['Model'] == model:\n",
    "    #    row[1]['binary f1'] = 4\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc_test['binary f1'] = None \n",
    "wesad_acc_test['binary acc'] = None\n",
    "#display(wesad_acc_test.drop(columns=[['binary f1', 'wow']]))\n",
    "display(wesad_acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in WESAD_cm_cr_dict['cr']:\n",
    "    #print('result', result)\n",
    "    if True:#'.' in result['id']:\n",
    "        #print(result['id'])\n",
    "        print(result['Classification Report'])\n",
    "        #print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wesad_acc_test = wesad_acc.copy()\n",
    "wesad_acc_test['Binary F1'] = None\n",
    "def extract_metrics(report):\n",
    "    # Regular expression to find numeric values\n",
    "    regex = r\"\\s+1\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\"\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = re.search(regex, report)\n",
    "    #print(match)\n",
    "    if match:\n",
    "        precision = float(match.group(1))\n",
    "        recall = float(match.group(2))\n",
    "        f1_score = float(match.group(3))\n",
    "        return precision, recall, f1_score\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "def calculate_binary_metrics(report):\n",
    "    precision, recall, f1_score = extract_metrics(report)\n",
    "\n",
    "    if precision is not None and recall is not None:\n",
    "        # Calculate binary F1 score for class 1\n",
    "        binary_f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return binary_f1_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#classification_report = \n",
    "# Extract and calculate binary F1 score\n",
    "fb_model_counter = fb_model_list\n",
    "\n",
    "\n",
    "secret_df = pd.DataFrame()\n",
    "\n",
    "for i, classification_report in enumerate(WESAD_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    #print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i]\n",
    "        secret_df = pd.concat([secret_df, pd.Series({'Model': fb_model_list[i], 'Binary F1 Score': binary_f1_score,\n",
    "                                                     'SNR': classification_report['id'][0], 'n':classification_report['id'][1],\n",
    "                                                     'Model': classification_report['id'][2]}).to_frame().T], ignore_index=True)\n",
    "        #print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        #display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        #wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        #wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        #print(fb_model_list[i])\n",
    "        wesad_acc_test.loc[wesad_acc_test['Model'] == fb_model_list[i], 'binary f1'] = binary_f1_score\n",
    "        print(wesad_acc_test.loc[wesad_acc_test['Model'] == fb_model_list[i], 'binary f1']) \n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")\n",
    "display(secret_df)\n",
    "display(wesad_acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([wesad_acc, secret_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fafi = pd.DataFrame(columns=['SNR', 'Accuracy', 'F1 Score', 'dataset', 'n_i', 'n',\n",
    "       'noise gen function', 'Precision', 'Recall', 'Model'])\n",
    "pd.Series(classification_report['id']).to_frame().T\n",
    "wow = (pd.Series(classification_report['id']).to_frame().T.rename(columns={0:'SNR', 1:'n', 2:'model'}))\n",
    "fafi = pd.concat([fafi,wow ])\n",
    "display(fafi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report['id'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(WESAD_cm_cr_dict['cr'])\n",
    "fb_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc['Accuracy'][i] \n",
    "wesad_acc['F1 Score'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    pd.set_option('display.max_columns', None) \n",
    "    #We want to drop columns in df that are not in RADWear to match modalities. \n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    #df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if 'EMG' in column or 'EDA_C' in column or 'Temp_C' in column or 'TEMP_C' in column or 'SCR_C' in column or 'SCL_C' in column:\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    return df \n",
    "print(os.listdir('../data/WESAD'))\n",
    "df = read_n_reduce('../data/WESAD//subject_feats/oct5_feats4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = df.drop(['label','subject'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=0):\n",
    "    # split data into features and labels\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)  \n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "the_splits = split_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "xx = df.drop(columns=['label'])\n",
    "yy = df['label']\n",
    "#xx.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xx)\n",
    "display(yy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#display(the_splits[0])\n",
    "X_train, X_test, y_train, y_test = the_splits\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_out = clf.predict(X_test)    \n",
    "\n",
    "svm_accuracy  = accuracy_score(y_test, y_out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_wesad = pd.read_csv('../data/WESAD/subject_feats/oct5_feats4.csv', index_col=0)\n",
    "new_wesad = pd.read_csv('../data/WESAD/subject_feats/WESADfeatures-win60stride1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_wesad = pd.read_csv('../data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-13_feats2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr in snrs:\n",
    "    for n_i in range(n_samples):\n",
    "        tgt = f'../data/GN-WESAD/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "        print(f'df {snr} n {n_i} shape is {(pd.read_csv(tgt, index_col=0)).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features used for fb_models\\n', *old_wesad.columns, sep=',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features used for fb_models\\n', *new_wesad.columns, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ downloads wesad loaded w pickle.load in  17.867 s\n",
      "gnwesad loaded in  1.823 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nnow = time()\\nwith open(load_freshws_path, 'rb') as file:\\n            freshws_df = pickle.load(file, encoding='latin1')\\nprint('d:\\\\ freshwesad loaded in ', round(time()-now,3),'s')  #d:\\\\ freshwesad loaded in  191.335 s\\n\\nnow = time()\\ndw_df = pd.read_pickle(load_dw_path)\\nprint('c:\\\\ downloads wesad loaded w pd.read_pickle in ', round(time()-now,3),'s') #c:\\\\ downloads wesad loaded w pd.read_pickle in  24.78 s\\n\\n\\n#this is the one that fails\\nload_ws_path = wesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\\nnow = time()\\nwith open(load_ws_path, 'rb') as file:\\n            ws_df = pickle.load(file, encoding='latin1')\\nprint('wesad loaded in ', round(time()-now,3),'s')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "subject_id = 2\n",
    "snr = 0.01\n",
    "n_i = 0\n",
    "\n",
    "\n",
    "\n",
    "wesad_path = '/mnt/d/Users/alkurdi/data/WESAD'\n",
    "freshwesad_path = '/mnt/d/Users/alkurdi/data/freshWESAD'\n",
    "downloaddwesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data'\n",
    "gn_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "sesh_path = '/n_'+str(n_i)+'/snr_'+str(snr)+'/S'+str(subject_id)\n",
    "load_gn_path =  gn_path + sesh_path + '/S'+str(subject_id)+'.pkl'\n",
    "load_dw_path = downloaddwesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "load_ws_path = wesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "load_freshws_path = freshwesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "\n",
    "#ws_df = pd.read_pickle(load_ws_path)\n",
    "\n",
    "#with open( load_ws_path, 'rb') as dest:\n",
    "#    ws_df = pickle.load(dest)\n",
    "\n",
    "now = time()\n",
    "with open(load_dw_path, 'rb') as file:\n",
    "            dw_df = pickle.load(file, encoding='latin1')\n",
    "print('c:\\ downloads wesad loaded w pickle.load in ', round(time()-now,3),'s') #c:\\ downloads wesad loaded w pickle.load in  17.112 s\n",
    "\n",
    "\n",
    "now = time()\n",
    "with open(load_gn_path, 'rb') as file:\n",
    "            gn_df = pickle.load(file, encoding='latin1')\n",
    "print('gnwesad loaded in ', round(time()-now,3),'s')\n",
    "\n",
    "'''\n",
    "now = time()\n",
    "with open(load_freshws_path, 'rb') as file:\n",
    "            freshws_df = pickle.load(file, encoding='latin1')\n",
    "print('d:\\ freshwesad loaded in ', round(time()-now,3),'s')  #d:\\ freshwesad loaded in  191.335 s\n",
    "\n",
    "now = time()\n",
    "dw_df = pd.read_pickle(load_dw_path)\n",
    "print('c:\\ downloads wesad loaded w pd.read_pickle in ', round(time()-now,3),'s') #c:\\ downloads wesad loaded w pd.read_pickle in  24.78 s\n",
    "\n",
    "\n",
    "#this is the one that fails\n",
    "load_ws_path = wesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "now = time()\n",
    "with open(load_ws_path, 'rb') as file:\n",
    "            ws_df = pickle.load(file, encoding='latin1')\n",
    "print('wesad loaded in ', round(time()-now,3),'s')\n",
    "'''\n",
    "#fresh_df = pd.read_pickle() \n",
    "#wesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/WESAD'\n",
    "#msg = f'starting  n_i: {n_i}; snr: {snr}, id: {subject_id}. iteration'\n",
    "#sesh_path = '/n_'+str(n_i)+'/snr_'+str(snr)+'/S'+str(subject_id)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampled_gn loaded in  0.052 s\n"
     ]
    }
   ],
   "source": [
    "now = time()\n",
    "with open('/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.6/S17/fixed_resampled140hz_S17.pkl', 'rb') as file:\n",
    "            resampled_gn = pickle.load(file, encoding='latin1')\n",
    "print('resampled_gn loaded in ', round(time()-now,3),'s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw_df: length of ACC is 4,255,300\n",
      "dw_df: length of ECG is 4,255,300\n",
      "dw_df: length of EMG is 4,255,300\n",
      "dw_df: length of EDA is 4,255,300\n",
      "dw_df: length of Temp is 4,255,300\n",
      "dw_df: length of Resp is 4,255,300\n",
      "dw_df: length of ACC is 194,528\n",
      "dw_df: length of BVP is 389,056\n",
      "dw_df: length of EDA is 24,316\n",
      "dw_df: length of TEMP is 24,316\n",
      "dw_df: length of label is 4,255,300\n",
      "_____\n",
      "total length of dw_df df is 39,318,972\n",
      "_________________________\n",
      "gs_df: length of ACC is 194,528\n",
      "gs_df: length of BVP is 389,056\n",
      "gs_df: length of EDA is 24,316\n",
      "gs_df: length of TEMP is 24,316\n",
      "gs_df: length of ACC_C is 4,255,300\n",
      "gs_df: length of ECG is 4,255,300\n",
      "gs_df: length of EDA_C is 4,255,300\n",
      "gs_df: length of EMG_C is 4,255,300\n",
      "gs_df: length of Resp_C is 4,255,300\n",
      "gs_df: length of Temp_C is 4,255,300\n",
      "_____\n",
      "total length of gs_df df is 35,063,672\n",
      "_________________________\n",
      "resampled_gn: length of ACC is 828,800\n",
      "resampled_gn: length of ECG is 828,800\n",
      "resampled_gn: length of EMG is 8,289\n",
      "resampled_gn: length of EDA is 8,289\n",
      "resampled_gn: length of Temp is 8,289\n",
      "resampled_gn: length of Resp is 828,800\n",
      "resampled_gn: length of ACC is 828,800\n",
      "resampled_gn: length of BVP is 378,880\n",
      "resampled_gn: length of EDA is 23,680\n",
      "resampled_gn: length of TEMP is 23,680\n",
      "resampled_gn: length of label is 828,800\n",
      "_____\n",
      "total length of resampled_gn df is 7,910,307\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def my_counter(df, name= None):\n",
    "    i= 0\n",
    "    long_list = []\n",
    "    for i_first_layer in df.keys():\n",
    "        #print(i_first_layer, dw_df[i_first_layer]) \n",
    "        if i_first_layer == 'label':\n",
    "            long_list = long_list + [*df[i_first_layer]]\n",
    "            i += len(df[i_first_layer])\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "        elif i_first_layer == 'signal':\n",
    "            for i_second_layer in df[i_first_layer].keys():\n",
    "                #print('part', i_second_layer)#, dw_df[i_first_layer][i_second_layer])\n",
    "\n",
    "                for i_third_layer in df[i_first_layer][i_second_layer].keys():\n",
    "                    sig_shape = np.shape(df[i_first_layer][i_second_layer][i_third_layer])\n",
    "                    #print('part', i_second_layer, 'signal', i_third_layer, sig_shape)\n",
    "                    i += np.prod(sig_shape)\n",
    "                    long_list = long_list + [*df[i_first_layer][i_second_layer][i_third_layer].flatten()]\n",
    "                    print(f'{name}: length of {i_third_layer} is {len(df[i_first_layer][i_second_layer][i_third_layer]):,.0f}')\n",
    "        elif i_first_layer == 'subject':\n",
    "            pass\n",
    "        else:\n",
    "            long_list = long_list + [*df[i_first_layer].flatten()]\n",
    "            i += np.prod(np.shape(df[i_first_layer]))\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "    print('_'*5)\n",
    "    print(f'total length of {name} df is {len(long_list):,.0f}')\n",
    "    print('_'*25)\n",
    "\n",
    "                \n",
    "my_counter(dw_df, name='dw_df')\n",
    "my_counter(gn_df, name='gs_df')\n",
    "my_counter(resampled_gn, name='resampled_gn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1202\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'python\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "print(os.getpid(), file=sys.stderr)\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "\n",
    "from subprocess import PIPE, Popen\n",
    "def get_cmd(pid):\n",
    "    with Popen(f\"ps -q {pid} -o comm=\", shell=True, stdout=PIPE) as p:\n",
    "        return p.communicate()[0]\n",
    "get_cmd(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length wesad 26,164,016\n"
     ]
    }
   ],
   "source": [
    "long_list = list(dw_df['signal']['chest'].items()) + list(dw_df['signal']['wrist'].items()) \n",
    "a, b = zip(*long_list)\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb Cell 59\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39;49mlistdir(path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2'"
     ]
    }
   ],
   "source": [
    "path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length gn wesad 26,164,016\n"
     ]
    }
   ],
   "source": [
    "a,b = 0,0\n",
    "a, b = zip(*list(gn_df.items()))\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length gn wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wesad_path = '/mnt/c/Users/alkurdi/data/WESAD'\n",
    "fresh_wesad_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/WESAD'\n",
    "\n",
    "#start_time = time()\n",
    "id = 2\n",
    "sesh_path = fresh_wesad_path + '/S'+str(id)+'/S'+str(id)+'.pkl'\n",
    "\n",
    "ws_df = pd.read_pickle(sesh_path)\n",
    "#with open( sesh_path, 'rb') as dest:\n",
    "#        ws2_df = pickle.load(dest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_resampled_df = pd.read_pickle( '/mnt/d/Users/alkurdi/data/GN-WESAD/n_0/snr_0.01/S17/fixed_resampled170hz64_S17.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in fixed_resampled_df.keys():\n",
    "    \n",
    "    print(f'length of signal {key}: ', len(fixed_resampled_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys', ws_df.keys())\n",
    "print('keys', ws_df['signal'].keys())\n",
    "print('length of signal', len(ws_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fixed_resampled_df['signal']['chest']['ECG'])/len(ws_df['signal']['chest']['ECG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import signal\n",
    "from time import time\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "n_samples = 0 #10\n",
    "n_i = n_samples\n",
    "snr1 = 0.01\n",
    "snr2 = 0.6\n",
    "subject_id = 17\n",
    "\n",
    "sesh_id =[n_i,snr1,subject_id]\n",
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "start_time = time()\n",
    "onedrive = '/mnt/d/Users/alkurdi/OneDrive - University of Illinois - Urbana/data/GN-WESAD'\n",
    "\n",
    "#print(os.path.isfile(rt_pth + '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(17)+'/S'+str(17)+'.pkl'))\n",
    "sesh_path = rt_pth + '/n_'+str(0)+'/snr_'+str(0.0001)+ '/S'+str(2)+'/S'+str(2)+'.pkl'\n",
    "#drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "with open( sesh_path, 'rb') as dest:\n",
    "        ws1_df = pickle.load(dest) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "#start_time = time()\n",
    "id = 4\n",
    "sesh_path = rt_pth + '/n_'+str(1)+'/snr_'+str(0.001)+ '/S'+str(id)+'/S'+str(id)+'.pkl'\n",
    "#drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "\n",
    "with open( sesh_path, 'rb') as dest:\n",
    "        ws2_df = pickle.load(dest) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in ws2_df.keys():\n",
    "    print('key', key)   \n",
    "    #print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))\n",
    "    #print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "#snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "snrs = [ 0.01, 0.05, 0.3]#,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] \n",
    "n_i = [0,1,3,4] # next is [5, 6, 7] [8, 9, 10] \n",
    "\n",
    "\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "to_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "\n",
    "for n in n_i:\n",
    "    for snr in snrs:\n",
    "        for s in subject_ids:\n",
    "            sesh_path = '/n_'+str(n_i)+'/snr_'+str(snr)+'/S'+str(subject_id)\n",
    "            \n",
    "            xact_from_path = from_path + sesh_path\n",
    "            xact_to_path = to_path+ sesh_path\n",
    "            \n",
    "            os.system(f'cp -r {from_path} {to_path}')\n",
    "            print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait for 5 seconds\n",
    "from time import sleep\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD'\n",
    "poop = 'poop'\n",
    "\n",
    "with open(from_path+'/poop.txt', 'w') as f:\n",
    "    f.write(poop)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "#os.system(f'{from_path}/poop>poop.txt')\n",
    "\n",
    "to_path = '/mnt/d/Users/alkurdi/data/'\n",
    "\n",
    "os.system(f'mv {from_path}/poop.txt {to_path}/poop.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/alkurdi/Downloads/WESADpoop.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_path+'poop.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
