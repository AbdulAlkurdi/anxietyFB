{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fb testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle\n",
    "import dask as pd\n",
    "import pandas as pd_old\n",
    "import warnings\n",
    "import utils\n",
    "import heartpy as hp\n",
    "from ECG_feature_extractor_1001 import *\n",
    "# import time \n",
    "import time\n",
    "from datetime import datetime\n",
    "from biosppy.signals import ecg\n",
    "from feature_extraction import SubjectData, compute_features, get_samples, combine_files\n",
    "\n",
    "# To ignore all warnings:\n",
    "warnings.filterwarnings(\"ignore\", module=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_IN_SECONDS = 60\n",
    "stride = 1\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "feat_names = None\n",
    "loadPath = '../data/WESAD'\n",
    "savePath = '../data/GN-WESAD'\n",
    "subject_feature_path = '/subject_feats'\n",
    "\n",
    "n_samples = 10\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6]  # 0.00001,\n",
    "fb_model_list = ['DT', 'RF', 'LDA', 'KNN', 'AdaBoost', 'SVM']\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_status(snrs, subject_ids, onedrive, n_samples=10):\n",
    "    # bads are the ones that do not have the gaussian-modified data.\n",
    "    bads = []\n",
    "    bad_snrs = []\n",
    "    bad_subjects = []\n",
    "    bad_ns = []\n",
    "    completed_snrs = []\n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            for subject_id in subject_ids:\n",
    "                # print(snr)\n",
    "\n",
    "                # print(f'{onedrive}/n_{n_i}/snr_{snr}/S{subject_id}/{a}')\n",
    "                try:\n",
    "                    a = os.listdir(f'{onedrive}/n_{n_i}/snr_{str(snr)}/S{subject_id}')\n",
    "                    a[0]\n",
    "                    completed_snrs.append(snr)\n",
    "                except:\n",
    "                    bads.append(f'n_{n_i}/snr_{snr}/S{subject_id}')\n",
    "                    bad_snrs.append(snr)\n",
    "                    bad_subjects.append(subject_id)\n",
    "                    bad_ns.append(n_i)\n",
    "\n",
    "    bad_snrs = sorted(set(bad_snrs))\n",
    "    bad_subjects = sorted(set(bad_subjects))\n",
    "    bad_ns = sorted(set(bad_ns))\n",
    "    completed_snrs = sorted(set(completed_snrs))\n",
    "    # printing after checking\n",
    "    print(f'completed snrs :{completed_snrs}')\n",
    "    print(f'incomplete snrs :{bad_snrs}')\n",
    "\n",
    "\n",
    "get_processing_status(snrs, subject_ids, onedrive, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "\n",
    "for each_dataset in dataset_list:\n",
    "    for snr in snrs:\n",
    "        for model in fb_model_list:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "savePath = 'C:/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "n_samples = [9]\n",
    "subject_ids = [2]\n",
    "snrs = [0.01]\n",
    "for n_i in n_samples:\n",
    "    if not os.path.exists(savePath + '/n_' + str(n_i)):\n",
    "        os.makedirs(savePath + '/n_' + str(n_i))\n",
    "    for snr in snrs:\n",
    "        if not os.path.exists(savePath + '/n_' + str(n_i) + '/snr_' + str(snr)):\n",
    "            os.makedirs(savePath + '/n_' + str(n_i) + '/snr_' + str(snr))\n",
    "        for subject_id in subject_ids:\n",
    "            if not os.path.exists(\n",
    "                savePath\n",
    "                + '/n_'\n",
    "                + str(n_i)\n",
    "                + '/snr_'\n",
    "                + str(snr)\n",
    "                + '/S'\n",
    "                + str(subject_id)\n",
    "            ):\n",
    "                os.makedirs(\n",
    "                    savePath\n",
    "                    + '/n_'\n",
    "                    + str(n_i)\n",
    "                    + '/snr_'\n",
    "                    + str(snr)\n",
    "                    + '/S'\n",
    "                    + str(subject_id)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{savePath}/n_{n_i}')\n",
    "print(os.path.isdir(f'{savePath}/n_{n_i}'))\n",
    "print(os.listdir(f'{savePath}/n_{n_i}'))\n",
    "print(\n",
    "    os.path.isdir(\n",
    "        f'{savePath}/n_{n_i}/snr_{snr}/fixed_resampled140hz_S{subject_id}.pkl'\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(f'{savePath}/n_{n_i}/poop.txt', 'w') as f:\n",
    "    f.write('poop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd_old.DataFrame(columns=['SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "results_table.loc[str('SVM')] = pd_old.Series(\n",
    "    {'SNR': 1, 'Accuracy': 5, 'F1 Score': 2, 'dataset': 'WESAD'}\n",
    ")\n",
    "results_table.loc[str('RF')] = pd_old.Series(\n",
    "    {'SNR': 1, 'Accuracy': 5, 'F1 Score': 2, 'dataset': 'WESAD'}\n",
    ")\n",
    "'''\n",
    "fb_model_list = ['SVM', 'RF']\n",
    "for model in fb_model_list:\n",
    "    for i in range(len(snrs)):\n",
    "        results_table.loc[str(model) + str(snrs[i])] = pd.Series({'SNR':snrs[i], 'Accuracy':svm_accuracy[i], 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "ecg_biosppy = ecg\n",
    "ecg = None\n",
    "fs_ecg = 700\n",
    "fs_ppg = 64\n",
    "subject_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = SubjectData(main_path=loadPath, subject_number=subject_id)\n",
    "data_dict = subject.get_wrist_and_chest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "BS_signal_analysis3, pack3, ecg_out3 = analyze_ecg(ecg, fs_ecg)\n",
    "print('analyze_ecg execution time is ', now - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for PPG\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR, time_dict = freq_ratio(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, time_dict = freq_ratio_hybrid(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio_hybrid execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "BS_signal_analysis, pack, ppg_out = analyze_ecg(ppg, fs_ppg)\n",
    "print('analyze_ecg execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "wd, m = hp.process(ppg, fs_ppg)\n",
    "print('hp.process execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR = freq_ratio_fast(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio_fast execution time is ', now - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_noiZ_files(subject_ids)\n",
    "# took 19m 30s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def process_subject_file(snr, n_i, s):\n",
    "    file_path = f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{s}_feats.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df['subject'] = s\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "\n",
    "def combine_noiZ_files(subjects):\n",
    "    now = datetime.now().strftime('%Y-%m-%d')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, filename=now + '-combine.log', filemode='w', force=True\n",
    "    )\n",
    "\n",
    "    for snr in snrs:\n",
    "        for n_i in range(n_samples):\n",
    "            # Parallelize file reading\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(process_subject_file, snr, n_i, s) for s in subjects\n",
    "                ]\n",
    "                df_list = [\n",
    "                    future.result() for future in futures if future.result() is not None\n",
    "                ]\n",
    "\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['label'] = df[['0', '1', '2']].idxmax(axis=1)\n",
    "                df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                df.to_csv(\n",
    "                    f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv'\n",
    "                )\n",
    "                logging.info('-' * 20)\n",
    "                logging.info(\n",
    "                    f'Saved file to: {savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv'\n",
    "                )\n",
    "\n",
    "                counts = df['label'].value_counts()\n",
    "                logging.info('Number of samples per class:')\n",
    "                logging.info(\n",
    "                    'baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(\n",
    "                        *list(zip(counts.index, counts.values))\n",
    "                    )\n",
    "                )\n",
    "    logging.info('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i = 5\n",
    "s = 2\n",
    "snr = 0.6\n",
    "df = pd_old.read_csv(\n",
    "    f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{subject_id}_feats.csv',\n",
    "    index_col=0,\n",
    ")\n",
    "df['subject'] = s\n",
    "# df_list.append(df)\n",
    "# f = pd_old.concat(df_list)\n",
    "df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(\n",
    "    lambda x: x.index('1')\n",
    ")\n",
    "df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "counts = df['label'].value_counts()\n",
    "print('Number of samples per class:')\n",
    "print(\n",
    "    'baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(\n",
    "        *list(zip(counts.index, counts.values))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    # print(os.listdir(path))\n",
    "    df = pd_old.read_csv(path, index_col=0)\n",
    "    print('len of df ', len(df))\n",
    "    pd_old.set_option('display.max_columns', None)\n",
    "    # We want to drop columns in df that are not in RADWear to match modalities.\n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    # df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if (\n",
    "            'EMG' in column\n",
    "            or 'EDA_C' in column\n",
    "            or 'Temp_C' in column\n",
    "            or 'TEMP_C' in column\n",
    "            or 'SCR_C' in column\n",
    "            or 'SCL_C' in column\n",
    "        ):\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    print('len of reduced df ', len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def gn_wesad_path(n_i, snr):\n",
    "    loadPath = '../data/GN-WESAD'\n",
    "    return (\n",
    "        f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "    )\n",
    "\n",
    "\n",
    "gn_wesad_day = '2023-11-13'\n",
    "matrix = np.zeros((len(snrs), n_samples))\n",
    "\n",
    "for n_i in range(n_samples):\n",
    "    for i, snr in enumerate(snrs):\n",
    "        print(f'for {n_i} and {snr} number {i}: ')\n",
    "        file_path = gn_wesad_path(n_i, snr)\n",
    "        df = read_n_reduce(file_path)\n",
    "        matrix[i][n_i] = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "'data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "loadPath = '../data/GN-WESAD'\n",
    "# display(os.listdir(f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}'))\n",
    "snr0001 = pd_old.read_csv(\n",
    "    '../data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-13_feats2.csv', index_col=0\n",
    ")\n",
    "snr001 = pd_old.read_csv(\n",
    "    '../data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-13_feats2.csv', index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'SNR': [0.6] * 10,\n",
    "        'Accuracy': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'F1 Score': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'dataset': ['GN-WESAD'] * 10,\n",
    "        'n_i': list(range(10)),\n",
    "        'n': [10] * 10,\n",
    "        'noise gen function': ['Gaussian Noise'] * 10,\n",
    "        'Precision': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Recall': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Model': ['DT'] * 10,\n",
    "    }\n",
    ")\n",
    "series = pd.Series(\n",
    "    {\n",
    "        'SNR': 0.600000,\n",
    "        'Accuracy': 0.945747,\n",
    "        'F1 Score': 0.945747,\n",
    "        'n_i': 4.500000,\n",
    "        'n': 10.000000,\n",
    "        'Precision': 0.945747,\n",
    "        'Recall': 0.945747,\n",
    "    },\n",
    "    name='SNR 0.6 Model DT mean',\n",
    ")\n",
    "\n",
    "print(set(df.columns))\n",
    "print(set(series.index))\n",
    "missing_columns = set(df.columns) - set(series.index)\n",
    "print(missing_columns)\n",
    "for col in missing_columns:\n",
    "    # You can assign a specific value or use a value from the DataFrame\n",
    "    # Here, I'm using the first row's value as an example\n",
    "    # print(col)\n",
    "    # print(df[col].iloc[0])\n",
    "\n",
    "    series[col] = df[col].iloc[0]\n",
    "    # print(series[col])\n",
    "\n",
    "# print(series)\n",
    "\n",
    "series_df2 = series.to_frame().T\n",
    "display(series_df2)\n",
    "df_combined2 = pd.concat([df, series_df2])\n",
    "display(df_combined2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate and plot\n",
    "\n",
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "with open('../data/WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    WESAD_cm_cr_dict = pickle.load(handle)\n",
    "wesad_acc = pd.read_csv(\n",
    "    '../data/WESAD/wesad_models_results-win60stride1_wcm_wcr.csv', index_col=0\n",
    ")\n",
    "display(wesad_acc)\n",
    "# combined_results = pd.concat([WESAD_model_results, GN_model_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wbinaryf1.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "# gn_wesad_acc['Binary F1'] = None\n",
    "print(gn_wesad_acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wcm_wcr.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "gn_wesad_acc['Binary F1'] = None\n",
    "\n",
    "for i, classification_report in enumerate(GN_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    print(classification_report['id'][2], fb_model_list[i % 6])\n",
    "    # print(cr)\n",
    "    # print(i//5)\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    # print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        # print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i % 6]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i % 6]\n",
    "        secret_df = pd.concat(\n",
    "            [\n",
    "                secret_df,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        'Model': fb_model_list[i % 6],\n",
    "                        'Binary F1 Score': binary_f1_score,\n",
    "                        'SNR': classification_report['id'][0],\n",
    "                        'n': classification_report['id'][1],\n",
    "                        'Model': classification_report['id'][2],\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        # print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        # display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        # print(fb_model_list[i])\n",
    "        gn_wesad_acc.loc[\n",
    "            gn_wesad_acc['Model'] == fb_model_list[i % 6], 'Binary f1'\n",
    "        ] = binary_f1_score\n",
    "        # print(gn_wesad_acc.loc[gn_wesad_acc['Model'] == fb_model_list[i], 'binary f1'])\n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wesad_acc_test = wesad_acc.copy()\n",
    "wesad_acc_test['Binary F1'] = None\n",
    "\n",
    "\n",
    "def extract_metrics(report):\n",
    "    # Regular expression to find numeric values\n",
    "    regex = r\"\\s+1\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\"\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = re.search(regex, report)\n",
    "    # print(match)\n",
    "    if match:\n",
    "        precision = float(match.group(1))\n",
    "        recall = float(match.group(2))\n",
    "        f1_score = float(match.group(3))\n",
    "        return precision, recall, f1_score\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def calculate_binary_metrics(report):\n",
    "    precision, recall, f1_score = extract_metrics(report)\n",
    "\n",
    "    if precision is not None and recall is not None:\n",
    "        # Calculate binary F1 score for class 1\n",
    "        binary_f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return binary_f1_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# classification_report =\n",
    "# Extract and calculate binary F1 score\n",
    "fb_model_counter = fb_model_list\n",
    "\n",
    "\n",
    "secret_df = pd.DataFrame()\n",
    "\n",
    "for i, classification_report in enumerate(WESAD_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    # print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i]\n",
    "        secret_df = pd.concat(\n",
    "            [\n",
    "                secret_df,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        'Model': fb_model_list[i],\n",
    "                        'Binary F1 Score': binary_f1_score,\n",
    "                        'SNR': classification_report['id'][0],\n",
    "                        'n': classification_report['id'][1],\n",
    "                        'Model': classification_report['id'][2],\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        # print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        # display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        # print(fb_model_list[i])\n",
    "        wesad_acc_test.loc[\n",
    "            wesad_acc_test['Model'] == fb_model_list[i], 'binary f1'\n",
    "        ] = binary_f1_score\n",
    "        print(\n",
    "            wesad_acc_test.loc[wesad_acc_test['Model'] == fb_model_list[i], 'binary f1']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")\n",
    "display(secret_df)\n",
    "display(wesad_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([wesad_acc, secret_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # We want to drop columns in df that are not in RADWear to match modalities.\n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    # df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if (\n",
    "            'EMG' in column\n",
    "            or 'EDA_C' in column\n",
    "            or 'Temp_C' in column\n",
    "            or 'TEMP_C' in column\n",
    "            or 'SCR_C' in column\n",
    "            or 'SCL_C' in column\n",
    "        ):\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    return df\n",
    "\n",
    "\n",
    "print(os.listdir('../data/WESAD'))\n",
    "df = read_n_reduce('../data/WESAD//subject_feats/oct5_feats4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# display(the_splits[0])\n",
    "X_train, X_test, y_train, y_test = the_splits\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_out = clf.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr in snrs:\n",
    "    for n_i in range(n_samples):\n",
    "        tgt = f'../data/GN-WESAD/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "        print(f'df {snr} n {n_i} shape is {(pd.read_csv(tgt, index_col=0)).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wesad loaded in  225.43 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "subject_id = 2\n",
    "snr = 0.01\n",
    "n_i = 0\n",
    "\n",
    "\n",
    "wesad_path = '/mnt/d/Users/alkurdi/data/WESAD'\n",
    "#freshwesad_path = '/mnt/d/Users/alkurdi/data/freshWESAD'\n",
    "downloaddwesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/WESAD'\n",
    "gn_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "sesh_path = '/n_' + str(n_i) + '/snr_' + str(snr) + '/S' + str(subject_id)\n",
    "load_gn_path = gn_path + sesh_path + '/S' + str(subject_id) + '.pkl'\n",
    "#load_dw_path = (\n",
    "#    downloaddwesad_path + '/S' + str(subject_id) + '/S' + str(subject_id) + '.pkl'\n",
    "#)\n",
    "\n",
    "load_ws_path = wesad_path + '/S' + str(subject_id) + '/S' + str(subject_id) + '.pkl'\n",
    "\n",
    "# ws_df = pd.read_pickle(load_ws_path)\n",
    "\n",
    "# with open( load_ws_path, 'rb') as dest:\n",
    "#    ws_df = pickle.load(dest)\n",
    "\n",
    "now = time()\n",
    "'''\n",
    "with open(load_dw_path, 'rb') as file:\n",
    "    dw_df = pickle.load(file, encoding='latin1')\n",
    "print(\n",
    "    'c:\\ downloads wesad loaded w pickle.load in ', round(time() - now, 3), 's'\n",
    ")  # c:\\ downloads wesad loaded w pickle.load in  17.112 s\n",
    "'''\n",
    "'''\n",
    "now = time()\n",
    "with open(load_gn_path, 'rb') as file:\n",
    "    gn_df = pickle.load(file, encoding='latin1')\n",
    "print('gnwesad loaded in ', round(time() - now, 3), 's')\n",
    "\n",
    "\n",
    "now = time()\n",
    "#with open(load_freshws_path, 'rb') as file:\n",
    "#            freshws_df = pickle.load(file, encoding='latin1')\n",
    "print('d:\\ freshwesad loaded in ', round(time()-now,3),'s')  #d:\\ freshwesad loaded in  191.335 s\n",
    "\n",
    "now = time()\n",
    "dw_df = pd.read_pickle(load_dw_path)\n",
    "print('c:\\ downloads wesad loaded w pd.read_pickle in ', round(time()-now,3),'s') #c:\\ downloads wesad loaded w pd.read_pickle in  24.78 s\n",
    "'''\n",
    "\n",
    "#this is the one that fails\n",
    "#load_ws_path = wesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "now = time()\n",
    "with open(load_ws_path, 'rb') as file:\n",
    "            ws_df = pickle.load(file, encoding='latin1')\n",
    "print('wesad loaded in ', round(time()-now,3),'s')\n",
    "\n",
    "# fresh_df = pd.read_pickle()\n",
    "# wesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/WESAD'\n",
    "# msg = f'starting  n_i: {n_i}; snr: {snr}, id: {subject_id}. iteration'\n",
    "# sesh_path = '/n_'+str(n_i)+'/snr_'+str(snr)+'/S'+str(subject_id)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws.keys():  dict_keys(['signal', 'label', 'subject'])\n",
      "signal keys: dict_keys(['chest', 'wrist'])\n",
      "chest keys: dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "wrist keys: dict_keys(['ACC', 'BVP', 'EDA', 'TEMP'])\n"
     ]
    }
   ],
   "source": [
    "print('ws.keys(): ',ws_df.keys())\n",
    "print(f'signal keys: {ws_df[\"signal\"].keys()}')\n",
    "print(f'chest keys: {ws_df[\"signal\"][\"chest\"].keys()}')\n",
    "print(f'wrist keys: {ws_df[\"signal\"][\"wrist\"].keys()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "print((ws_df['label']))\n",
    "numpy.array(ws_df['label'])\n",
    "print(type(ws_df[\"signal\"][\"wrist\"]['ACC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time()\n",
    "with open(\n",
    "    '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.6/S17/fixed_resampled140hz_S17.pkl',\n",
    "    'rb',\n",
    ") as file:\n",
    "    resampled_gn = pickle.load(file, encoding='latin1')\n",
    "print('resampled_gn loaded in ', round(time() - now, 3), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def my_counter(df, name=None):\n",
    "    i = 0\n",
    "    long_list = []\n",
    "    for i_first_layer in df.keys():\n",
    "        # print(i_first_layer, dw_df[i_first_layer])\n",
    "        if i_first_layer == 'label':\n",
    "            long_list = long_list + [*df[i_first_layer]]\n",
    "            i += len(df[i_first_layer])\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "        elif i_first_layer == 'signal':\n",
    "            for i_second_layer in df[i_first_layer].keys():\n",
    "                # print('part', i_second_layer)#, dw_df[i_first_layer][i_second_layer])\n",
    "\n",
    "                for i_third_layer in df[i_first_layer][i_second_layer].keys():\n",
    "                    sig_shape = np.shape(\n",
    "                        df[i_first_layer][i_second_layer][i_third_layer]\n",
    "                    )\n",
    "                    # print('part', i_second_layer, 'signal', i_third_layer, sig_shape)\n",
    "                    i += np.prod(sig_shape)\n",
    "                    long_list = long_list + [\n",
    "                        *df[i_first_layer][i_second_layer][i_third_layer].flatten()\n",
    "                    ]\n",
    "                    print(\n",
    "                        f'{name}: length of {i_third_layer} is {len(df[i_first_layer][i_second_layer][i_third_layer]):,.0f}'\n",
    "                    )\n",
    "        elif i_first_layer == 'subject':\n",
    "            pass\n",
    "        else:\n",
    "            long_list = long_list + [*df[i_first_layer].flatten()]\n",
    "            i += np.prod(np.shape(df[i_first_layer]))\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "    print('_' * 5)\n",
    "    print(f'total length of {name} df is {len(long_list):,.0f}')\n",
    "    print('_' * 25)\n",
    "\n",
    "\n",
    "my_counter(dw_df, name='dw_df')\n",
    "my_counter(gn_df, name='gs_df')\n",
    "my_counter(resampled_gn, name='resampled_gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(os.getpid(), file=sys.stderr)\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "\n",
    "def get_cmd(pid):\n",
    "    with Popen(f\"ps -q {pid} -o comm=\", shell=True, stdout=PIPE) as p:\n",
    "        return p.communicate()[0]\n",
    "\n",
    "\n",
    "get_cmd(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_list = list(dw_df['signal']['chest'].items()) + list(\n",
    "    dw_df['signal']['wrist'].items()\n",
    ")\n",
    "a, b = zip(*long_list)\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 0\n",
    "a, b = zip(*list(gn_df.items()))\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length gn wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wesad_path = '/mnt/c/Users/alkurdi/data/WESAD'\n",
    "fresh_wesad_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/WESAD'\n",
    "\n",
    "# start_time = time()\n",
    "id = 2\n",
    "sesh_path = fresh_wesad_path + '/S' + str(id) + '/S' + str(id) + '.pkl'\n",
    "\n",
    "ws_df = pd.read_pickle(sesh_path)\n",
    "# with open( sesh_path, 'rb') as dest:\n",
    "#        ws2_df = pickle.load(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_resampled_df = pd.read_pickle(\n",
    "    '/mnt/d/Users/alkurdi/data/GN-WESAD/n_0/snr_0.01/S17/fixed_resampled170hz64_S17.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in fixed_resampled_df.keys():\n",
    "    print(\n",
    "        f'length of signal {key}: ', len(fixed_resampled_df['signal']['chest']['ECG'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys', ws_df.keys())\n",
    "print('keys', ws_df['signal'].keys())\n",
    "print('length of signal', len(ws_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fixed_resampled_df['signal']['chest']['ECG']) / len(ws_df['signal']['chest']['ECG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import signal\n",
    "from time import time\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "n_samples = 0 #10\n",
    "n_i = n_samples\n",
    "snr1 = 0.01\n",
    "snr2 = 0.6\n",
    "subject_id = 17\n",
    "\n",
    "sesh_id =[n_i,snr1,subject_id]\n",
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "start_time = time()\n",
    "onedrive = '/mnt/d/Users/alkurdi/OneDrive - University of Illinois - Urbana/data/GN-WESAD'\n",
    "\n",
    "#print(os.path.isfile(rt_pth + '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(17)+'/S'+str(17)+'.pkl'))\n",
    "sesh_path = rt_pth + '/n_'+str(0)+'/snr_'+str(0.0001)+ '/S'+str(2)+'/S'+str(2)+'.pkl'\n",
    "#drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "with open( sesh_path, 'rb') as dest:\n",
    "        ws1_df = pickle.load(dest) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "# start_time = time()\n",
    "id = 4\n",
    "sesh_path = (\n",
    "    rt_pth\n",
    "    + '/n_'\n",
    "    + str(1)\n",
    "    + '/snr_'\n",
    "    + str(0.001)\n",
    "    + '/S'\n",
    "    + str(id)\n",
    "    + '/S'\n",
    "    + str(id)\n",
    "    + '.pkl'\n",
    ")\n",
    "# drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "\n",
    "with open(sesh_path, 'rb') as dest:\n",
    "    ws2_df = pickle.load(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in ws2_df.keys():\n",
    "    print('key', key)\n",
    "    # print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))\n",
    "    # print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "# snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "snrs = [0.01, 0.05, 0.3]  # ,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6]\n",
    "n_i = [0, 1, 3, 4]  # next is [5, 6, 7] [8, 9, 10]\n",
    "\n",
    "\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "to_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "\n",
    "for n in n_i:\n",
    "    for snr in snrs:\n",
    "        for s in subject_ids:\n",
    "            sesh_path = '/n_' + str(n_i) + '/snr_' + str(snr) + '/S' + str(subject_id)\n",
    "\n",
    "            xact_from_path = from_path + sesh_path\n",
    "            xact_to_path = to_path + sesh_path\n",
    "\n",
    "            os.system(f'cp -r {from_path} {to_path}')\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for 5 seconds\n",
    "from time import sleep\n",
    "\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD'\n",
    "poop = 'poop'\n",
    "\n",
    "with open(from_path + '/poop.txt', 'w') as f:\n",
    "    f.write(poop)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# os.system(f'{from_path}/poop>poop.txt')\n",
    "\n",
    "to_path = '/mnt/d/Users/alkurdi/data/'\n",
    "\n",
    "os.system(f'mv {from_path}/poop.txt {to_path}/poop.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RADWear Calibration fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'modified by abdul alkurdi; 10/05/2023'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import cudf\n",
    "import pickle, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# import cupy as cp\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal, stats\n",
    "\n",
    "# import peakutils, wfdb, pywt\n",
    "import csv\n",
    "import os, statistics\n",
    "from datetime import datetime\n",
    "\n",
    "# import heartpy as hp\n",
    "import json\n",
    "\n",
    "# import neurokit2 as nk\n",
    "\n",
    "# add path to sys\n",
    "sys.path.append('../')\n",
    "from syncfcns import *\n",
    "from process_redcap import process_redcap\n",
    "calib_dict = {'meditation': 1, 'cpt': 2, 'baseline': 0}\n",
    "rot_anx_dict = {'calibration': 0, 'LA': 1, 'HA': 2}\n",
    "fs_hx = {'ECG': 256, 'ACC': 64, 'BR': 1, 'RESP': 128, 'label': 256} # for RADWear, Wear\n",
    "fs_e4 = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4} # for RADWear, Wear\n",
    "\n",
    "radwear_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 16\n",
    "p_path = radwear_path + 'Participant ' + str(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(radwear_path + 'all_p_metadata.json', 'rb') as f:\n",
    "    all_p_metadata = json.load(f)\n",
    "\n",
    "incomplete = [16]\n",
    "\n",
    "\n",
    "for p in incomplete:\n",
    "    p_path = radwear_path + 'Participant ' + str(p)\n",
    "    p_df = pd.DataFrame()\n",
    "\n",
    "    # check if file exist\n",
    "    a = (\n",
    "        'available.'\n",
    "        if os.path.isfile(p_path + '/p_' + str(p) + '.pkl')\n",
    "        else ' not available.'\n",
    "    )\n",
    "    print('pickle file for participant ' + str(p) + ' is ' + a)\n",
    "\n",
    "    # all_p[p] = p_data ## this takes too much memory so i will just load each p when needed\n",
    "    if not (os.path.isfile(radwear_path + 'p_' + str(p) + '.pkl')) or True:\n",
    "        with open(p_path + '/p_' + str(p) + '.pkl', 'rb') as f:\n",
    "            p_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap_path = radwear_path + 'REDCap responses/'\n",
    "with open(radwear_path + 'all_p_metadata.json', 'rb') as f:\n",
    "    all_p_metadata = json.load(f)\n",
    "# load all participant redcap data\n",
    "redcap_calib_dict = pd.read_pickle(\n",
    "    radwear_path + '/REDCap responses/redcap_calib_dict.pkl'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e4sn = all_p_metadata[str(p)]['e4sn']\n",
    "calibration_files = all_p_metadata[str(p)]['calibration']\n",
    "p_calib = {}\n",
    "\n",
    "# load calibration data\n",
    "e4_num = all_p_metadata[str(p)]['e4sn'] + '_' + all_p_metadata[str(p)]['calibration'][0]\n",
    "hx_num = str(all_p_metadata[str(p)]['calibration'][1])\n",
    "p_calib[p] = read_sync_return(p_path, e4_num, hx_num)\n",
    "p_calib[p]['rot_label'] = rot_anx_dict['calibration'] * np.ones(len(p_calib[p]['ECG'])) # add label to designate calibration segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in p_calib[p]:\n",
    "    print(key,(p_calib[p][key]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radwear_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'\n",
    "redcap_path = radwear_path + 'REDCap responses/'\n",
    "print(os.listdir(redcap_path))\n",
    "#redcap_df_2nd.pkl\n",
    "with open(redcap_path + 'redcap_dict_1st.pkl', 'rb') as f:\n",
    "    redcap_dict_1st = pickle.load(f)\n",
    "with open(redcap_path + 'redcap_df_2nd.pkl', 'rb') as f:\n",
    "    redcap_df_2nd = pickle.load(f)\n",
    "#redcap_dict_1st.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(redcap_dict_1st[16])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap_df_2nd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
