{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fb testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import dask as pd\n",
    "import pandas as pd_old\n",
    "import warnings\n",
    "import utils\n",
    "import heartpy as hp\n",
    "from ECG_feature_extractor_1001 import *\n",
    "# import time \n",
    "import time\n",
    "from datetime import datetime\n",
    "from biosppy.signals import ecg\n",
    "from feature_extraction import SubjectData, compute_features, get_samples, combine_files\n",
    "\n",
    "# To ignore all warnings:\n",
    "warnings.filterwarnings(\"ignore\", module=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_IN_SECONDS = 60\n",
    "stride = 1\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "feat_names = None\n",
    "loadPath = '../data/WESAD'\n",
    "savePath = '../data/GN-WESAD'\n",
    "subject_feature_path = '/subject_feats'\n",
    "onedrive = (\n",
    "    '/mnt/d/Users/alkurdi/OneDrive - University of Illinois - Urbana/data/GN-WESAD'\n",
    ")\n",
    "n_samples = 10\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6]  # 0.00001,\n",
    "fb_model_list = ['DT', 'RF', 'LDA', 'KNN', 'AdaBoost', 'SVM']\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcompleted snrs :\u001b[39m\u001b[39m{\u001b[39;00mcompleted_snrs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mincomplete snrs :\u001b[39m\u001b[39m{\u001b[39;00mbad_snrs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m get_processing_status(snrs, subject_ids, onedrive, n_samples\u001b[39m=\u001b[39mn_samples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snrs' is not defined"
     ]
    }
   ],
   "source": [
    "def get_processing_status(snrs, subject_ids, onedrive, n_samples=10):\n",
    "    # bads are the ones that do not have the gaussian-modified data.\n",
    "    bads = []\n",
    "    bad_snrs = []\n",
    "    bad_subjects = []\n",
    "    bad_ns = []\n",
    "    completed_snrs = []\n",
    "    for n_i in range(n_samples):\n",
    "        for snr in snrs:\n",
    "            for subject_id in subject_ids:\n",
    "                # print(snr)\n",
    "\n",
    "                # print(f'{onedrive}/n_{n_i}/snr_{snr}/S{subject_id}/{a}')\n",
    "                try:\n",
    "                    a = os.listdir(f'{onedrive}/n_{n_i}/snr_{str(snr)}/S{subject_id}')\n",
    "                    a[0]\n",
    "                    completed_snrs.append(snr)\n",
    "                except:\n",
    "                    bads.append(f'n_{n_i}/snr_{snr}/S{subject_id}')\n",
    "                    bad_snrs.append(snr)\n",
    "                    bad_subjects.append(subject_id)\n",
    "                    bad_ns.append(n_i)\n",
    "\n",
    "    bad_snrs = sorted(set(bad_snrs))\n",
    "    bad_subjects = sorted(set(bad_subjects))\n",
    "    bad_ns = sorted(set(bad_ns))\n",
    "    completed_snrs = sorted(set(completed_snrs))\n",
    "    # printing after checking\n",
    "    print(f'completed snrs :{completed_snrs}')\n",
    "    print(f'incomplete snrs :{bad_snrs}')\n",
    "\n",
    "\n",
    "get_processing_status(snrs, subject_ids, onedrive, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "for i in os.listdir(f'{loadPath}/subject_feats'):\n",
    "    # print (i)\n",
    "    if 'S' not in i[0]:\n",
    "        dataset_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "\n",
    "for each_dataset in dataset_list:\n",
    "    for snr in snrs:\n",
    "        for model in fb_model_list:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "savePath = 'C:/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "n_samples = [9]\n",
    "subject_ids = [2]\n",
    "snrs = [0.01]\n",
    "for n_i in n_samples:\n",
    "    if not os.path.exists(savePath + '/n_' + str(n_i)):\n",
    "        os.makedirs(savePath + '/n_' + str(n_i))\n",
    "    for snr in snrs:\n",
    "        if not os.path.exists(savePath + '/n_' + str(n_i) + '/snr_' + str(snr)):\n",
    "            os.makedirs(savePath + '/n_' + str(n_i) + '/snr_' + str(snr))\n",
    "        for subject_id in subject_ids:\n",
    "            if not os.path.exists(\n",
    "                savePath\n",
    "                + '/n_'\n",
    "                + str(n_i)\n",
    "                + '/snr_'\n",
    "                + str(snr)\n",
    "                + '/S'\n",
    "                + str(subject_id)\n",
    "            ):\n",
    "                os.makedirs(\n",
    "                    savePath\n",
    "                    + '/n_'\n",
    "                    + str(n_i)\n",
    "                    + '/snr_'\n",
    "                    + str(snr)\n",
    "                    + '/S'\n",
    "                    + str(subject_id)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{savePath}/n_{n_i}')\n",
    "print(os.path.isdir(f'{savePath}/n_{n_i}'))\n",
    "print(os.listdir(f'{savePath}/n_{n_i}'))\n",
    "print(\n",
    "    os.path.isdir(\n",
    "        f'{savePath}/n_{n_i}/snr_{snr}/fixed_resampled140hz_S{subject_id}.pkl'\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(f'{savePath}/n_{n_i}/poop.txt', 'w') as f:\n",
    "    f.write('poop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd_old.DataFrame(columns=['SNR', 'Accuracy', 'F1-Score', 'dataset'])\n",
    "results_table.loc[str('SVM')] = pd_old.Series(\n",
    "    {'SNR': 1, 'Accuracy': 5, 'F1 Score': 2, 'dataset': 'WESAD'}\n",
    ")\n",
    "results_table.loc[str('RF')] = pd_old.Series(\n",
    "    {'SNR': 1, 'Accuracy': 5, 'F1 Score': 2, 'dataset': 'WESAD'}\n",
    ")\n",
    "'''\n",
    "fb_model_list = ['SVM', 'RF']\n",
    "for model in fb_model_list:\n",
    "    for i in range(len(snrs)):\n",
    "        results_table.loc[str(model) + str(snrs[i])] = pd.Series({'SNR':snrs[i], 'Accuracy':svm_accuracy[i], 'F1 Score':2, 'dataset':'WESAD'})\n",
    "'''\n",
    "display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "ecg_biosppy = ecg\n",
    "ecg = None\n",
    "fs_ecg = 700\n",
    "fs_ppg = 64\n",
    "subject_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = SubjectData(main_path=loadPath, subject_number=subject_id)\n",
    "data_dict = subject.get_wrist_and_chest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = data_dict['ECG'][40000:60000].flatten()\n",
    "ppg = data_dict['BVP'][int(40000 / 700 * 64) : int(60000 / 700 * 64)].flatten()\n",
    "print('ecg len', len(ecg))\n",
    "print('ppg len', len(ppg))\n",
    "\n",
    "\n",
    "# testing for ECG\n",
    "\n",
    "now = time.time()\n",
    "# wd4, m4 = hp.process(ecg, fs_ecg)\n",
    "# print('hp.process execution time is %5.2fs' % (time.time()-now))\n",
    "\n",
    "now = time.time()\n",
    "# pack2, ecgout2, time_dict2 = freq_ratio_hybrid(ecg, fs_ecg, RR1, method='welch', factor = 1)\n",
    "# print('freq_ratio_hybrid execution time is %5.2fs' % (time.time()-now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "BS_signal_analysis3, pack3, ecg_out3 = analyze_ecg(ecg, fs_ecg)\n",
    "print('analyze_ecg execution time is ', now - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = data_dict['ECG'][10000:80000].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for PPG\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR, time_dict = freq_ratio(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, time_dict = freq_ratio_hybrid(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio_hybrid execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "BS_signal_analysis, pack, ppg_out = analyze_ecg(ppg, fs_ppg)\n",
    "print('analyze_ecg execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "wd, m = hp.process(ppg, fs_ppg)\n",
    "print('hp.process execution time is ', now - time.time())\n",
    "\n",
    "now = datetime.now()\n",
    "pack, ppg, RR = freq_ratio_fast(ppg, fs_ppg, method='welch', factor=1)\n",
    "print('freq_ratio_fast execution time is ', now - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "import logging\n",
    "\n",
    "\n",
    "def combine_noiZ_files(subject_ids):\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG, filename=today + '-combine.log', filemode='w', force=True\n",
    "    )\n",
    "    logging.info('Started')\n",
    "    print('total number of combines: ', len(snrs) * n_samples)\n",
    "    df_list = []\n",
    "    i = 0\n",
    "    for snr in snrs:\n",
    "        for n_i in range(n_samples):\n",
    "            for s in subject_ids:\n",
    "                df = pd_old.read_csv(\n",
    "                    f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{s}_feats.csv',\n",
    "                    index_col=0,\n",
    "                )\n",
    "                df['subject'] = s\n",
    "                df_list.append(df)\n",
    "            df = pd_old.concat(df_list)\n",
    "            df['label'] = (\n",
    "                df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)\n",
    "            ).apply(lambda x: x.index('1'))\n",
    "            df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            now = datetime.now().strftime('%Y-%m-%d')\n",
    "            df.to_csv(\n",
    "                f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv'\n",
    "            )\n",
    "            i += 1\n",
    "            logging.info(\n",
    "                f'Saved file to: {savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{today}_feats_filt.csv  {i}/{len(snrs)*n_samples}'\n",
    "            )\n",
    "            print(\n",
    "                'Saved file to: ',\n",
    "                f'.../n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv   {i}/{len(snrs)*n_samples}',\n",
    "            )\n",
    "            counts = df['label'].value_counts()\n",
    "            logging.info('Number of samples per class:')\n",
    "            logging.info(\n",
    "                'baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(\n",
    "                    *list(zip(counts.index, counts.values))\n",
    "                )\n",
    "            )\n",
    "    logging.info('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_noiZ_files(subject_ids)\n",
    "# took 19m 30s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def process_subject_file(snr, n_i, s):\n",
    "    file_path = f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{s}_feats.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df['subject'] = s\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "\n",
    "def combine_noiZ_files(subjects):\n",
    "    now = datetime.now().strftime('%Y-%m-%d')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, filename=now + '-combine.log', filemode='w', force=True\n",
    "    )\n",
    "\n",
    "    for snr in snrs:\n",
    "        for n_i in range(n_samples):\n",
    "            # Parallelize file reading\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(process_subject_file, snr, n_i, s) for s in subjects\n",
    "                ]\n",
    "                df_list = [\n",
    "                    future.result() for future in futures if future.result() is not None\n",
    "                ]\n",
    "\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['label'] = df[['0', '1', '2']].idxmax(axis=1)\n",
    "                df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                df.to_csv(\n",
    "                    f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv'\n",
    "                )\n",
    "                logging.info('-' * 20)\n",
    "                logging.info(\n",
    "                    f'Saved file to: {savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/{now}_feats_filt.csv'\n",
    "                )\n",
    "\n",
    "                counts = df['label'].value_counts()\n",
    "                logging.info('Number of samples per class:')\n",
    "                logging.info(\n",
    "                    'baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(\n",
    "                        *list(zip(counts.index, counts.values))\n",
    "                    )\n",
    "                )\n",
    "    logging.info('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i = 5\n",
    "s = 2\n",
    "snr = 0.6\n",
    "df = pd_old.read_csv(\n",
    "    f'{savePath}/n_{n_i}/snr_{snr}{subject_feature_path}/S{subject_id}_feats.csv',\n",
    "    index_col=0,\n",
    ")\n",
    "df['subject'] = s\n",
    "# df_list.append(df)\n",
    "# f = pd_old.concat(df_list)\n",
    "df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(\n",
    "    lambda x: x.index('1')\n",
    ")\n",
    "df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "counts = df['label'].value_counts()\n",
    "print('Number of samples per class:')\n",
    "print(\n",
    "    'baseline: {0[1]}; stress: {1[1]}; amusement: {2[1]} '.format(\n",
    "        *list(zip(counts.index, counts.values))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_noiZ_files(subject_ids)\n",
    "# this paralellized took 34seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, filemode='w', force=True)\n",
    "logging.info('GN-WESAD models ran and results generated and saved in: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    # print(os.listdir(path))\n",
    "    df = pd_old.read_csv(path, index_col=0)\n",
    "    print('len of df ', len(df))\n",
    "    pd_old.set_option('display.max_columns', None)\n",
    "    # We want to drop columns in df that are not in RADWear to match modalities.\n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    # df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if (\n",
    "            'EMG' in column\n",
    "            or 'EDA_C' in column\n",
    "            or 'Temp_C' in column\n",
    "            or 'TEMP_C' in column\n",
    "            or 'SCR_C' in column\n",
    "            or 'SCL_C' in column\n",
    "        ):\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    print('len of reduced df ', len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def gn_wesad_path(n_i, snr):\n",
    "    loadPath = '../data/GN-WESAD'\n",
    "    return (\n",
    "        f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "    )\n",
    "\n",
    "\n",
    "gn_wesad_day = '2023-11-13'\n",
    "matrix = np.zeros((len(snrs), n_samples))\n",
    "\n",
    "for n_i in range(n_samples):\n",
    "    for i, snr in enumerate(snrs):\n",
    "        print(f'for {n_i} and {snr} number {i}: ')\n",
    "        file_path = gn_wesad_path(n_i, snr)\n",
    "        df = read_n_reduce(file_path)\n",
    "        matrix[i][n_i] = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "'data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-12_feats_filt.csv'\n",
    "loadPath = '../data/GN-WESAD'\n",
    "# display(os.listdir(f'{loadPath}/n_{n_i}/snr_{snr}{subject_feature_path}'))\n",
    "snr0001 = pd_old.read_csv(\n",
    "    '../data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-13_feats2.csv', index_col=0\n",
    ")\n",
    "snr001 = pd_old.read_csv(\n",
    "    '../data/GN-WESAD/n_0/snr_0.001/subject_feats/2023-11-13_feats2.csv', index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'SNR': [0.6] * 10,\n",
    "        'Accuracy': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'F1 Score': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'dataset': ['GN-WESAD'] * 10,\n",
    "        'n_i': list(range(10)),\n",
    "        'n': [10] * 10,\n",
    "        'noise gen function': ['Gaussian Noise'] * 10,\n",
    "        'Precision': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Recall': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Model': ['DT'] * 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Example Series\n",
    "series = pd.Series(\n",
    "    {\n",
    "        'SNR': 0.600000,\n",
    "        'Accuracy': 0.945747,\n",
    "        'F1 Score': 0.945747,\n",
    "        'n_i': 4.500000,\n",
    "        'n': 10.000000,\n",
    "        'Precision': 0.945747,\n",
    "        'Recall': 0.945747,\n",
    "    },\n",
    "    name='SNR 0.6 Model DT mean',\n",
    ")\n",
    "\n",
    "# Convert the series to a DataFrame\n",
    "series_df = series.to_frame().T\n",
    "\n",
    "# Append the new DataFrame to the existing one\n",
    "df_combined = pd.concat([df, series_df], ignore_index=True)\n",
    "\n",
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'SNR': [0.6] * 10,\n",
    "        'Accuracy': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'F1 Score': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'dataset': ['GN-WESAD'] * 10,\n",
    "        'n_i': list(range(10)),\n",
    "        'n': [10] * 10,\n",
    "        'noise gen function': ['Gaussian Noise'] * 10,\n",
    "        'Precision': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Recall': [\n",
    "            0.940639,\n",
    "            0.968134,\n",
    "            0.964939,\n",
    "            0.940909,\n",
    "            0.937785,\n",
    "            0.923780,\n",
    "            0.942337,\n",
    "            0.960486,\n",
    "            0.952816,\n",
    "            0.925645,\n",
    "        ],\n",
    "        'Model': ['DT'] * 10,\n",
    "    }\n",
    ")\n",
    "series = pd.Series(\n",
    "    {\n",
    "        'SNR': 0.600000,\n",
    "        'Accuracy': 0.945747,\n",
    "        'F1 Score': 0.945747,\n",
    "        'n_i': 4.500000,\n",
    "        'n': 10.000000,\n",
    "        'Precision': 0.945747,\n",
    "        'Recall': 0.945747,\n",
    "    },\n",
    "    name='SNR 0.6 Model DT mean',\n",
    ")\n",
    "\n",
    "print(set(df.columns))\n",
    "print(set(series.index))\n",
    "missing_columns = set(df.columns) - set(series.index)\n",
    "print(missing_columns)\n",
    "for col in missing_columns:\n",
    "    # You can assign a specific value or use a value from the DataFrame\n",
    "    # Here, I'm using the first row's value as an example\n",
    "    # print(col)\n",
    "    # print(df[col].iloc[0])\n",
    "\n",
    "    series[col] = df[col].iloc[0]\n",
    "    # print(series[col])\n",
    "\n",
    "# print(series)\n",
    "\n",
    "series_df2 = series.to_frame().T\n",
    "display(series_df2)\n",
    "df_combined2 = pd.concat([df, series_df2])\n",
    "display(df_combined2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate and plot\n",
    "\n",
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "with open('../data/WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    WESAD_cm_cr_dict = pickle.load(handle)\n",
    "wesad_acc = pd.read_csv(\n",
    "    '../data/WESAD/wesad_models_results-win60stride1_wcm_wcr.csv', index_col=0\n",
    ")\n",
    "display(wesad_acc)\n",
    "# combined_results = pd.concat([WESAD_model_results, GN_model_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wbinaryf1.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "# gn_wesad_acc['Binary F1'] = None\n",
    "print(gn_wesad_acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_wesad_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GN-WESAD/cm_cr_dict.pickle', 'rb') as handle:\n",
    "    GN_cm_cr_dict = pickle.load(handle)\n",
    "tgt_file = '../data/GN-WESAD/GN_wesad_models_results_wcm_wcr.csv'\n",
    "gn_wesad_acc = pd.read_csv(tgt_file, index_col=0)\n",
    "gn_wesad_acc['Binary F1'] = None\n",
    "\n",
    "for i, classification_report in enumerate(GN_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    print(classification_report['id'][2], fb_model_list[i % 6])\n",
    "    # print(cr)\n",
    "    # print(i//5)\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    # print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        # print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i % 6]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i % 6]\n",
    "        secret_df = pd.concat(\n",
    "            [\n",
    "                secret_df,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        'Model': fb_model_list[i % 6],\n",
    "                        'Binary F1 Score': binary_f1_score,\n",
    "                        'SNR': classification_report['id'][0],\n",
    "                        'n': classification_report['id'][1],\n",
    "                        'Model': classification_report['id'][2],\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        # print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        # display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        # print(fb_model_list[i])\n",
    "        gn_wesad_acc.loc[\n",
    "            gn_wesad_acc['Model'] == fb_model_list[i % 6], 'Binary f1'\n",
    "        ] = binary_f1_score\n",
    "        # print(gn_wesad_acc.loc[gn_wesad_acc['Model'] == fb_model_list[i], 'binary f1'])\n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(GN_cm_cr_dict['cr']))\n",
    "print(len(WESAD_cm_cr_dict['cr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc_test = wesad_acc.copy()\n",
    "display(wesad_acc_test)\n",
    "# wesad_acc_test.loc['Model'] = 'DT'\n",
    "model = 'DT'\n",
    "# display (wesad_acc_test[wesad_acc_test['Model'] == model]['binary f1'] = 4)#[wesad_acc_test['SNR'] == np.nan] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in wesad_acc_test.iterrows():\n",
    "    # if row[1]['Model'] == model:\n",
    "    #    row[1]['binary f1'] = 4\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc_test['binary f1'] = None\n",
    "wesad_acc_test['binary acc'] = None\n",
    "# display(wesad_acc_test.drop(columns=[['binary f1', 'wow']]))\n",
    "display(wesad_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in WESAD_cm_cr_dict['cr']:\n",
    "    # print('result', result)\n",
    "    if True:  #'.' in result['id']:\n",
    "        # print(result['id'])\n",
    "        print(result['Classification Report'])\n",
    "        # print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wesad_acc_test = wesad_acc.copy()\n",
    "wesad_acc_test['Binary F1'] = None\n",
    "\n",
    "\n",
    "def extract_metrics(report):\n",
    "    # Regular expression to find numeric values\n",
    "    regex = r\"\\s+1\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\"\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = re.search(regex, report)\n",
    "    # print(match)\n",
    "    if match:\n",
    "        precision = float(match.group(1))\n",
    "        recall = float(match.group(2))\n",
    "        f1_score = float(match.group(3))\n",
    "        return precision, recall, f1_score\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def calculate_binary_metrics(report):\n",
    "    precision, recall, f1_score = extract_metrics(report)\n",
    "\n",
    "    if precision is not None and recall is not None:\n",
    "        # Calculate binary F1 score for class 1\n",
    "        binary_f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return binary_f1_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# classification_report =\n",
    "# Extract and calculate binary F1 score\n",
    "fb_model_counter = fb_model_list\n",
    "\n",
    "\n",
    "secret_df = pd.DataFrame()\n",
    "\n",
    "for i, classification_report in enumerate(WESAD_cm_cr_dict['cr']):\n",
    "    cr = classification_report['Classification Report']\n",
    "    binary_f1_score = calculate_binary_metrics(cr)\n",
    "    # print(classification_report['id'])\n",
    "    if binary_f1_score is not None:\n",
    "        print(\"Binary F1 Score for \", fb_model_list[i], \" :\\t\\t\", binary_f1_score)\n",
    "        insta_acc = wesad_acc['Accuracy'][i]\n",
    "        insta_f1 = wesad_acc['F1 Score'][i]\n",
    "        secret_df = pd.concat(\n",
    "            [\n",
    "                secret_df,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        'Model': fb_model_list[i],\n",
    "                        'Binary F1 Score': binary_f1_score,\n",
    "                        'SNR': classification_report['id'][0],\n",
    "                        'n': classification_report['id'][1],\n",
    "                        'Model': classification_report['id'][2],\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        # print(f'my calc acc {insta_acc}, calc f1 {insta_f1}')\n",
    "        # display(wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]])#['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary f1'] = binary_f1_score\n",
    "        # wesad_acc_test[wesad_acc_test['Model'] == fb_model_list[i]]['binary acc'] = insta_acc\n",
    "        # print(fb_model_list[i])\n",
    "        wesad_acc_test.loc[\n",
    "            wesad_acc_test['Model'] == fb_model_list[i], 'binary f1'\n",
    "        ] = binary_f1_score\n",
    "        print(\n",
    "            wesad_acc_test.loc[wesad_acc_test['Model'] == fb_model_list[i], 'binary f1']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Could not extract metrics from the report.\")\n",
    "display(secret_df)\n",
    "display(wesad_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([wesad_acc, secret_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fafi = pd.DataFrame(\n",
    "    columns=[\n",
    "        'SNR',\n",
    "        'Accuracy',\n",
    "        'F1 Score',\n",
    "        'dataset',\n",
    "        'n_i',\n",
    "        'n',\n",
    "        'noise gen function',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Model',\n",
    "    ]\n",
    ")\n",
    "pd.Series(classification_report['id']).to_frame().T\n",
    "wow = (\n",
    "    pd.Series(classification_report['id'])\n",
    "    .to_frame()\n",
    "    .T.rename(columns={0: 'SNR', 1: 'n', 2: 'model'})\n",
    ")\n",
    "fafi = pd.concat([fafi, wow])\n",
    "display(fafi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report['id'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(WESAD_cm_cr_dict['cr'])\n",
    "fb_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_acc['Accuracy'][i]\n",
    "wesad_acc['F1 Score'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_reduce(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # We want to drop columns in df that are not in RADWear to match modalities.\n",
    "    # drop _c columns\n",
    "    columns_list = df.columns.tolist()\n",
    "    drop_list = []\n",
    "    # df.drop(columns=['Resp_C'])\n",
    "    for column in columns_list:\n",
    "        if (\n",
    "            'EMG' in column\n",
    "            or 'EDA_C' in column\n",
    "            or 'Temp_C' in column\n",
    "            or 'TEMP_C' in column\n",
    "            or 'SCR_C' in column\n",
    "            or 'SCL_C' in column\n",
    "        ):\n",
    "            drop_list.append(column)\n",
    "\n",
    "    reduced_df = df.drop(columns=drop_list)\n",
    "    df = reduced_df\n",
    "    return df\n",
    "\n",
    "\n",
    "print(os.listdir('../data/WESAD'))\n",
    "df = read_n_reduce('../data/WESAD//subject_feats/oct5_feats4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = df.drop(['label', 'subject'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=0):\n",
    "    # split data into features and labels\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "\n",
    "the_splits = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "xx = df.drop(columns=['label'])\n",
    "yy = df['label']\n",
    "# xx.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xx)\n",
    "display(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# display(the_splits[0])\n",
    "X_train, X_test, y_train, y_test = the_splits\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_out = clf.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_wesad = pd.read_csv('../data/WESAD/subject_feats/oct5_feats4.csv', index_col=0)\n",
    "new_wesad = pd.read_csv(\n",
    "    '../data/WESAD/subject_feats/WESADfeatures-win60stride1.csv', index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_wesad = pd.read_csv(\n",
    "    '../data/GN-WESAD/n_0/snr_0.0001/subject_feats/2023-11-13_feats2.csv', index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr in snrs:\n",
    "    for n_i in range(n_samples):\n",
    "        tgt = f'../data/GN-WESAD/n_{n_i}/snr_{snr}{subject_feature_path}/{gn_wesad_day}_feats2.csv'\n",
    "        print(f'df {snr} n {n_i} shape is {(pd.read_csv(tgt, index_col=0)).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features used for fb_models\\n', *old_wesad.columns, sep=',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features used for fb_models\\n', *new_wesad.columns, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "subject_id = 2\n",
    "snr = 0.01\n",
    "n_i = 0\n",
    "\n",
    "\n",
    "wesad_path = '/mnt/d/Users/alkurdi/data/WESAD'\n",
    "freshwesad_path = '/mnt/d/Users/alkurdi/data/freshWESAD'\n",
    "downloaddwesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data'\n",
    "gn_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "sesh_path = '/n_' + str(n_i) + '/snr_' + str(snr) + '/S' + str(subject_id)\n",
    "load_gn_path = gn_path + sesh_path + '/S' + str(subject_id) + '.pkl'\n",
    "load_dw_path = (\n",
    "    downloaddwesad_path + '/S' + str(subject_id) + '/S' + str(subject_id) + '.pkl'\n",
    ")\n",
    "load_ws_path = wesad_path + '/S' + str(subject_id) + '/S' + str(subject_id) + '.pkl'\n",
    "load_freshws_path = (\n",
    "    freshwesad_path + '/S' + str(subject_id) + '/S' + str(subject_id) + '.pkl'\n",
    ")\n",
    "\n",
    "# ws_df = pd.read_pickle(load_ws_path)\n",
    "\n",
    "# with open( load_ws_path, 'rb') as dest:\n",
    "#    ws_df = pickle.load(dest)\n",
    "\n",
    "now = time()\n",
    "with open(load_dw_path, 'rb') as file:\n",
    "    dw_df = pickle.load(file, encoding='latin1')\n",
    "print(\n",
    "    'c:\\ downloads wesad loaded w pickle.load in ', round(time() - now, 3), 's'\n",
    ")  # c:\\ downloads wesad loaded w pickle.load in  17.112 s\n",
    "\n",
    "\n",
    "now = time()\n",
    "with open(load_gn_path, 'rb') as file:\n",
    "    gn_df = pickle.load(file, encoding='latin1')\n",
    "print('gnwesad loaded in ', round(time() - now, 3), 's')\n",
    "\n",
    "'''\n",
    "now = time()\n",
    "with open(load_freshws_path, 'rb') as file:\n",
    "            freshws_df = pickle.load(file, encoding='latin1')\n",
    "print('d:\\ freshwesad loaded in ', round(time()-now,3),'s')  #d:\\ freshwesad loaded in  191.335 s\n",
    "\n",
    "now = time()\n",
    "dw_df = pd.read_pickle(load_dw_path)\n",
    "print('c:\\ downloads wesad loaded w pd.read_pickle in ', round(time()-now,3),'s') #c:\\ downloads wesad loaded w pd.read_pickle in  24.78 s\n",
    "\n",
    "\n",
    "#this is the one that fails\n",
    "load_ws_path = wesad_path + '/S'+str(subject_id) + '/S'+str(subject_id)+'.pkl'\n",
    "now = time()\n",
    "with open(load_ws_path, 'rb') as file:\n",
    "            ws_df = pickle.load(file, encoding='latin1')\n",
    "print('wesad loaded in ', round(time()-now,3),'s')\n",
    "'''\n",
    "# fresh_df = pd.read_pickle()\n",
    "# wesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/WESAD'\n",
    "# msg = f'starting  n_i: {n_i}; snr: {snr}, id: {subject_id}. iteration'\n",
    "# sesh_path = '/n_'+str(n_i)+'/snr_'+str(snr)+'/S'+str(subject_id)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time()\n",
    "with open(\n",
    "    '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.6/S17/fixed_resampled140hz_S17.pkl',\n",
    "    'rb',\n",
    ") as file:\n",
    "    resampled_gn = pickle.load(file, encoding='latin1')\n",
    "print('resampled_gn loaded in ', round(time() - now, 3), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def my_counter(df, name=None):\n",
    "    i = 0\n",
    "    long_list = []\n",
    "    for i_first_layer in df.keys():\n",
    "        # print(i_first_layer, dw_df[i_first_layer])\n",
    "        if i_first_layer == 'label':\n",
    "            long_list = long_list + [*df[i_first_layer]]\n",
    "            i += len(df[i_first_layer])\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "        elif i_first_layer == 'signal':\n",
    "            for i_second_layer in df[i_first_layer].keys():\n",
    "                # print('part', i_second_layer)#, dw_df[i_first_layer][i_second_layer])\n",
    "\n",
    "                for i_third_layer in df[i_first_layer][i_second_layer].keys():\n",
    "                    sig_shape = np.shape(\n",
    "                        df[i_first_layer][i_second_layer][i_third_layer]\n",
    "                    )\n",
    "                    # print('part', i_second_layer, 'signal', i_third_layer, sig_shape)\n",
    "                    i += np.prod(sig_shape)\n",
    "                    long_list = long_list + [\n",
    "                        *df[i_first_layer][i_second_layer][i_third_layer].flatten()\n",
    "                    ]\n",
    "                    print(\n",
    "                        f'{name}: length of {i_third_layer} is {len(df[i_first_layer][i_second_layer][i_third_layer]):,.0f}'\n",
    "                    )\n",
    "        elif i_first_layer == 'subject':\n",
    "            pass\n",
    "        else:\n",
    "            long_list = long_list + [*df[i_first_layer].flatten()]\n",
    "            i += np.prod(np.shape(df[i_first_layer]))\n",
    "            print(f'{name}: length of {i_first_layer} is {len(df[i_first_layer]):,.0f}')\n",
    "    print('_' * 5)\n",
    "    print(f'total length of {name} df is {len(long_list):,.0f}')\n",
    "    print('_' * 25)\n",
    "\n",
    "\n",
    "my_counter(dw_df, name='dw_df')\n",
    "my_counter(gn_df, name='gs_df')\n",
    "my_counter(resampled_gn, name='resampled_gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(os.getpid(), file=sys.stderr)\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "\n",
    "def get_cmd(pid):\n",
    "    with Popen(f\"ps -q {pid} -o comm=\", shell=True, stdout=PIPE) as p:\n",
    "        return p.communicate()[0]\n",
    "\n",
    "\n",
    "get_cmd(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_list = list(dw_df['signal']['chest'].items()) + list(\n",
    "    dw_df['signal']['wrist'].items()\n",
    ")\n",
    "a, b = zip(*long_list)\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD/n_0/snr_0.0001/S2'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 0\n",
    "a, b = zip(*list(gn_df.items()))\n",
    "tot_len = 0\n",
    "for i in list(b):\n",
    "    tot_len += len(i)\n",
    "print(f'total length gn wesad {tot_len:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wesad_path = '/mnt/c/Users/alkurdi/data/WESAD'\n",
    "fresh_wesad_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/WESAD'\n",
    "\n",
    "# start_time = time()\n",
    "id = 2\n",
    "sesh_path = fresh_wesad_path + '/S' + str(id) + '/S' + str(id) + '.pkl'\n",
    "\n",
    "ws_df = pd.read_pickle(sesh_path)\n",
    "# with open( sesh_path, 'rb') as dest:\n",
    "#        ws2_df = pickle.load(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_resampled_df = pd.read_pickle(\n",
    "    '/mnt/d/Users/alkurdi/data/GN-WESAD/n_0/snr_0.01/S17/fixed_resampled170hz64_S17.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in fixed_resampled_df.keys():\n",
    "    print(\n",
    "        f'length of signal {key}: ', len(fixed_resampled_df['signal']['chest']['ECG'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys', ws_df.keys())\n",
    "print('keys', ws_df['signal'].keys())\n",
    "print('length of signal', len(ws_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fixed_resampled_df['signal']['chest']['ECG']) / len(ws_df['signal']['chest']['ECG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import signal\n",
    "from time import time\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "n_samples = 0 #10\n",
    "n_i = n_samples\n",
    "snr1 = 0.01\n",
    "snr2 = 0.6\n",
    "subject_id = 17\n",
    "\n",
    "sesh_id =[n_i,snr1,subject_id]\n",
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "start_time = time()\n",
    "onedrive = '/mnt/d/Users/alkurdi/OneDrive - University of Illinois - Urbana/data/GN-WESAD'\n",
    "\n",
    "#print(os.path.isfile(rt_pth + '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(17)+'/S'+str(17)+'.pkl'))\n",
    "sesh_path = rt_pth + '/n_'+str(0)+'/snr_'+str(0.0001)+ '/S'+str(2)+'/S'+str(2)+'.pkl'\n",
    "#drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "with open( sesh_path, 'rb') as dest:\n",
    "        ws1_df = pickle.load(dest) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_pth = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "# start_time = time()\n",
    "id = 4\n",
    "sesh_path = (\n",
    "    rt_pth\n",
    "    + '/n_'\n",
    "    + str(1)\n",
    "    + '/snr_'\n",
    "    + str(0.001)\n",
    "    + '/S'\n",
    "    + str(id)\n",
    "    + '/S'\n",
    "    + str(id)\n",
    "    + '.pkl'\n",
    ")\n",
    "# drive_path = onedrive +  '/n_'+str(0)+'/snr_'+str(snr1)+ '/S'+str(subject_id)+'/S'+str(subject_id)+'.pkl'\n",
    "\n",
    "with open(sesh_path, 'rb') as dest:\n",
    "    ws2_df = pickle.load(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for resampled dataset: ')\n",
    "for key in ws2_df.keys():\n",
    "    print('key', key)\n",
    "    # print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))\n",
    "    # print(f'length of chest ignal {key}: ', len(ws2_df['signal']['chest']['ECG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "# snrs = [ 0.01,0.1,0.6 0.05,  0.3,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6] # this is what we ran #0.00001,\n",
    "snrs = [0.01, 0.05, 0.3]  # ,0.1, 0.15, 0.2,0.0001, 0.001, 0.4, 0.5, 0.6]\n",
    "n_i = [0, 1, 3, 4]  # next is [5, 6, 7] [8, 9, 10]\n",
    "\n",
    "\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD/GN-WESAD'\n",
    "to_path = '/mnt/d/Users/alkurdi/data/GN-WESAD'\n",
    "\n",
    "\n",
    "for n in n_i:\n",
    "    for snr in snrs:\n",
    "        for s in subject_ids:\n",
    "            sesh_path = '/n_' + str(n_i) + '/snr_' + str(snr) + '/S' + str(subject_id)\n",
    "\n",
    "            xact_from_path = from_path + sesh_path\n",
    "            xact_to_path = to_path + sesh_path\n",
    "\n",
    "            os.system(f'cp -r {from_path} {to_path}')\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for 5 seconds\n",
    "from time import sleep\n",
    "\n",
    "from_path = '/mnt/c/Users/alkurdi/Downloads/WESAD'\n",
    "poop = 'poop'\n",
    "\n",
    "with open(from_path + '/poop.txt', 'w') as f:\n",
    "    f.write(poop)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# os.system(f'{from_path}/poop>poop.txt')\n",
    "\n",
    "to_path = '/mnt/d/Users/alkurdi/data/'\n",
    "\n",
    "os.system(f'mv {from_path}/poop.txt {to_path}/poop.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RADWear Calibration fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'modified by abdul alkurdi; 10/05/2023'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import cudf\n",
    "import pickle, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# import cupy as cp\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal, stats\n",
    "\n",
    "# import peakutils, wfdb, pywt\n",
    "import csv\n",
    "import os, statistics\n",
    "from datetime import datetime\n",
    "\n",
    "# import heartpy as hp\n",
    "import json\n",
    "\n",
    "# import neurokit2 as nk\n",
    "\n",
    "# from syncfcns import *\n",
    "calib_dict = {'meditation': 1, 'cpt': 2, 'baseline': 0}\n",
    "rot_anx_dict = {'calibration': 0, 'LA': 1, 'HA': 2}\n",
    "\n",
    "radwear_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sync_return(filepath, e4_today, hx_today):\n",
    "    '''\n",
    "    usage: read_sync_return(filepath, e4_today, hx_today)\n",
    "    returns: synced_participant_data\n",
    "    '''\n",
    "    e4_dict = read_E4(filepath + '/' + e4_today, hx_today)\n",
    "    hx_dict = read_hx(filepath + '/record_' + hx_today, hx_today)\n",
    "\n",
    "    # e4sync_offset = E4sync_offset(e4_dict)\n",
    "    # hxsync_offset = Hexsync_offset(hx_dict['ECG'],hx_dict['BR'],hx_dict['accx'],hx_dict['accy'], hx_dict['accz'])\n",
    "    e4sync_offset = e4_dict\n",
    "    hxsync_offset = hx_dict\n",
    "\n",
    "    synced_participant_data = {}\n",
    "    # cross syncing between devices but only on bvp and ecg\n",
    "    synced_participant_data['ECG'], synced_participant_data['BVP'] = doublesync_offset(\n",
    "        hxsync_offset['ECG'], e4sync_offset['BVP']\n",
    "    )\n",
    "    # for some reason we're losing seconds from beginning and end of hx data even though e4 is contained within it's timeframe.\n",
    "\n",
    "    # running syncing on rest of signals for hx (ECG, breathing_rateacc, br)\n",
    "    t0 = synced_participant_data['ECG']['Second'].iloc[0]  # hx\n",
    "    tf = synced_participant_data['ECG']['Second'].iloc[-1]  # hx\n",
    "    synced_participant_data['BR'] = (\n",
    "        hxsync_offset['BR']\n",
    "        .loc[hxsync_offset['BR']['Second'] >= t0]\n",
    "        .loc[hxsync_offset['BR']['Second'] <= tf]\n",
    "    )\n",
    "    del tf, t0\n",
    "\n",
    "    # running syncing on rest of signals for e4 (acc, eda, temp, hr, ibi )\n",
    "    t0 = synced_participant_data['BVP']['Second'].iloc[0]  # e4\n",
    "    tf = synced_participant_data['BVP']['Second'].iloc[-1]  # e4\n",
    "    synced_participant_data['TEMP'] = (\n",
    "        e4sync_offset['TEMP']\n",
    "        .loc[e4sync_offset['TEMP']['Second'] >= t0]\n",
    "        .loc[e4sync_offset['TEMP']['Second'] <= tf]\n",
    "    )\n",
    "    synced_participant_data['EDA'] = (\n",
    "        e4sync_offset['EDA']\n",
    "        .loc[e4sync_offset['EDA']['Second'] >= t0]\n",
    "        .loc[e4sync_offset['EDA']['Second'] <= tf]\n",
    "    )\n",
    "    synced_participant_data['HR'] = (\n",
    "        e4_dict['HR']\n",
    "        .loc[e4_dict['HR']['Second'] >= t0]\n",
    "        .loc[e4_dict['HR']['Second'] <= tf]\n",
    "    )\n",
    "    # synced_participant_data['IBI'] = e4sync_offset['IBI'].loc[e4sync_offset['IBI']['Second'] >= t0 ].loc[e4sync_offset['IBI']['Second'] <= tf ]\n",
    "    del tf, t0\n",
    "\n",
    "    hx_dict['ACC'] = hx_dict['accx']\n",
    "    hx_dict['ACC']['Acc_Y'] = hx_dict['accy']['Acc_Y']\n",
    "    hx_dict['ACC']['Acc_Z'] = hx_dict['accz']['Acc_Z']\n",
    "\n",
    "    acc_dic = accsync_offset(e4sync_offset['ACC'], hxsync_offset['accx'])\n",
    "    # offset_ecg_cross, offset_bvp_cross = doublesync_offset(e4sync_offset['ECG'], e4sync_offset['BVP'])\n",
    "    synced_participant_data['ACC_hx'] = acc_dic['acc_hx']\n",
    "    synced_participant_data['ACC_e4'] = acc_dic['acc_e4']\n",
    "\n",
    "    return synced_participant_data\n",
    "\n",
    "def read_hx(participant_day_filepath, date):\n",
    "    '''\n",
    "    takes in the participant filepath and the date of the data. it reads ecg, br, accx, accy, accz,    \n",
    "    fhex = 'record_265679'\n",
    "    '''\n",
    "    # read raw ECG file; ECG_I.wav only\n",
    "    # change wavefile pathway\n",
    "    path = participant_day_filepath \n",
    "\n",
    "    raw_ECG = wavfile.read(path+'/ECG_I.wav')\n",
    "    #settings = {}\n",
    "    #settings['fs'] = 256 # sampling rate\n",
    "\n",
    "    # ECG\n",
    "    raw_ECG = pd.DataFrame(data = raw_ECG[1])\n",
    "    ecg = 0.0064 * raw_ECG #get correct magnitude of ECG\n",
    "    ecg.rename(columns = {0: 'ECG'}, inplace = True)\n",
    "    # Opening JSON file and return it as dictionary\n",
    "    # change file pathway\n",
    "    f = open(path+'/info.json')\n",
    "    date_info = json.load(f)\n",
    "\n",
    "    # BR \n",
    "    raw_br = wavfile.read(path+'/breathing_rate.wav')\n",
    "    raw_br = pd.DataFrame(data = raw_br[1])\n",
    "    br = 1.0000 * raw_br\n",
    "    br.rename(columns = {0: 'breathing_rate'}, inplace = True)\n",
    "\n",
    "    # inspiration\n",
    "    insp = pd.read_csv(path+'/inspiration.csv')\n",
    "    insp['breathing_phase'] = 'insp'\n",
    "    # expiration\n",
    "    exp = pd.read_csv(path+'/expiration.csv')\n",
    "    exp['breathing_phase'] = 'exp'\n",
    "    # combine insp and exp\n",
    "    b_ph = pd.concat([insp, exp])\n",
    "    b_ph = b_ph.sort_values(by=['time [s]'])\n",
    "    combined_b_ph = pd.concat([b_ph['inspiration [NA](/api/datatype/34/)'].dropna(),\n",
    "                               b_ph['expiration [NA](/api/datatype/35/)'].dropna()]).sort_index()\n",
    "    b_ph['ins | exp'] = combined_b_ph\n",
    "    b_ph.drop(columns=['inspiration [NA](/api/datatype/34/)', 'expiration [NA](/api/datatype/35/)'], inplace=True)\n",
    "    \n",
    "    # respiration. took thoraccic because it's similar to the wesad resp location\n",
    "    resp_thor = wavfile.read(path+'/respiration_thoracic.wav') \n",
    "    resp_thor = pd.DataFrame(data = resp_thor[1], columns = ['RESP'])\n",
    "    \n",
    "    # acc x\n",
    "    raw_accX = wavfile.read(path+'/acceleration_X.wav')\n",
    "    raw_accX = pd.DataFrame(data = raw_accX[1])\n",
    "    accx = 1.0000 * raw_accX \n",
    "    accx.rename(columns = {0: 'Acc_X'}, inplace = True)\n",
    "    # acc y\n",
    "    raw_accY = wavfile.read(path+'/acceleration_Y.wav')\n",
    "    raw_accY = pd.DataFrame(data = raw_accY[1])\n",
    "    accy = 1.0000 * raw_accY \n",
    "    accy.rename(columns = {0: 'Acc_Y'}, inplace = True)\n",
    "    # acc z\n",
    "    raw_accZ = wavfile.read(path+'/acceleration_Z.wav')\n",
    "    raw_accZ = pd.DataFrame(data = raw_accZ[1])\n",
    "    accz = 1.0000 * raw_accZ \n",
    "    accz.rename(columns = {0: 'Acc_Z'}, inplace = True)\n",
    "\n",
    "\n",
    "    # Add timestamp to Hex (ECG & ACC) signal\n",
    "    t0_ecg = list(date_info.values())[0]/256\n",
    "    ecg['Timestamp'] = list(range(0, len(raw_ECG),1))\n",
    "    ecg['Timestamp'] = ecg['Timestamp'].apply(lambda x: x/256+t0_ecg)\n",
    "    #ecg['Timestamp'] = ecg['Timestamp'].str.get(0)\n",
    "    ecg['Second'] = ecg['Timestamp']\n",
    "    ecg = ecg.set_index('Timestamp')\n",
    "    ecg['Second'] = ecg['Second'].apply(lambda x: x-ecg.index[0])\n",
    "    ecg = ecg.reset_index()\n",
    "    #ecg.columns = ['Heart rate', 'Second']\n",
    "    \n",
    "    t0_br = list(date_info.values())[0]/256\n",
    "    br['Timestamp'] = list(range(0, len(raw_br),1))\n",
    "    br['Timestamp'] = br['Timestamp'].apply(lambda x: x/1+t0_br)\n",
    "    br['Second'] = br['Timestamp']\n",
    "    br = br.set_index('Timestamp')\n",
    "    br['Second'] = br['Second'].apply(lambda x: x-br.index[0])\n",
    "    br = br.reset_index()\n",
    "    \n",
    "    t0_b_hp = list(date_info.values())[0]/256\n",
    "    b_ph['Timestamp'] = list(range(0, len(b_ph),1))\n",
    "    b_ph['Timestamp'] = b_ph['Timestamp'].apply(lambda x: x/64+t0_b_hp)\n",
    "    b_ph['Second'] = b_ph['Timestamp']\n",
    "    b_ph = b_ph.set_index('Timestamp')\n",
    "    b_ph['Second'] = b_ph['Second'].apply(lambda x: x-b_ph.index[0])\n",
    "    b_ph = b_ph.reset_index()\n",
    "\n",
    "    t0_resp = list(date_info.values())[0]/256\n",
    "    resp_thor['Timestamp'] = list(range(0, len(resp_thor),1))\n",
    "    resp_thor['Timestamp'] = resp_thor['Timestamp'].apply(lambda x: x/64+t0_resp)\n",
    "    resp_thor['Second'] = resp_thor['Timestamp']\n",
    "    resp_thor = resp_thor.set_index('Timestamp')\n",
    "    resp_thor['Second'] = resp_thor['Second'].apply(lambda x: x-resp_thor.index[0])\n",
    "    resp_thor = resp_thor.reset_index()\n",
    "\n",
    "    t0_acc = list(date_info.values())[0]/256\n",
    "    accx['Timestamp'] = list(range(0, len(raw_accX),1))\n",
    "    accx['Timestamp'] = accx['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accx['Second'] = accx['Timestamp']\n",
    "    accx = accx.set_index('Timestamp')\n",
    "    accx['Second'] = accx['Second'].apply(lambda x: x-accx.index[0])\n",
    "    accx = accx.reset_index()\n",
    "\n",
    "    accy['Timestamp'] = list(range(0, len(raw_accY),1))\n",
    "    accy['Timestamp'] = accy['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accy['Second'] = accy['Timestamp']\n",
    "    accy = accy.set_index('Timestamp')\n",
    "    accy['Second'] = accy['Second'].apply(lambda x: x-accy.index[0])\n",
    "    accy = accy.reset_index()\n",
    "\n",
    "    accz['Timestamp'] = list(range(0, len(raw_accZ),1))\n",
    "    accz['Timestamp'] = accz['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accz['Second'] = accz['Timestamp']\n",
    "    accz = accz.set_index('Timestamp')\n",
    "    accz['Second'] = accz['Second'].apply(lambda x: x-accz.index[0])\n",
    "    accz = accz.reset_index()\n",
    "    \n",
    "    data_dict = {'Date':date, 'ECG':ecg, 'BR':br, 'B_PH':b_ph,'RESP':resp_thor, 'accx':accx, 'accy':accy,'accz':accz}\n",
    "    return data_dict\n",
    "\n",
    "def read_E4(participant_filepath, date):\n",
    "    '''\n",
    "    usage:\n",
    "        filepath=r'/home/maxinehe/Downloads/' + fE4\n",
    "        a=read_E4(filepath, '230429')\n",
    "        fE4 = 'A04BA8_230429-142458'\n",
    "        fe4 = '230429-142458'\n",
    "        \n",
    "    '''\n",
    "    filepath = participant_filepath\n",
    "    # HR data -- started 10 seconds later than other metrics\n",
    "    hr = pd.read_csv(filepath+str('/HR.csv'), header = None)\n",
    "    # clean up HR file\n",
    "    start_time = hr.values[0]\n",
    "    hr_samp_rate = hr.values[1]\n",
    "    hr = hr.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    hr['Timestamp'] = list(range(0, len(hr),1))\n",
    "    hr['Timestamp'] = hr['Timestamp'].apply(lambda x: x/hr_samp_rate+start_time)\n",
    "    hr['Timestamp'] = hr['Timestamp'].str.get(0)\n",
    "    hr['Second'] = hr['Timestamp']\n",
    "    hr = hr.set_index('Timestamp')\n",
    "    hr['Second'] = hr['Second'].apply(lambda x: x-hr.index[0])\n",
    "    hr.columns = ['Heart rate', 'Second']\n",
    "    hr = hr.reset_index(inplace=False)\n",
    "    \n",
    "    # EDA data\n",
    "    eda = pd.read_csv(filepath+str('/EDA.csv'), header = None)\n",
    "    start_time = eda.values[0]\n",
    "    eda_samp_rate = eda.values[1]\n",
    "    eda = eda.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    eda['Timestamp'] = list(range(0, len(eda),1))\n",
    "    eda['Timestamp'] = eda['Timestamp'].apply(lambda x: x/eda_samp_rate+start_time)\n",
    "    eda['Timestamp'] = eda['Timestamp'].str.get(0)\n",
    "    eda['Second'] = eda['Timestamp']\n",
    "    eda = eda.set_index('Timestamp')\n",
    "    eda['Second'] = eda['Second'].apply(lambda x: x-eda.index[0])\n",
    "    eda.columns = ['EDA', 'Second']\n",
    "    eda = eda.reset_index(inplace=False)\n",
    "\n",
    "    temp = pd.read_csv(filepath+str('/TEMP.csv'), header = None)\n",
    "    # clean up TEMP file\n",
    "    start_time = temp.values[0]\n",
    "    temp_samp_rate = temp.values[1]\n",
    "    temp = temp.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    temp['Timestamp'] = list(range(0, len(temp),1))\n",
    "    temp['Timestamp'] = temp['Timestamp'].apply(lambda x: x/temp_samp_rate+start_time)\n",
    "    temp['Timestamp'] = temp['Timestamp'].str.get(0)\n",
    "    temp['Second'] = temp['Timestamp']\n",
    "    temp = temp.set_index('Timestamp')\n",
    "    temp['Second'] = temp['Second'].apply(lambda x: x-temp.index[0])\n",
    "    temp.columns = ['Temp', 'Second']\n",
    "    temp = temp.reset_index(inplace=False)\n",
    "    \n",
    "    try:\n",
    "        ibi = pd.read_csv(filepath+str('/IBI.csv'), header = None) # no correction of timestamp needed\n",
    "        ibi = ibi.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "        ibi.columns = ['Second', 'IBI']\n",
    "    except:\n",
    "        print('no IBI data available')\n",
    "        ibi = pd.DataFrame()\n",
    "    \n",
    "    bvp = pd.read_csv(filepath+str('/BVP.csv'), header = None)\n",
    "    start_time = bvp.values[0]\n",
    "    bvp_samp_rate = bvp.values[1]\n",
    "    bvp = bvp.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    bvp['Timestamp'] = list(range(0, len(bvp),1))\n",
    "    bvp['Timestamp'] = bvp['Timestamp'].apply(lambda x: np.round(x/bvp_samp_rate+start_time, 2))\n",
    "    bvp['Timestamp'] = bvp['Timestamp'].str.get(0)\n",
    "    bvp['Second'] = bvp['Timestamp']\n",
    "    bvp = bvp.set_index('Timestamp')\n",
    "    bvp['Second'] = bvp['Second'].apply(lambda x: x-bvp.index[0])\n",
    "    bvp.columns = ['BVP', 'Second']\n",
    "    bvp = bvp.reset_index(inplace=False)\n",
    "    \n",
    "    acc = pd.read_csv(filepath+str('/ACC.csv'), header = None)\n",
    "    start_time = acc.values[0,0]\n",
    "    acc_samp_rate = acc.values[1,0]\n",
    "    acc = acc.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    acc['Timestamp'] = list(range(0, len(acc),1))\n",
    "    acc['Timestamp'] = acc['Timestamp'].apply(lambda x: x/acc_samp_rate+start_time)\n",
    "    acc['Second'] = acc['Timestamp']\n",
    "    acc = acc.set_index('Timestamp')\n",
    "    acc['Second'] = acc['Second'].apply(lambda x: x-acc.index[0])\n",
    "    acc.columns = ['Acceleration_X','Acceleration_Y','Acceleration_Z','Second']\n",
    "    acc = acc.reset_index(inplace=False)\n",
    "    \n",
    "    data_dict = {'Date':date, 'HR':hr, 'EDA':eda, 'TEMP':temp, 'IBI':ibi, 'BVP':bvp,'ACC':acc}\n",
    "    return data_dict \n",
    "\n",
    "def E4sync_offset(a):\n",
    "\n",
    "    eda = a['EDA']\n",
    "    temp = a['TEMP']\n",
    "    bvp = a['BVP']\n",
    "    acc = a['ACC']\n",
    "\n",
    "    \n",
    "    t_eda = eda['Timestamp'].iat[-1]\n",
    "    t_temp = temp['Timestamp'].iat[-1]\n",
    "    t_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t_acc = acc['Timestamp'].iat[-1]\n",
    "    t0 = 0\n",
    "\n",
    "    print(t_eda < t_temp and t_eda < t_bvp and t_eda < t_acc)\n",
    "    print(t_temp < t_eda and t_temp < t_bvp and t_temp < t_acc)\n",
    "    print(t_bvp < t_eda and t_bvp < t_temp and t_bvp < t_acc)\n",
    "    print(t_acc < t_eda and t_acc < t_temp and t_acc < t_bvp)\n",
    "\n",
    "    if t_eda < t_temp and t_eda < t_bvp and t_eda < t_acc :\n",
    "        print(1)\n",
    "        t1_loc = temp.loc[temp['Timestamp'] == round(t_eda, 2)]\n",
    "        t2_loc = bvp.loc[bvp['Timestamp'] == round(t_eda, 2)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] == round(t_eda, 2)]\n",
    "        offset_eda = eda\n",
    "        offset_temp = temp.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "        \n",
    "    elif t_temp < t_eda and t_temp < t_bvp and t_temp < t_acc :\n",
    "        print(2)\n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_temp, 0)]\n",
    "        t2_loc = bvp.loc[bvp['Timestamp'] == round(t_temp, 2)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] == round(t_temp, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "        \n",
    "    elif t_bvp < t_eda and t_bvp < t_temp and t_bvp < t_acc :\n",
    "        print(3)\n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_bvp, 0)]\n",
    "        t2_loc = temp.loc[temp['Timestamp'] == round(t_bvp, 0)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] ==round(t_bvp, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_bvp = bvp\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "    \n",
    "    elif t_acc < t_eda and t_acc < t_temp and t_acc < t_bvp :\n",
    "        print(4)\n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_acc, 0)]\n",
    "        t2_loc = temp.loc[temp['Timestamp'] == round(t_acc, 0)]\n",
    "        t3_loc = bvp.loc[bvp['Timestamp'] == round(t_acc, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_acc = acc\n",
    "    \n",
    "    # E4_acc separation\n",
    "    l1 = offset_acc.iloc[:,0:1]\n",
    "    l2 = offset_acc.iloc[:,4]\n",
    "\n",
    "    Acceleration_x = offset_acc.iloc[:,1]\n",
    "    Acceleration_X1 = pd.concat([l1,Acceleration_x], axis=1, join='outer')\n",
    "    Acceleration_X = pd.concat([Acceleration_X1,l2], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y = offset_acc.iloc[:,2]\n",
    "    Acceleration_Y1 = pd.concat([l1,Acceleration_y], axis=1, join='outer')\n",
    "    Acceleration_Y = pd.concat([Acceleration_Y1,l2], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z = offset_acc.iloc[:,3]\n",
    "    Acceleration_Z1 = pd.concat([l1,Acceleration_z], axis=1, join='outer')\n",
    "    Acceleration_Z = pd.concat([Acceleration_Z1,l2], axis=1, join='outer')\n",
    "\n",
    "\n",
    "\n",
    "    eda2 = offset_eda.iloc[:,1]\n",
    "    eda2 = np.expand_dims(eda2.values, axis = 1)\n",
    "    temp2 = offset_temp.iloc[:,1]\n",
    "    temp2 = np.expand_dims(temp2.values, axis = 1)\n",
    "    bvp2 = offset_bvp.iloc[:,1]\n",
    "    bvp2 = np.expand_dims(bvp2.values, axis = 1)\n",
    "    Accx = Acceleration_X.iloc[:,1]\n",
    "    Accx = np.expand_dims(Accx.values, axis = 1)\n",
    "    Accy = Acceleration_Y.iloc[:,1]\n",
    "    Accy = np.expand_dims(Accy.values, axis = 1)\n",
    "    Accz = Acceleration_Z.iloc[:,1]\n",
    "    Accz = np.expand_dims(Accz.values, axis = 1)\n",
    "\n",
    "    acc2 = offset_acc.iloc[:,1:4]\n",
    "    acc2 = acc2.values\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    E4_to_dic[\"EDA\"] = offset_eda\n",
    "    E4_to_dic[\"TEMP\"] = offset_temp\n",
    "    E4_to_dic[\"BVP\"] = offset_bvp\n",
    "    #E4_to_dic[\"Acceleration_X\"] = Accx\n",
    "    #E4_to_dic[\"Acceleration_Y\"] = Accy\n",
    "    #E4_to_dic[\"Acceleration_Z\"] = Accz \n",
    "    E4_to_dic[\"ACC\"] = offset_acc\n",
    "    \n",
    "    if False:\n",
    "        with open(filepath+'_E4.pkl', 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return E4_to_dic\n",
    "\n",
    "def Hexsync_offset(ecg,br,accx,accy, accz):\n",
    "    t_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t_br = br['Timestamp'].iat[-1]\n",
    "    t_accx = accx['Timestamp'].iat[-1]\n",
    "    t_accy = accy['Timestamp'].iat[-1]\n",
    "    t0 = 0\n",
    "    \n",
    "\n",
    "    if t_ecg <= t_br and t_ecg <= t_accx and t_ecg<=t_accy :\n",
    "        t1_loc = br.loc[round(br['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        t2_loc = accx.loc[round(accx['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        t3_loc = accy.loc[round(accy['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        offset_ecg = ecg\n",
    "        offset_br = br.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_accx = accx.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accy = accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz = accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_br <= t_ecg and t_br <= t_accx and t_br<=t_accy :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_br, 7)]\n",
    "        t2_loc = accx.loc[accx['Timestamp'] == round(t_br, 7)]\n",
    "        t3_loc = accy.loc[accy['Timestamp'] == round(t_br, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_br = br\n",
    "        offset_accx = accx.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accy= accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz= accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_accx <= t_ecg and t_accx <= t_br and t_accx <= t_accy :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_accx, 7)]\n",
    "        t2_loc = br.loc[br['Timestamp'] == round(t_accx, 7)]\n",
    "        t3_loc = accy.loc[accy['Timestamp'] ==round(t_accx, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_br = br.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accx = accx\n",
    "        offset_accy = accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz = accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_accy <= t_ecg and t_accy <= t_br and t_accy <= t_accx :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_accy, 7)]\n",
    "        t2_loc = br.loc[br['Timestamp'] == round(t_accy, 7)]\n",
    "        t3_loc = accx.loc[accx['Timestamp'] == round(t_accy, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_br = br.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accx = accx.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accy = accy\n",
    "        offset_accz = accz\n",
    "\n",
    "    ecg2 = offset_ecg.iloc[:,1]\n",
    "    ecg2 = np.expand_dims(ecg2.values, axis = 1)\n",
    "    accx2 = offset_accx.iloc[:,1]\n",
    "    accx2 = np.expand_dims(accx2.values, axis = 1)\n",
    "    accy2 = offset_accy.iloc[:,1]\n",
    "    accy2 = np.expand_dims(accy2.values, axis = 1)\n",
    "    accz2 = offset_accz.iloc[:,1]\n",
    "    accz2 = np.expand_dims(accz2.values, axis = 1)\n",
    "    br2 = offset_br.iloc[:,1]\n",
    "    br2 = np.expand_dims(br2.values, axis = 1)\n",
    "\n",
    "    hx_to_dic = {}\n",
    "    hx_to_dic[\"ECG\"] = offset_ecg\n",
    "    hx_to_dic[\"BR\"] = offset_br\n",
    "    hx_to_dic[\"ACCX\"] = offset_accx\n",
    "    hx_to_dic[\"ACCY\"] = offset_accy\n",
    "    hx_to_dic[\"ACCZ\"] = offset_accz\n",
    " \n",
    "    \n",
    "    \n",
    "    accx3 = accx.iloc[:,0:2]\n",
    "    accy3 = accy.iloc[:,1]\n",
    "    accz3 = accz.iloc[:,1:3]\n",
    "    hx_to_dic['ACC_n'] = pd.concat([accx3,\n",
    "                                    accy3,\n",
    "                                    accz3],\n",
    "                                    axis=1, join='outer')\n",
    "    #hx_to_dic['ACC_n'] = np.concatenate([accx2, accy2, accz2], axis=1, join='outer')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        string2 = fhex+'_hex.pkl'\n",
    "        with open(string2, 'wb') as handle:\n",
    "            pickle.dump(hx_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    return hx_to_dic\n",
    "    \n",
    "def doublesync_offset(ecg, bvp):\n",
    "    \n",
    "    t1_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t1_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t0_bvp = bvp['Timestamp'].iat[0]\n",
    "    t0_ecg = ecg['Timestamp'].iat[0]\n",
    "\n",
    "    if t0_ecg < t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(after = t1_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0])\n",
    "        \n",
    "        \n",
    "    elif t0_ecg > t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(after = t1_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg < t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        offset_bvp = bvp\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "    \n",
    "    elif t0_ecg > t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "        offset_ecg = ecg\n",
    "    \n",
    "        #offset = {'offset_ecg':offset_ecg, 'offset_bvp':offset_bvp}\n",
    "        #return offset\n",
    "    \n",
    "        \n",
    "    ecg2 = offset_ecg.iloc[:,1]\n",
    "    ecg2 = np.expand_dims(ecg2.values, axis = 1)\n",
    "    bvp2 = offset_bvp.iloc[:,1]\n",
    "    bvp2 = np.expand_dims(bvp2.values, axis = 1)\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    E4_to_dic[\"ECG\"] = ecg2\n",
    "    E4_to_dic[\"BVP\"] = bvp2\n",
    "\n",
    "    if False:\n",
    "        string3 = fhex+'_ECG&BVP.pkl'\n",
    "        with open(string3, 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return offset_ecg,offset_bvp\n",
    "\n",
    "def accsync_offset(ecg, bvp):\n",
    "    \n",
    "    t1_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t1_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t0_bvp = bvp['Timestamp'].iat[0]\n",
    "    t0_ecg = ecg['Timestamp'].iat[0]\n",
    "\n",
    "    if t0_ecg < t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "\n",
    "        offset_bvp = bvp.truncate(after = t1_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg > t0_bvp and t1_ecg > t1_bvp:\n",
    "\n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg,1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp,1)].head(1)\n",
    "\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(after = t1_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg < t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        \n",
    "        offset_bvp = bvp\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "    \n",
    "    elif t0_ecg > t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        \n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "        offset_ecg = ecg\n",
    "    \n",
    "        #offset = {'offset_ecg':offset_ecg, 'offset_bvp':offset_bvp}\n",
    "        #return offset\n",
    "\n",
    "\n",
    "    offset_acc2 = offset_ecg\n",
    "    offset_ACC2 = offset_bvp\n",
    "    offset_ACC2.iloc[:,2]\n",
    "\n",
    "    l3 = offset_acc2.iloc[:,0:1]\n",
    "    l4 = offset_acc2.iloc[:,4]\n",
    "\n",
    "    Acceleration_x2 = offset_acc2.iloc[:,1]\n",
    "    Acceleration_X12 = pd.concat([l3,Acceleration_x2], axis=1, join='outer')\n",
    "    Acceleration_X2 = pd.concat([Acceleration_X12,l4], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y2 = offset_acc2.iloc[:,2]\n",
    "    Acceleration_Y12 = pd.concat([l3,Acceleration_y2], axis=1, join='outer')\n",
    "    Acceleration_Y2 = pd.concat([Acceleration_Y12,l4], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z2 = offset_acc2.iloc[:,3]\n",
    "    Acceleration_Z12 = pd.concat([l3,Acceleration_z2], axis=1, join='outer')\n",
    "    Acceleration_Z2 = pd.concat([Acceleration_Z12,l4], axis=1, join='outer')\n",
    "\n",
    "    l5 = offset_ACC2.iloc[:,0:1]\n",
    "    l6 = offset_ACC2.iloc[:,4]\n",
    "\n",
    "    Acceleration_x3 = offset_ACC2.iloc[:,1]\n",
    "    Acceleration_X13 = pd.concat([l5,Acceleration_x3], axis=1, join='outer')\n",
    "    Acceleration_X3 = pd.concat([Acceleration_X13,l6], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y3 = offset_ACC2.iloc[:,2]\n",
    "    Acceleration_Y13 = pd.concat([l5,Acceleration_y3], axis=1, join='outer')\n",
    "    Acceleration_Y3 = pd.concat([Acceleration_Y13,l6], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z3 = offset_ACC2.iloc[:,3]\n",
    "    Acceleration_Z13 = pd.concat([l5,Acceleration_z3], axis=1, join='outer')\n",
    "    Acceleration_Z3 = pd.concat([Acceleration_Z13,l6], axis=1, join='outer')\n",
    "\n",
    "\n",
    "\n",
    "    acc22 = offset_acc2.iloc[:,1:4]\n",
    "    acc22 = acc22.values\n",
    "    ACC22 = offset_ACC2.iloc[:,1:4]\n",
    "    ACC22 = ACC22.values\n",
    "\n",
    "    ax = Acceleration_X2.iloc[:,1]\n",
    "    ax = np.expand_dims(ax.values, axis = 1)\n",
    "    ay = Acceleration_Y2.iloc[:,1]\n",
    "    ay = np.expand_dims(ay.values, axis = 1)\n",
    "    az = Acceleration_Z2.iloc[:,1]\n",
    "    az = np.expand_dims(az.values, axis = 1)\n",
    "\n",
    "    Ax = Acceleration_X3.iloc[:,1]\n",
    "    Ax = np.expand_dims(Ax.values, axis = 1)\n",
    "    Ay = Acceleration_Y3.iloc[:,1]\n",
    "    Ay = np.expand_dims(Ay.values, axis = 1)\n",
    "    Az = Acceleration_Z3.iloc[:,1]\n",
    "    Az = np.expand_dims(Az.values, axis = 1)\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    #E4_to_dic[\"accx_e4\"] = ax\n",
    "    #E4_to_dic[\"accy_e4\"] = ay\n",
    "    #E4_to_dic[\"accz_e4\"] = az\n",
    "\n",
    "    #E4_to_dic[\"accx_hx\"] = Ax\n",
    "    #E4_to_dic[\"accy_hx\"] = Ay\n",
    "    #E4_to_dic[\"accz_hx\"] = Az\n",
    "\n",
    "    E4_to_dic[\"acc_e4\"] = offset_bvp\n",
    "    E4_to_dic[\"acc_hx\"] = offset_acc2\n",
    "\n",
    "    acc_dic = E4_to_dic\n",
    "\n",
    "    if False:\n",
    "        string4 = fhex+'_accE4&accHex.pkl'\n",
    "        with open(string4, 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return acc_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'incomplete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb Cell 79\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     all_p_metadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#incomplete = [16]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m incomplete:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     p_path \u001b[39m=\u001b[39m radwear_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mParticipant \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(p)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/fb_code/fb_testing.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     p_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'incomplete' is not defined"
     ]
    }
   ],
   "source": [
    "with open(radwear_path + 'all_p_metadata.json', 'rb') as f:\n",
    "    all_p_metadata = json.load(f)\n",
    "\n",
    "#incomplete = [16]\n",
    "\n",
    "\n",
    "for p in incomplete:\n",
    "    p_path = radwear_path + 'Participant ' + str(p)\n",
    "    p_df = pd.DataFrame()\n",
    "\n",
    "    # check if file exist\n",
    "    a = (\n",
    "        'available.'\n",
    "        if os.path.isfile(p_path + '/p_' + str(p) + '.pkl')\n",
    "        else ' not available.'\n",
    "    )\n",
    "    print('pickle file for participant ' + str(p) + ' is ' + a)\n",
    "\n",
    "    # all_p[p] = p_data ## this takes too much memory so i will just load each p when needed\n",
    "    if not (os.path.isfile(radwear_path + 'p_' + str(p) + '.pkl')) or True:\n",
    "        with open(p_path + '/p_' + str(p) + '.pkl', 'rb') as f:\n",
    "            p_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = pd.read_pickle(radwear_path + 'p_' + str(16) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(p_data['calib']['BR']))\n",
    "print(len(p_data['calib']['EDA']))\n",
    "print(len(p_data['calib']['HR']))\n",
    "print(len(p_data['calib']['ECG']))\n",
    "print(len(p_data['calib']['TEMP']))\n",
    "print(len(p_data['calib']['ACC_hx']))\n",
    "print(len(p_data['calib']['ACC_e4']))\n",
    "print(len(p_data['calib']['BVP']))\n",
    "print(len(p_data['calib']['rot_label']))\n",
    "tot_p_data = (\n",
    "    len(p_data['calib']['BR'])\n",
    "    + len(p_data['calib']['ECG'])\n",
    "    + len(p_data['calib']['EDA'])\n",
    "    + len(p_data['calib']['HR'])\n",
    "    + len(p_data['calib']['TEMP'])\n",
    "    + len(p_data['calib']['ACC_hx'])\n",
    "    + len(p_data['calib']['ACC_e4'])\n",
    "    + len(p_data['calib']['BVP'])\n",
    "    + len(p_data['calib']['rot_label'])\n",
    ")\n",
    "print('total length of calib', tot_p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_redcap(path, participants, force_update=False):\n",
    "    '''\n",
    "    Written by A. Alkurdi and X. Fan\n",
    "    usage: process_redcap(path, participants, force_update = False) wow\n",
    "    If force_update is set to True, the function will update the pickle file containing the redcap data. use when redcap data is updated.\n",
    "    returns:\n",
    "        label_df: pandas dataframe containing all the relevant information from the redcap csv files\n",
    "        and saves the dataframe as a pickle file in the same directory as the redcap csv files. Enjoy!\n",
    "    '''\n",
    "    participants = participants\n",
    "    redcap_path = path\n",
    "    relevant_keys = [\n",
    "        'daily_check_in_date',\n",
    "        'daily_feeling',\n",
    "        'daily_distressed_level',\n",
    "        'daily_anxious_level',\n",
    "        'daily_overall_anxiety',\n",
    "        'daily_covid_contact',\n",
    "        'daily_covid_team_contact',\n",
    "    ]\n",
    "    partcipant_days = {\n",
    "        4: [0, 0],\n",
    "        5: [0, 8],\n",
    "        7: [11, 10],\n",
    "        9: [10, 10],\n",
    "        12: [8, 10],\n",
    "        14: [9, 10],\n",
    "        16: [9, 0],\n",
    "        17: [10, 10],\n",
    "        18: [0, 0],\n",
    "        20: [0, 0],\n",
    "        21: [9, 0],\n",
    "    }\n",
    "    all_d = {}\n",
    "\n",
    "    my_file = Path(redcap_path + 'redcap_dict.pkl')\n",
    "    if my_file.is_file() and not force_update:\n",
    "        print('redcap dict exists')\n",
    "        with open(redcap_path + 'redcap_dict.pkl', 'rb') as f:\n",
    "            all_participants_redcap_dict = pickle.load(f)\n",
    "            # all_participants_dict = pd.read_pickle(f)\n",
    "    else:\n",
    "        if force_update:\n",
    "            print('redcap dict exists but update forced')\n",
    "        else:\n",
    "            print('redcap dict does not exist')\n",
    "\n",
    "        for participant in participants:\n",
    "            participant__d = {}\n",
    "            path = redcap_path + 'Participant_' + str(participant) + '_RADWearStudy.csv'\n",
    "            try:\n",
    "                participant__d['global_info_dict'] = create_global_dict(path)\n",
    "                participant__d['local_info_dict'] = get_daily_info_dict(path)\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                print(\n",
    "                    'MISTAKE! REDCap csv file for participant {} not found'.format(\n",
    "                        participant\n",
    "                    )\n",
    "                )  # Lord Jaraxxus, Eredar Lord of the Burning Legion, reference.\n",
    "                participant__d['global_info_dict'] = []\n",
    "                participant__d['local_info_dict'] = []\n",
    "            all_d[participant] = participant__d\n",
    "\n",
    "        with open(redcap_path + 'redcap_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(all_d, f)\n",
    "            all_participants_redcap_dict = all_d\n",
    "\n",
    "    my_file = Path(redcap_path + 'redcap_df.pkl')\n",
    "    if my_file.is_file() and not force_update:\n",
    "        print('redcap df exists')\n",
    "        with open(redcap_path + 'redcap_df.pkl', 'rb') as f:\n",
    "            # all_participants_redcap_dict = pickle.load(f)\n",
    "            label_df = pickle.load(f)\n",
    "            print('redcap df pickle loaded')\n",
    "    else:\n",
    "        if force_update:\n",
    "            print('redcap df exists but update forced')\n",
    "        else:\n",
    "            print('redcap df does not exist')\n",
    "        subject_labels = []\n",
    "        for key in all_participants_redcap_dict.keys():\n",
    "            subject_data = []\n",
    "            for instance in all_participants_redcap_dict[key]['local_info_dict'].keys():\n",
    "                key_data = []\n",
    "                for rel_key in relevant_keys:\n",
    "                    # Ensure that the relevant keys exist to avoid runtime error\n",
    "                    if (\n",
    "                        len(\n",
    "                            all_participants_redcap_dict[key]['local_info_dict'][\n",
    "                                instance\n",
    "                            ]\n",
    "                        )\n",
    "                        > 0\n",
    "                    ):\n",
    "                        key_data.append(\n",
    "                            all_participants_redcap_dict[key]['local_info_dict'][\n",
    "                                instance\n",
    "                            ][rel_key]\n",
    "                        )\n",
    "                    else:\n",
    "                        break\n",
    "                # Ensure that subject labels contain all the necessary information\n",
    "                if len(key_data) == len(relevant_keys):\n",
    "                    key_data.insert(0, key)\n",
    "                    subject_labels.append(key_data)\n",
    "        label_df = pd.DataFrame(subject_labels)\n",
    "        label_df.rename(\n",
    "            columns={\n",
    "                0: 'participant',\n",
    "                1: 'daily_check_in_date',\n",
    "                2: 'daily_anxious_level',\n",
    "                3: 'daily_overall_anxiety',\n",
    "                4: 'daily_distressed_level',\n",
    "                5: 'daily_feeling',\n",
    "                6: 'daily_covid_contact',\n",
    "                7: 'daily_covid_team_contact',\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Include Covid-19 contact\n",
    "        attributes = [\n",
    "            'daily_anxious_level',\n",
    "            'daily_overall_anxiety',\n",
    "            'daily_distressed_level',\n",
    "            'daily_feeling',\n",
    "            'daily_covid_contact',\n",
    "            'daily_covid_team_contact',\n",
    "        ]\n",
    "        label_df.loc[label_df['daily_covid_team_contact'] == ''] = '0'\n",
    "        label_df[attributes] = label_df[attributes].astype(int)\n",
    "        np_label = np.array(label_df[attributes])\n",
    "        redcap_labels = attributes\n",
    "\n",
    "        with open(redcap_path + 'redcap_df.pkl', 'wb') as f:\n",
    "            pickle.dump(label_df, f)\n",
    "            print('redcap df created, pickle dumped')\n",
    "\n",
    "    # fixing for p7 missing data\n",
    "    label_df.at[28, 'daily_check_in_date'] = '2022-09-12'\n",
    "    label_df.at[30, 'daily_check_in_date'] = '2022-09-13'\n",
    "    label_df.loc[29], label_df.loc[30] = (\n",
    "        label_df.loc[30].copy(),\n",
    "        label_df.loc[29].copy(),\n",
    "    )\n",
    "    label_df.drop([44], inplace=True)\n",
    "    return label_df\n",
    "\n",
    "\n",
    "def create_global_dict(path):\n",
    "    global_info_dict = {}\n",
    "    with open(path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['redcap_event_name'][:5] == 'daily':\n",
    "                break\n",
    "            for key in row.keys():\n",
    "                if (\n",
    "                    key != 'subject_id'\n",
    "                    and key != 'redcap_event_name'\n",
    "                    and key != 'redcap_repeat_instrument'\n",
    "                    and row[key] != ''\n",
    "                ):\n",
    "                    global_info_dict[key] = row[key]\n",
    "    return global_info_dict\n",
    "\n",
    "\n",
    "def get_daily_info_dict(path):\n",
    "    daily_info_dict = {}\n",
    "    event_type = [\n",
    "        'daily_checkin_timestamp',\n",
    "        'daily_check_in_date',\n",
    "        'synce_reminder',\n",
    "        'daily_feeling',\n",
    "        'daily_distressed_level',\n",
    "        'daily_covid_contact',\n",
    "        'daily_covid_team_contact',\n",
    "        'daily_contact_in_rotation',\n",
    "        'daily_anxious_level',\n",
    "        'daily_overall_anxiety',\n",
    "        'tag_event_0',\n",
    "        'daily_anxious_event',\n",
    "        'tag_event_1',\n",
    "        'daily_other_event_detail',\n",
    "        'daily_time_anxiety',\n",
    "        'daily_anxiety_recall',\n",
    "        'daily_anxiety_event',\n",
    "        'daily_checkin_complete',\n",
    "    ]\n",
    "    with open(path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['redcap_event_name'][:5] != 'daily':\n",
    "                continue\n",
    "            unique_id = (\n",
    "                row['redcap_event_name']\n",
    "                + '/'\n",
    "                + row['redcap_repeat_instrument']\n",
    "                + '/instance'\n",
    "                + row['redcap_repeat_instance']\n",
    "            )\n",
    "            nested_d = {}\n",
    "            counter = 0\n",
    "            for key in row.keys():\n",
    "                if (key[-9:] != 'timestamp' or row[key] == '') and counter == 0:\n",
    "                    continue\n",
    "                if counter == len(event_type):\n",
    "                    break\n",
    "                nested_d[event_type[counter]] = row[key]\n",
    "                counter += 1\n",
    "\n",
    "            daily_info_dict[unique_id] = nested_d\n",
    "    return daily_info_dict\n",
    "\n",
    "\n",
    "def process_redcap_calibration(\n",
    "    radwear_path='/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/',\n",
    "):\n",
    "    # check if file exists\n",
    "    my_file = Path(radwear_path + 'redcap_calib_dict.pkl')\n",
    "    if my_file.is_file():\n",
    "        print('redcap calibration dict exists')\n",
    "        with open(radwear_path + 'redcap_calib_dict.pkl', 'rb') as f:\n",
    "            all_calib_df = pickle.load(f)\n",
    "            print('redcap calibration dict pickle loaded')\n",
    "    else:\n",
    "        print('redcap calibration dict does not exist')\n",
    "        print('BUT WORRY NOT, WE WILL CREATE IT FOR YOU')\n",
    "\n",
    "        with open(radwear_path + 'all_p_metadata.json') as f:\n",
    "            all_p_metadata = json.load(f)\n",
    "\n",
    "            participant__d = {}\n",
    "            global_info_dict = {}\n",
    "            all_calib_df = pd.DataFrame()\n",
    "            redcap_path = radwear_path + 'REDCap responses/'\n",
    "\n",
    "            # with open(radwear_path+'all_p_metadata.json', 'rb') as f:\n",
    "            #            all_p_metadata = json.load(f)\n",
    "            list_of_participants = all_p_metadata['list of participant IDs']\n",
    "            for i in list_of_participants:\n",
    "                print(\n",
    "                    f' participant {i} status is {all_p_metadata[str(i)][\"status\"]} with e4 file {all_p_metadata[str(i)][\"calibration\"][1]} and hx file {all_p_metadata[str(i)][\"calibration\"][0]}'\n",
    "                )\n",
    "\n",
    "            for participant in list_of_participants:\n",
    "                path = (\n",
    "                    redcap_path\n",
    "                    + 'Participant_'\n",
    "                    + str(participant)\n",
    "                    + '_RADWearStudy.csv'\n",
    "                )\n",
    "                with open(path, newline='') as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        if row['redcap_event_name'][:5] == 'basel':\n",
    "                            for key in row.keys():\n",
    "                                # print(key[-2:])\n",
    "                                # if key != 'subject_id' and key != 'redcap_event_name' and key != 'redcap_repeat_instrument' and row[key] != '':\n",
    "                                # if key[-2:] == 'x2' or key[-2:] == 'y6':\n",
    "                                if '_cal' in key:\n",
    "                                    global_info_dict[key] = row[key]\n",
    "                                    # print('row[key]: ', row[key])\n",
    "                daily_info_dict = {}\n",
    "                event_type = [\n",
    "                    'calm_cal_x2',\n",
    "                    'secure_cal_x2',\n",
    "                    'tense_cal_x2',\n",
    "                    'regretful_cal_x2',\n",
    "                    'ease_cal_x2',\n",
    "                    'upset_cal_x2',\n",
    "                    'worrying_cal_x2',\n",
    "                    'rested_cal_x2',\n",
    "                    'anxious_cal_x2',\n",
    "                    'comfort_cal_x2',\n",
    "                    'self_conf_cal_x2',\n",
    "                    'nervous_cal_x2',\n",
    "                    'jittery_cal_x2',\n",
    "                    'strun_cal_x2',\n",
    "                    'relaxed_cal_x2',\n",
    "                    'content_cal_x2',\n",
    "                    'worried_cal_x2',\n",
    "                    'excited_cal_x2',\n",
    "                    'joyful_cal_x2',\n",
    "                    'pleasant_cal_x2',\n",
    "                    'calm_cal_y6',\n",
    "                    'tense_cal_y6',\n",
    "                    'upset_cal_y6',\n",
    "                    'relax_cal_y6',\n",
    "                    'content_cal_y6',\n",
    "                    'worried_cal_y6',\n",
    "                    'calm_cal_y6_post',\n",
    "                    'tense_cal_y6_post',\n",
    "                    'upset_cal_y6_post',\n",
    "                    'relax_cal_y6_post',\n",
    "                    ' content_cal_y6_post',\n",
    "                    'worry_cal_y6_post',\n",
    "                    'calm_cal_y6_cold',\n",
    "                    'tense_cal_y6_cold',\n",
    "                    'upset_cal_y6_cold',\n",
    "                    'relax_cal_y6_cold',\n",
    "                    'content_cal_y6_cold',\n",
    "                    'worry_cal_y6_cold',\n",
    "                    'baseline_calibration_complete',\n",
    "                ]\n",
    "                with open(path, newline='') as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        if row['redcap_event_name'][:5] != 'basel':\n",
    "                            continue\n",
    "                        unique_id = (\n",
    "                            row['redcap_event_name']\n",
    "                            + '/'\n",
    "                            + row['redcap_repeat_instrument']\n",
    "                            + '/instance'\n",
    "                            + row['redcap_repeat_instance']\n",
    "                        )\n",
    "                        nested_d = {}\n",
    "                        counter = 0\n",
    "                        for key in row.keys():\n",
    "                            if (\n",
    "                                key[-9:] != 'timestamp' or row[key] == ''\n",
    "                            ) and counter == 0:\n",
    "                                continue\n",
    "                            if counter == len(event_type):\n",
    "                                break\n",
    "                            nested_d[event_type[counter]] = row[key]\n",
    "                            counter += 1\n",
    "\n",
    "                        daily_info_dict[unique_id] = nested_d\n",
    "\n",
    "                subject_labels = []\n",
    "                subject_data = []\n",
    "                for instance in daily_info_dict.keys():\n",
    "                    key_data = []\n",
    "                    for rel_key in event_type:\n",
    "                        # Ensure that the relevant keys exist to avoid runtime error\n",
    "                        if len(daily_info_dict[instance]) > 0:\n",
    "                            key_data.append(daily_info_dict[instance][rel_key])\n",
    "                        else:\n",
    "                            break\n",
    "                    # Ensure that subject labels contain all the necessary information\n",
    "                    if len(key_data) == len(event_type):\n",
    "                        key_data.insert(0, 12)\n",
    "                        subject_labels.append(key_data)\n",
    "                label_df = pd.DataFrame(subject_labels)\n",
    "                label_df.columns = list(global_info_dict.keys())\n",
    "                label_df['participant'] = np.ones(len(label_df)) * participant\n",
    "                all_calib_df = pd.concat([all_calib_df, label_df])\n",
    "\n",
    "            with open(redcap_path + 'redcap_calib_dict.pkl', 'wb') as f:\n",
    "                pickle.dump(all_calib_df, f)\n",
    "            # get HADs survey, baseline calibration\n",
    "    return all_calib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redcap dict exists\n",
      "redcap df exists\n",
      "redcap df pickle loaded\n"
     ]
    }
   ],
   "source": [
    "redcap_path = radwear_path + 'REDCap responses/'\n",
    "with open(radwear_path + 'all_p_metadata.json', 'rb') as f:\n",
    "    all_p_metadata = json.load(f)\n",
    "# load all participant redcap data\n",
    "redcap_df = process_redcap(redcap_path, all_p_metadata['list of participant IDs'])\n",
    "redcap_calib_dict = pd.read_pickle(\n",
    "    radwear_path + '/REDCap responses/redcap_calib_dict.pkl'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1725908/2231213626.py:98: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  b_ph = pd.concat([insp, exp])\n"
     ]
    }
   ],
   "source": [
    "p = 16\n",
    "p_path = radwear_path + 'Participant ' + str(p)\n",
    "e4sn = all_p_metadata[str(p)]['e4sn']\n",
    "calibration_files = all_p_metadata[str(p)]['calibration']\n",
    "p_calib = {}\n",
    "\n",
    "# load calibration data\n",
    "e4_num = all_p_metadata[str(p)]['e4sn'] + '_' + all_p_metadata[str(p)]['calibration'][0]\n",
    "hx_num = str(all_p_metadata[str(p)]['calibration'][1])\n",
    "p_calib[p] = read_sync_return(p_path, e4_num, hx_num)\n",
    "p_calib[p]['rot_label'] = rot_anx_dict['calibration'] * np.ones(len(p_calib[p]['ECG'])) # add label to designate calibration segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# skips first tag for the following p:  7 16 17 18 19 21\n",
    "# skip first 2 for p 18\n",
    "# p14 add 60 seconds to the last tag\n",
    "tag_info = {4: '',\n",
    "            5: '',\n",
    "            7: 'skip 0',\n",
    "            9: '',\n",
    "            12: '',\n",
    "            14:'add _60s_to_3_to_get_ 4',\n",
    "            16:'skip 0',\n",
    "            17:'skip 0',\n",
    "            18:'skip 01',\n",
    "            19:'skip 0',\n",
    "            21:'skip 0',\n",
    "    }    \n",
    "#pd.options.display.float_format = '{:.2f}'.format\n",
    "list_of_participants = all_p_metadata['list of participant IDs']\n",
    "for p in list_of_participants:\n",
    "    e4sn = all_p_metadata[str(p)]['e4sn']\n",
    "    e4_num = (\n",
    "        all_p_metadata[str(p)]['e4sn'] + '_' + all_p_metadata[str(p)]['calibration'][0]\n",
    "    )\n",
    "    tags = pd.read_csv(\n",
    "        radwear_path + 'Participant ' + str(p) + '/' + e4_num + '/tags.csv', header=None\n",
    "    )\n",
    "    tag_command = tag_info[p].split(' ')\n",
    "    #print(tag_command[0])\n",
    "    if len(tag_command[-1]) ==1 and tag_command[0] == 'skip':\n",
    "        tags = tags.drop(int(tag_command[-1]))\n",
    "    elif len(tag_command[-1]) ==2:\n",
    "        #print(tag_command[-1][0], tag_command[-1][1])\n",
    "        tags.drop([int(tag_command[-1][0]),int(tag_command[-1][1])], axis=0 ,inplace=True)\n",
    "        #display(tags)\n",
    "    elif len(tag_command[-1]) ==1 and tag_command[0] == 'add':\n",
    "        new_row = pd.DataFrame(tags.values[-1]+60,columns=tags.columns)\n",
    "        tags = tags.append(new_row, ignore_index=True)\n",
    "    else:\n",
    "        tag_command = None\n",
    "    tags.reset_index(drop=True, inplace=True)\n",
    "    #display(tags)    \n",
    "    \n",
    "    p_calib[p]['calib_label'] = p_calib[p]['BVP'].copy() # add label to designate calibration segment\n",
    "    p_calib[p]['calib_label'].drop(columns=['BVP','Second'], inplace=True)\n",
    "    p_calib[p]['calib_label']['calib_label'] = 0\n",
    "    display(p_calib[p]['calib_label'])\n",
    "    p_calib[p]['calib_label']['calib_label'][(p_calib[p]['calib_label']['Timestamp'] > tags.iloc[0].values[0]) & \\\n",
    "                                            (p_calib[p]['calib_label']['Timestamp'] < tags.iloc[1].values[0])] \\\n",
    "                                                = calib_dict['meditation']\n",
    "    p_calib[p]['calib_label']['calib_label'][(p_calib[p]['calib_label']['Timestamp'] > tags.iloc[2].values[0]) & \\\n",
    "                                                (p_calib[p]['calib_label']['Timestamp'] < tags.iloc[3].values[0])] \\\n",
    "                                                    = calib_dict['cpt']\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300.0, 1380.0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(*p_calib[p]['calib_label']['calib_label'].values, sep='\\n')\n",
    "plt.plot((p_calib[p]['calib_label']['Timestamp']-p_calib[p]['calib_label']['Timestamp'].iloc[0]).values, p_calib[p]['calib_label']['calib_label'].values)\n",
    "plt.xlim([1300,1380])\n",
    "# looks good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
