{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.arrays' has no attribute 'ArrowStringArray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/alkurdi/Desktop/Vansh/E4+Hexoskin_preprocess-final-dask.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/E4%2BHexoskin_preprocess-final-dask.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39mmodified by abdul alkurdi; 10/05/2023\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/E4%2BHexoskin_preprocess-final-dask.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/E4%2BHexoskin_preprocess-final-dask.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/E4%2BHexoskin_preprocess-final-dask.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/alkurdi/Desktop/Vansh/E4%2BHexoskin_preprocess-final-dask.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#import cudf\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFwesad/lib/python3.8/site-packages/dask/dataframe/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pyarrow_compat\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m compute\n\u001b[1;32m      4\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m backends, dispatch, rolling\n",
      "File \u001b[0;32m~/anaconda3/envs/TFwesad/lib/python3.8/site-packages/dask/dataframe/_pyarrow_compat.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m         copyreg\u001b[39m.\u001b[39mdispatch_table[type_] \u001b[39m=\u001b[39m reduce_arrowextensionarray\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39m# Only `string[pyarrow]` is implemented, so just patch that\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     copyreg\u001b[39m.\u001b[39mdispatch_table[pd\u001b[39m.\u001b[39;49marrays\u001b[39m.\u001b[39;49mArrowStringArray] \u001b[39m=\u001b[39m reduce_arrowextensionarray\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.arrays' has no attribute 'ArrowStringArray'"
     ]
    }
   ],
   "source": [
    "'modified by abdul alkurdi; 10/05/2023'\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "#import cudf\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import correlate\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from datetime import datetime\n",
    "#import heartpy as hp\n",
    "import json\n",
    "%matplotlib notebook\n",
    "# import neurokit2 as nk\n",
    "\n",
    "import process_redcap \n",
    "from syncfcns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radwear_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'\n",
    "redcap_path = radwear_path+'REDCap responses/'\n",
    "\n",
    "# load all participant meta data\n",
    "with open(radwear_path+'all_p_metadata.json', 'rb') as f:\n",
    "            all_p_metadata = json.load(f)\n",
    "# load all participant redcap data\n",
    "redcap_df = process_redcap.process_redcap(redcap_path,all_p_metadata['list of participant IDs'])\n",
    "list_of_participants = all_p_metadata['list of participant IDs']\n",
    "completed_participants = []\n",
    "\n",
    "\n",
    "\n",
    "filepath = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'\n",
    "# label definitions\n",
    "calib_dict = {'meditation': 0, 'cpt': 1}\n",
    "rot_anx_dict = {'calibration': 0, 'LA': 1, 'HA': 2}\n",
    "\n",
    "p_calib = {}\n",
    "p_la = {}\n",
    "p_ha = {}\n",
    "p_all = {} \n",
    "incomplete = [guy for guy in list_of_participants if guy not in completed_participants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sync_return(filepath, e4_today, hx_today):\n",
    "    '''\n",
    "    usage: read_sync_return(filepath, e4_today, hx_today)\n",
    "    returns: synced_participant_data\n",
    "    '''\n",
    "    e4_dict = read_E4(filepath+'/'+e4_today, hx_today)\n",
    "    hx_dict = read_hx(filepath+'/record_'+hx_today, hx_today)\n",
    "\n",
    "    #e4sync_offset = E4sync_offset(e4_dict)\n",
    "    #hxsync_offset = Hexsync_offset(hx_dict['ECG'],hx_dict['BR'],hx_dict['accx'],hx_dict['accy'], hx_dict['accz'])\n",
    "    e4sync_offset = e4_dict\n",
    "    hxsync_offset = hx_dict\n",
    "    \n",
    "    synced_participant_data = {}\n",
    "    #cross syncing between devices but only on bvp and ecg\n",
    "    synced_participant_data['ECG'], synced_participant_data['BVP'] = doublesync_offset(hxsync_offset['ECG'], e4sync_offset['BVP'])\n",
    "    # for some reason we're losing seconds from beginning and end of hx data even though e4 is contained within it's timeframe.\n",
    "\n",
    "\n",
    "    # running syncing on rest of signals for hx (ECG, breathing_rateacc, br)\n",
    "    t0 = synced_participant_data['ECG']['Second'].iloc[0] #hx\n",
    "    tf = synced_participant_data['ECG']['Second'].iloc[-1] #hx\n",
    "    synced_participant_data['BR'] = hxsync_offset['BR'].loc[hxsync_offset['BR']['Second'] >= t0 ].loc[hxsync_offset['BR']['Second'] <= tf ]\n",
    "    del tf, t0\n",
    "\n",
    "    # running syncing on rest of signals for e4 (acc, eda, temp, hr, ibi )\n",
    "    t0 = synced_participant_data['BVP']['Second'].iloc[0] #e4\n",
    "    tf = synced_participant_data['BVP']['Second'].iloc[-1] #e4\n",
    "    synced_participant_data['TEMP'] = e4sync_offset['TEMP'].loc[e4sync_offset['TEMP']['Second'] >= t0 ].loc[e4sync_offset['TEMP']['Second'] <= tf ]\n",
    "    synced_participant_data['EDA'] = e4sync_offset['EDA'].loc[e4sync_offset['EDA']['Second'] >= t0 ].loc[e4sync_offset['EDA']['Second'] <= tf ]\n",
    "    synced_participant_data['HR'] = e4_dict['HR'].loc[e4_dict['HR']['Second'] >= t0 ].loc[e4_dict['HR']['Second'] <= tf ]\n",
    "    #synced_participant_data['IBI'] = e4sync_offset['IBI'].loc[e4sync_offset['IBI']['Second'] >= t0 ].loc[e4sync_offset['IBI']['Second'] <= tf ]\n",
    "    del tf, t0\n",
    "\n",
    "    hx_dict['ACC'] = hx_dict['accx']\n",
    "    hx_dict['ACC']['Acc_Y'] = hx_dict['accy']['Acc_Y']\n",
    "    hx_dict['ACC']['Acc_Z'] = hx_dict['accz']['Acc_Z']\n",
    "     \n",
    "    acc_dic = accsync_offset(e4sync_offset['ACC'],hxsync_offset['accx'])\n",
    "    #offset_ecg_cross, offset_bvp_cross = doublesync_offset(e4sync_offset['ECG'], e4sync_offset['BVP'])\n",
    "    synced_participant_data['ACC_hx'] = acc_dic['acc_hx']\n",
    "    synced_participant_data['ACC_e4'] = acc_dic['acc_e4']\n",
    "\n",
    "    return synced_participant_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# p5 HA day 8 and p9 HA day 10 will be done manually. redcap is marked as 0 but data is available\n",
    "#########################\n",
    "\n",
    "# wesad.keys() = dict_keys(['signal', 'label', 'subject']); label returns array subject: S$ \n",
    "incomplete = [guy for guy in list_of_participants if guy not in completed_participants]\n",
    "\n",
    "for p in list_of_participants:\n",
    "    # check if participant pickle file exists\n",
    "    if Path(radwear_path+'Participant '+str(p)+'/p_'+str(p)+'.pkl').is_file():\n",
    "        print(radwear_path+'Participant '+str(p)+'/p_'+str(p)+'.pkl')\n",
    "        print('participant ', p, ' already processed.')\n",
    "        completed_participants.append(p)\n",
    "incomplete = [guy for guy in list_of_participants if guy not in completed_participants]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completed_participants)\n",
    "print(incomplete)\n",
    "p_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in incomplete:\n",
    "    p_la = {}\n",
    "    p_ha = {}\n",
    "    p_all = {}\n",
    "    print('procesing participant: ', p, ' calibration data...')\n",
    "    p_all[p] = {}\n",
    "    p_la[p] = {}\n",
    "    p_ha[p] = {}\n",
    "    p_calib = {}\n",
    "\n",
    "    p_path = radwear_path+'Participant '+str(p)\n",
    "    # load participant e4 data\n",
    "    e4sn = all_p_metadata[str(p)]['e4sn']\n",
    "    calibration_files = all_p_metadata[str(p)]['calibration']\n",
    "    LA = all_p_metadata[str(p)]['LA']\n",
    "    HA = all_p_metadata[str(p)]['HA']\n",
    "    \n",
    "    # load calibration data\n",
    "    e4_num = all_p_metadata[str(p)]['e4sn']+'_'+all_p_metadata[str(p)]['calibration'][0]\n",
    "    hx_num = str(all_p_metadata[str(p)]['calibration'][1])\n",
    "    p_calib[p] = read_sync_return(p_path, e4_num, hx_num)\n",
    "    p_calib[p]['rot_label'] = rot_anx_dict['calibration'] * np.ones(len(p_calib[p]['ECG'])) # add label to designate calibration segment\n",
    "    p_calib[p] = dd.from_dict(p_calib[p]) # for dask conversion\n",
    "    \n",
    "    #####################\n",
    "    # for when adding labels for cpt and meditation\n",
    "    #calib_label = for \n",
    "    \n",
    "    if all_p_metadata[str(p)]['complete days'][0] > 0:\n",
    "        # loop for LA\n",
    "        for i in range(all_p_metadata[str(p)]['complete days'][0]): # loop for LA\n",
    "            if not (all_p_metadata[str(p)]['RedCap available'][0][i] == 0 or all_p_metadata[str(p)]['LA'][0][i] == 0 or all_p_metadata[str(p)]['LA'][1][i] == 0): \n",
    "                print('procesing participant: ', p, ' LA day ', i+1, '...')    \n",
    "                e4_num = all_p_metadata[str(p)]['e4sn']+'_'+all_p_metadata[str(p)]['LA'][0][i]\n",
    "                hx_num = str(all_p_metadata[str(p)]['LA'][1][i])\n",
    "                \n",
    "                p_la[p][i] = read_sync_return(p_path, e4_num, hx_num)\n",
    "                p_la[p][i]['rot_label']= rot_anx_dict['LA'] * np.ones(len(p_la[p][i]['ECG'])) \n",
    "            else:\n",
    "                p_la[p][i] = {}\n",
    "                p_la[p][i] = dd.from_dict(p_la[p][i]) # for dask conversion \n",
    "    if all_p_metadata[str(p)]['complete days'][1] > 0:\n",
    "        # loop for HA    \n",
    "        for j in range(all_p_metadata[str(p)]['complete days'][1]): # loop for HA\n",
    "            if not (all_p_metadata[str(p)]['RedCap available'][1][j] == 0 or all_p_metadata[str(p)]['HA'][0][j] == 0 or all_p_metadata[str(p)]['HA'][1][j] == 0): \n",
    "                print('procesing participant: ', p, ' HA day ', j+1, '...')    \n",
    "                e4_num = all_p_metadata[str(p)]['e4sn']+'_'+all_p_metadata[str(p)]['HA'][0][j]\n",
    "                hx_num = str(all_p_metadata[str(p)]['HA'][1][j])\n",
    "                \n",
    "                p_ha[p][j] = read_sync_return(p_path, e4_num, hx_num)\n",
    "                p_ha[p][j]['rot_label']= rot_anx_dict['HA'] * np.ones(len(p_ha[p][j]['ECG']))\n",
    "            else:\n",
    "                p_ha[p][j] = {}\n",
    "                p_ha[p][j] = dd.from_dict(p_ha[p][j]) # for dask conversion\n",
    "    print('participant ', p, ' done.')\n",
    "    if True:\n",
    "        try:\n",
    "            p_all[p]['calib']= p_calib[p]\n",
    "        except:\n",
    "            print('participant ', p, ' has no calibration data.')\n",
    "            p_all[p]['calib'] = {}\n",
    "        try:    \n",
    "            p_all[p]['LA'] = p_la[p]\n",
    "        except:\n",
    "            print('participant ', p, ' has no LA data.')\n",
    "            p_all[p]['LA'] = {}      \n",
    "        try: \n",
    "            p_all[p]['HA'] = p_ha[p]\n",
    "        except:\n",
    "            print('participant ', p, ' has no HA data.')\n",
    "            p_all[p]['HA'] = {}\n",
    "        #save this participant data to a pickle file\n",
    "        #with open(p_path+'/p_'+str(p)+'.pkl', 'wb') as handle:\n",
    "        #        pickle.dump(p_all[p], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        #pq.write_table(p_all[p], p_path+'/p_'+str(p)+'.pkl')\n",
    "        # or \n",
    "        df.to_parquet(p_path+'/p_'+str(p)+'_parq')\n",
    "        print('participant ', p, ' saved.')\n",
    "    completed_participants.append(p)\n",
    "    print('-----------------------------------')\n",
    "\n",
    "print('completed participants: ', completed_participants)\n",
    "print('incomplete participants: ', incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_la[p][i] = read_sync_return(p_path, e4_num, hx_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 17\n",
    "e4_num = all_p_metadata[str(x)]['e4sn']+'_'+'230817-203116'\n",
    "hx_num = 'record_271784'\n",
    "p_path = radwear_path+'Participant '+str(x)\n",
    "print(e4_num)\n",
    "print(hx_num)\n",
    "print(p_path)\n",
    "print(p_path+'/'+e4_num)\n",
    "print(p_path+'/'+hx_num)\n",
    "e4_dict = read_E4(p_path+'/'+e4_num, hx_num)\n",
    "hx_dict = read_hx(p_path+'/'+hx_num, hx_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e4_dict.keys())\n",
    "print(hx_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_dict['ECG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_dict['ECG']\n",
    "print('total time is = ', 32572/60/60,'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_path = radwear_path+'Participant '+str(9)\n",
    "p_path\n",
    "\n",
    "with open(p_path+'/p_'+str(9)+'.pkl', 'rb') as handle:\n",
    "    p9 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_la.keys())\n",
    "print(p_ha.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/WESAD/'\n",
    "with open(wesad_path+'S2/S2.pkl', 'rb') as f:\n",
    "            wesad = pickle.load(f,encoding='latin1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in wesad['signal']['wrist'].keys():\n",
    "    print(i)\n",
    "print('\\n\\n')\n",
    "for i in wesad['signal']['wrist'].keys():\n",
    "    print(len(wesad['signal']['wrist'][i]))\n",
    "    \n",
    "for i in wesad['signal']['chest'].keys():\n",
    "    print(i)\n",
    "print('\\n\\n')\n",
    "for i in wesad['signal']['chest'].keys():\n",
    "    print(len(wesad['signal']['chest'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hx_dict.keys():\n",
    "    print(i)\n",
    "print('\\n\\n')\n",
    "for i in hx_dict.keys():\n",
    "    print(len(hx_dict[i]))\n",
    "for i in e4_dict.keys():\n",
    "    print(i)\n",
    "print('\\n\\n')\n",
    "for i in e4_dict.keys():\n",
    "    print(len(e4_dict[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAGS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
