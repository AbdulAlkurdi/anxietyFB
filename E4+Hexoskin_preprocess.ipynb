{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'modified by abdul alkurdi; 10/05/2023'\n",
    "\n",
    "import pandas as pd\n",
    "#import cudf\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import correlate\n",
    "#import cupy as cp\n",
    "from scipy.io import wavfile\n",
    "#from scipy import signal, stats\n",
    "#import peakutils, wfdb, pywt\n",
    "import csv\n",
    "import os, statistics\n",
    "from datetime import datetime\n",
    "#import heartpy as hp\n",
    "import json\n",
    "%matplotlib notebook\n",
    "# import neurokit2 as nk\n",
    "\n",
    "import process_redcap \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring participant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing participant configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redcap dict exists\n",
      "redcap dict pickle loaded\n",
      "redcap df exists\n",
      "redcap df pickle loaded\n"
     ]
    }
   ],
   "source": [
    "radwear_path = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'\n",
    "redcap_path = radwear_path+'REDCap responses/'\n",
    "\n",
    "# load all participant meta data\n",
    "with open(radwear_path+'all_p_metadata.json', 'rb') as f:\n",
    "            all_p_metadata = json.load(f)\n",
    "\n",
    "# load all participant redcap data\n",
    "redcap_df = process_redcap.process_redcap(redcap_path,all_p_metadata['list of participant IDs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (618720513.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    e4_file =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# loop to get participant e4 and hexoskin data\n",
    "\n",
    "for p in all_p_metadata['list of participant IDs']:\n",
    "\n",
    "    p_path = radwear_path+'participant '+str(p)+'/'\n",
    "\n",
    "    # load participant e4 data\n",
    "    all_p_metadata[str(p)]\n",
    "    e4sn = all_p_metadata[str(p)]['e4sn']\n",
    "    e4_files = all_p_metadata[str(p)][  \n",
    "    # load participant hexoskin data\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 'e4sn', 'hxsn', 'complete days', 'RedCap available', 'calibration', 'LA', 'HA'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_p_metadata[str(9)].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import E4 CSV files (CPU version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read E4 data, clean up with corrected timstamp and store the csv files in dict\n",
    "def read_E4(participant_filepath, date):\n",
    "    '''\n",
    "    usage:\n",
    "        filepath=r'/home/maxinehe/Downloads/' + fE4\n",
    "        a=read_E4(filepath, '230429')\n",
    "        fE4 = 'A04BA8_230429-142458'\n",
    "        fe4 = '230429-142458'\n",
    "        \n",
    "    '''\n",
    "    filepath = participant_filepath\n",
    "    # HR data -- started 10 seconds later than other metrics\n",
    "    hr = pd.read_csv(filepath+str('/HR.csv'), header = None)\n",
    "    # clean up HR file\n",
    "    start_time = hr.values[0]\n",
    "    hr_samp_rate = hr.values[1]\n",
    "    hr = hr.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    hr['Timestamp'] = list(range(0, len(hr),1))\n",
    "    hr['Timestamp'] = hr['Timestamp'].apply(lambda x: x/hr_samp_rate+start_time)\n",
    "    hr['Timestamp'] = hr['Timestamp'].str.get(0)\n",
    "    hr['Second'] = hr['Timestamp']\n",
    "    hr = hr.set_index('Timestamp')\n",
    "    hr['Second'] = hr['Second'].apply(lambda x: x-hr.index[0])\n",
    "    hr.columns = ['Heart rate', 'Second']\n",
    "    hr = hr.reset_index(inplace=False)\n",
    "    \n",
    "    # EDA data\n",
    "    eda = pd.read_csv(filepath+str('/EDA.csv'), header = None)\n",
    "    start_time = eda.values[0]\n",
    "    eda_samp_rate = eda.values[1]\n",
    "    eda = eda.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    eda['Timestamp'] = list(range(0, len(eda),1))\n",
    "    eda['Timestamp'] = eda['Timestamp'].apply(lambda x: x/eda_samp_rate+start_time)\n",
    "    eda['Timestamp'] = eda['Timestamp'].str.get(0)\n",
    "    eda['Second'] = eda['Timestamp']\n",
    "    eda = eda.set_index('Timestamp')\n",
    "    eda['Second'] = eda['Second'].apply(lambda x: x-eda.index[0])\n",
    "    eda.columns = ['EDA', 'Second']\n",
    "    eda = eda.reset_index(inplace=False)\n",
    "\n",
    "    temp = pd.read_csv(filepath+str('/TEMP.csv'), header = None)\n",
    "    # clean up TEMP file\n",
    "    start_time = temp.values[0]\n",
    "    temp_samp_rate = temp.values[1]\n",
    "    temp = temp.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    temp['Timestamp'] = list(range(0, len(temp),1))\n",
    "    temp['Timestamp'] = temp['Timestamp'].apply(lambda x: x/temp_samp_rate+start_time)\n",
    "    temp['Timestamp'] = temp['Timestamp'].str.get(0)\n",
    "    temp['Second'] = temp['Timestamp']\n",
    "    temp = temp.set_index('Timestamp')\n",
    "    temp['Second'] = temp['Second'].apply(lambda x: x-temp.index[0])\n",
    "    temp.columns = ['Temp', 'Second']\n",
    "    temp = temp.reset_index(inplace=False)\n",
    "    \n",
    "    ibi = pd.read_csv(filepath+str('/IBI.csv'), header = None) # no correction of timestamp needed\n",
    "    ibi = ibi.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    ibi.columns = ['Second', 'IBI']\n",
    "    \n",
    "    \n",
    "    bvp = pd.read_csv(filepath+str('/BVP.csv'), header = None)\n",
    "    start_time = bvp.values[0]\n",
    "    bvp_samp_rate = bvp.values[1]\n",
    "    bvp = bvp.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    bvp['Timestamp'] = list(range(0, len(bvp),1))\n",
    "    bvp['Timestamp'] = bvp['Timestamp'].apply(lambda x: np.round(x/bvp_samp_rate+start_time, 2))\n",
    "    bvp['Timestamp'] = bvp['Timestamp'].str.get(0)\n",
    "    bvp['Second'] = bvp['Timestamp']\n",
    "    bvp = bvp.set_index('Timestamp')\n",
    "    bvp['Second'] = bvp['Second'].apply(lambda x: x-bvp.index[0])\n",
    "    bvp.columns = ['BVP', 'Second']\n",
    "    bvp = bvp.reset_index(inplace=False)\n",
    "    \n",
    "    acc = pd.read_csv(filepath+str('/ACC.csv'), header = None)\n",
    "    start_time = acc.values[0,0]\n",
    "    acc_samp_rate = acc.values[1,0]\n",
    "    acc = acc.drop(labels = [0, 1], axis = 0, inplace = False)\n",
    "    acc['Timestamp'] = list(range(0, len(acc),1))\n",
    "    acc['Timestamp'] = acc['Timestamp'].apply(lambda x: x/acc_samp_rate+start_time)\n",
    "    acc['Second'] = acc['Timestamp']\n",
    "    acc = acc.set_index('Timestamp')\n",
    "    acc['Second'] = acc['Second'].apply(lambda x: x-acc.index[0])\n",
    "    acc.columns = ['Acceleration_X','Acceleration_Y','Acceleration_Z','Second']\n",
    "    acc = acc.reset_index(inplace=False)\n",
    "    \n",
    "    data_dict = {'Date':date, 'HR':hr, 'EDA':eda, 'TEMP':temp, 'IBI':ibi, 'BVP':bvp,'ACC':acc}\n",
    "    return data_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Hexoskin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hx(participant_day_filepath, date):\n",
    "    '''\n",
    "    takes in the participant filepath and the date of the data. it reads ecg, br, accx, accy, accz,    \n",
    "    fhex = 'record_265679'\n",
    "    '''\n",
    "    # read raw ECG file; ECG_I.wav only\n",
    "    # change wavefile pathway\n",
    "    path = participant_day_filepath \n",
    "\n",
    "    raw_ECG = wavfile.read(path+'/ECG_I.wav')\n",
    "    #settings = {}\n",
    "    #settings['fs'] = 256 # sampling rate\n",
    "\n",
    "    # ECG\n",
    "    raw_ECG = pd.DataFrame(data = raw_ECG[1])\n",
    "    ecg = 0.0064 * raw_ECG #get correct magnitude of ECG\n",
    "    ecg.rename(columns = {0: 'ECG'}, inplace = True)\n",
    "    # Opening JSON file and return it as dictionary\n",
    "    # change file pathway\n",
    "    f = open(path+'/info.json')\n",
    "    date_info = json.load(f)\n",
    "\n",
    "    # BR \n",
    "    raw_br = wavfile.read(path+'/breathing_rate.wav')\n",
    "    raw_br = pd.DataFrame(data = raw_br[1])\n",
    "    br = 1.0000 * raw_br\n",
    "    br.rename(columns = {0: 'breathing_rate'}, inplace = True)\n",
    "\n",
    "    # acc x\n",
    "    raw_accX = wavfile.read(path+'/acceleration_X.wav')\n",
    "    raw_accX = pd.DataFrame(data = raw_accX[1])\n",
    "    accx = 1.0000 * raw_accX \n",
    "    accx.rename(columns = {0: 'Acc_X'}, inplace = True)\n",
    "    # acc y\n",
    "    raw_accY = wavfile.read(path+'/acceleration_Y.wav')\n",
    "    raw_accY = pd.DataFrame(data = raw_accY[1])\n",
    "    accy = 1.0000 * raw_accY \n",
    "    accy.rename(columns = {0: 'Acc_Y'}, inplace = True)\n",
    "    # acc z\n",
    "    raw_accZ = wavfile.read(path+'/acceleration_Z.wav')\n",
    "    raw_accZ = pd.DataFrame(data = raw_accZ[1])\n",
    "    accz = 1.0000 * raw_accZ \n",
    "    accz.rename(columns = {0: 'Acc_Z'}, inplace = True)\n",
    "\n",
    "\n",
    "    # Add timestamp to Hex (ECG & ACC) signal\n",
    "    t0_ecg = list(date_info.values())[0]/256\n",
    "    ecg['Timestamp'] = list(range(0, len(raw_ECG),1))\n",
    "    ecg['Timestamp'] = ecg['Timestamp'].apply(lambda x: x/256+t0_ecg)\n",
    "    #ecg['Timestamp'] = ecg['Timestamp'].str.get(0)\n",
    "    ecg['Second'] = ecg['Timestamp']\n",
    "    ecg = ecg.set_index('Timestamp')\n",
    "    ecg['Second'] = ecg['Second'].apply(lambda x: x-ecg.index[0])\n",
    "    ecg = ecg.reset_index()\n",
    "    #ecg.columns = ['Heart rate', 'Second']\n",
    "\n",
    "    t0_br = list(date_info.values())[0]/256\n",
    "    br['Timestamp'] = list(range(0, len(raw_br),1))\n",
    "    br['Timestamp'] = br['Timestamp'].apply(lambda x: x/1+t0_br)\n",
    "    br['Second'] = br['Timestamp']\n",
    "    br = br.set_index('Timestamp')\n",
    "    br['Second'] = br['Second'].apply(lambda x: x-br.index[0])\n",
    "    br = br.reset_index()\n",
    "\n",
    "    t0_acc = list(date_info.values())[0]/256\n",
    "    accx['Timestamp'] = list(range(0, len(raw_accX),1))\n",
    "    accx['Timestamp'] = accx['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accx['Second'] = accx['Timestamp']\n",
    "    accx = accx.set_index('Timestamp')\n",
    "    accx['Second'] = accx['Second'].apply(lambda x: x-accx.index[0])\n",
    "    accx = accx.reset_index()\n",
    "\n",
    "    accy['Timestamp'] = list(range(0, len(raw_accY),1))\n",
    "    accy['Timestamp'] = accy['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accy['Second'] = accy['Timestamp']\n",
    "    accy = accy.set_index('Timestamp')\n",
    "    accy['Second'] = accy['Second'].apply(lambda x: x-accy.index[0])\n",
    "    accy = accy.reset_index()\n",
    "\n",
    "    accz['Timestamp'] = list(range(0, len(raw_accZ),1))\n",
    "    accz['Timestamp'] = accz['Timestamp'].apply(lambda x: x/64+t0_acc)\n",
    "    accz['Second'] = accz['Timestamp']\n",
    "    accz = accz.set_index('Timestamp')\n",
    "    accz['Second'] = accz['Second'].apply(lambda x: x-accz.index[0])\n",
    "    accz = accz.reset_index()\n",
    "    \n",
    "    data_dict = {'Date':date, 'ECG':ecg, 'BR':br, 'BR':br, 'accx':accx, 'accy':accy,'accz':accz}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filepath=r'/home/maxinehe/Downloads/' + fE4\n",
    "#read_hx(filepath, data, participants)\n",
    "\n",
    "filepath = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear/'\n",
    "filepath = filepath +'Participant X'\n",
    "\n",
    "e4_today = 'A04BA8_220704-164707'\n",
    "hx_today = str(248258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4_dict = read_E4(filepath+'/'+e4_today+'/', hx_today)\n",
    "hx_dict = read_hx(filepath+'/record_'+hx_today+'/', hx_today)\n",
    "a = e4_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for participant in participants: \n",
    "    read_hx(filepath, data, participants)\n",
    "    read_E4(filepath, '230429')\n",
    "\n",
    "    # Export E4_CSV\n",
    "    if False:\n",
    "        Acceleration_X.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_acc_x.csv', sep=str(','), header=True)\n",
    "        Acceleration_Y.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_acc_y.csv', sep=str(','), header=True)\n",
    "        Acceleration_Z.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_acc_z.csv', sep=str(','), header=True)\n",
    "        offset_eda.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_EDA.csv', sep=str(','), header=True)\n",
    "        offset_temp.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_TEMP.csv', sep=str(','), header=True)\n",
    "        offset_bvp.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_BVP.csv', sep=str(','), header=True)\n",
    "\n",
    "    # export Hx_CSV\n",
    "    if False:\n",
    "        offset_br.to_csv('/home/maxinehe/Desktop/'+fhex+'_hex_br.csv', sep=str(','), header=True)\n",
    "        offset_ecg.to_csv('/home/maxinehe/Desktop/'+fhex+'_hex_ECG.csv', sep=str(','), header=True)\n",
    "        offset_accx.to_csv('/home/maxinehe/Desktop/'+fhex+'_hex_accx.csv', sep=str(','), header=True)\n",
    "        offset_accy.to_csv('/home/maxinehe/Desktop/'+fhex+'_hex_accy.csv', sep=str(','), header=True)\n",
    "        offset_accz.to_csv('/home/maxinehe/Desktop/'+fhex+'_hex_accz.csv', sep=str(','), header=True)\n",
    "\n",
    "        offset_ecg_cross.to_csv('/home/maxinehe/Desktop/'+fhex+'_ecg_cross.csv', sep=str(','), header=True)\n",
    "        offset_bvp_cross.to_csv('/home/maxinehe/Desktop/'+fe4+'_E4_bvp_cross.csv', sep=str(','), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization for E4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E4sync_offset(a):\n",
    "    eda = a['EDA']\n",
    "    temp = a['TEMP']\n",
    "    bvp = a['BVP']\n",
    "    acc = a['ACC']\n",
    "\n",
    "    \n",
    "    t_eda = eda['Timestamp'].iat[-1]\n",
    "    t_temp = temp['Timestamp'].iat[-1]\n",
    "    t_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t_acc = acc['Timestamp'].iat[-1]\n",
    "    t0 = 0\n",
    "    \n",
    "    if t_eda < t_temp and t_eda < t_bvp and t_eda<t_acc :\n",
    "        \n",
    "        t1_loc = temp.loc[temp['Timestamp'] == round(t_eda, 2)]\n",
    "        t2_loc = bvp.loc[bvp['Timestamp'] == round(t_eda, 2)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] == round(t_eda, 2)]\n",
    "        offset_eda = eda\n",
    "        offset_temp = temp.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "        \n",
    "    elif t_temp < t_eda and t_temp < t_bvp and t_temp<t_acc :\n",
    "        \n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_temp, 0)]\n",
    "        t2_loc = bvp.loc[bvp['Timestamp'] == round(t_temp, 2)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] == round(t_temp, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "        \n",
    "    elif t_bvp < t_eda and t_bvp < t_temp and t_bvp < t_acc :\n",
    "        \n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_bvp, 0)]\n",
    "        t2_loc = temp.loc[temp['Timestamp'] == round(t_bvp, 0)]\n",
    "        t3_loc = acc.loc[acc['Timestamp'] ==round(t_bvp, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_bvp = bvp\n",
    "        offset_acc = acc.truncate(before = t0, after = t3_loc.index[0])\n",
    "    \n",
    "    elif t_acc < t_eda and t_acc < t_temp and t_acc < t_bvp :\n",
    "        \n",
    "        t1_loc = eda.loc[eda['Timestamp'] == round(t_acc, 0)]\n",
    "        t2_loc = temp.loc[temp['Timestamp'] == round(t_acc, 0)]\n",
    "        t3_loc = bvp.loc[bvp['Timestamp'] == round(t_acc, 2)]\n",
    "        offset_eda = eda.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_temp = temp.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_bvp = bvp.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_acc = acc\n",
    "    \n",
    "    \n",
    "    # E4_acc separation\n",
    "    l1 = offset_acc.iloc[:,0:1]\n",
    "    l2 = offset_acc.iloc[:,4]\n",
    "\n",
    "    Acceleration_x = offset_acc.iloc[:,1]\n",
    "    Acceleration_X1 = pd.concat([l1,Acceleration_x], axis=1, join='outer')\n",
    "    Acceleration_X = pd.concat([Acceleration_X1,l2], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y = offset_acc.iloc[:,2]\n",
    "    Acceleration_Y1 = pd.concat([l1,Acceleration_y], axis=1, join='outer')\n",
    "    Acceleration_Y = pd.concat([Acceleration_Y1,l2], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z = offset_acc.iloc[:,3]\n",
    "    Acceleration_Z1 = pd.concat([l1,Acceleration_z], axis=1, join='outer')\n",
    "    Acceleration_Z = pd.concat([Acceleration_Z1,l2], axis=1, join='outer')\n",
    "\n",
    "\n",
    "\n",
    "    eda2 = offset_eda.iloc[:,1]\n",
    "    eda2 = np.expand_dims(eda2.values, axis = 1)\n",
    "    temp2 = offset_temp.iloc[:,1]\n",
    "    temp2 = np.expand_dims(temp2.values, axis = 1)\n",
    "    bvp2 = offset_bvp.iloc[:,1]\n",
    "    bvp2 = np.expand_dims(bvp2.values, axis = 1)\n",
    "    Accx = Acceleration_X.iloc[:,1]\n",
    "    Accx = np.expand_dims(Accx.values, axis = 1)\n",
    "    Accy = Acceleration_Y.iloc[:,1]\n",
    "    Accy = np.expand_dims(Accy.values, axis = 1)\n",
    "    Accz = Acceleration_Z.iloc[:,1]\n",
    "    Accz = np.expand_dims(Accz.values, axis = 1)\n",
    "\n",
    "    acc2 = offset_acc.iloc[:,1:4]\n",
    "    acc2 = acc2.values\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    E4_to_dic[\"EDA\"] = eda2\n",
    "    E4_to_dic[\"TEMP\"] = temp2\n",
    "    E4_to_dic[\"BVP\"] = bvp2\n",
    "    E4_to_dic[\"Acceleration_X\"] = Accx\n",
    "    E4_to_dic[\"Acceleration_Y\"] = Accy\n",
    "    E4_to_dic[\"Acceleration_Z\"] = Accz \n",
    "    E4_to_dic[\"Acceleration\"] = offset_acc\n",
    "    \n",
    "    if False:\n",
    "        with open(filepath+'_E4.pkl', 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return E4_to_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offset_eda, offset_temp, offset_bvp, offset_acc = E4sync_offset(a)\n",
    "e4sync_offset = E4sync_offset(a)\n",
    "\n",
    "#if E4.pkl exist E4_to_dic = E4sync_offset(a['EDA'],a['TEMP'],a['BVP'],a['ACC']) # not done yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling for E4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(filepath+'_E4.pkl', 'wb') as handle:\n",
    "#    pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('A04BA8_220801-180621_E4.pkl', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization for Hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hexsync_offset(ecg,br,accx,accy, accz):\n",
    "    t_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t_br = br['Timestamp'].iat[-1]\n",
    "    t_accx = accx['Timestamp'].iat[-1]\n",
    "    t_accy = accy['Timestamp'].iat[-1]\n",
    "    t0 = 0\n",
    "    \n",
    "\n",
    "    if t_ecg <= t_br and t_ecg <= t_accx and t_ecg<=t_accy :\n",
    "        t1_loc = br.loc[round(br['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        t2_loc = accx.loc[round(accx['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        t3_loc = accy.loc[round(accy['Timestamp'],0) == round(t_ecg, 0)].head(1)\n",
    "        offset_ecg = ecg\n",
    "        offset_br = br.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_accx = accx.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accy = accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz = accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_br <= t_ecg and t_br <= t_accx and t_br<=t_accy :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_br, 7)]\n",
    "        t2_loc = accx.loc[accx['Timestamp'] == round(t_br, 7)]\n",
    "        t3_loc = accy.loc[accy['Timestamp'] == round(t_br, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_br = br\n",
    "        offset_accx = accx.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accy= accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz= accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_accx <= t_ecg and t_accx <= t_br and t_accx <= t_accy :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_accx, 7)]\n",
    "        t2_loc = br.loc[br['Timestamp'] == round(t_accx, 7)]\n",
    "        t3_loc = accy.loc[accy['Timestamp'] ==round(t_accx, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_br = br.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accx = accx\n",
    "        offset_accy = accy.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accz = accz.truncate(before = t0, after = t3_loc.index[0])\n",
    "\n",
    "    elif t_accy <= t_ecg and t_accy <= t_br and t_accy <= t_accx :\n",
    "        t1_loc = ecg.loc[ecg['Timestamp'] == round(t_accy, 7)]\n",
    "        t2_loc = br.loc[br['Timestamp'] == round(t_accy, 7)]\n",
    "        t3_loc = accx.loc[accx['Timestamp'] == round(t_accy, 7)]\n",
    "        offset_ecg = ecg.truncate(before = t0, after = t1_loc.index[0])\n",
    "        offset_brp = br.truncate(before = t0, after = t2_loc.index[0])\n",
    "        offset_accx = accx.truncate(before = t0, after = t3_loc.index[0])\n",
    "        offset_accy = accy\n",
    "        offset_accz = accz\n",
    "\n",
    "    ecg2 = offset_accx.iloc[:,1]\n",
    "    ecg2 = np.expand_dims(ecg2.values, axis = 1)\n",
    "    accx2 = offset_accx.iloc[:,1]\n",
    "    accx2 = np.expand_dims(accx2.values, axis = 1)\n",
    "    accy2 = offset_accy.iloc[:,1]\n",
    "    accy2 = np.expand_dims(accy2.values, axis = 1)\n",
    "    accz2 = offset_accz.iloc[:,1]\n",
    "    accz2 = np.expand_dims(accz2.values, axis = 1)\n",
    "    br2 = offset_br.iloc[:,1]\n",
    "    br2 = np.expand_dims(br2.values, axis = 1)\n",
    "\n",
    "    hx_to_dic = {}\n",
    "    hx_to_dic[\"ECG\"] = ecg2\n",
    "    hx_to_dic[\"breathing_rate\"] = br2\n",
    "    hx_to_dic[\"ACCX\"] = accx2\n",
    "    hx_to_dic[\"ACCY\"] = accy2\n",
    "    hx_to_dic[\"ACCZ\"] = accz2\n",
    "    \n",
    "    accx3 = accx.iloc[:,0:2]\n",
    "    accy3 = accy.iloc[:,1]\n",
    "    accz3 = accz.iloc[:,1:3]\n",
    "\n",
    "    hx_to_dic['ACC_n'] = pd.concat([accx3,\n",
    "                                    accy3,\n",
    "                                    accz3],\n",
    "                                    axis=1, join='outer')\n",
    "    #hx_to_dic['ACC_n'] = np.concatenate([accx2, accy2, accz2], axis=1, join='outer')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        string2 = fhex+'_hex.pkl'\n",
    "        with open(string2, 'wb') as handle:\n",
    "            pickle.dump(hx_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    return hx_to_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offset_ecg, offset_br, offset_accx, offset_accz = Hexsync_offset(ecg,br,accx,accz)\n",
    "#offset_ecg, offset_br, offset_accy, offset_accz = Hexsync_offset(ecg,br,accy,accz)\n",
    "hxsync_offset = Hexsync_offset(hx_dict['ECG'],hx_dict['BR'],hx_dict['accx'],hx_dict['accy'], hx_dict['accz'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization for ECG & BVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublesync_offset(ecg, bvp):\n",
    "    \n",
    "    t1_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t1_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t0_bvp = bvp['Timestamp'].iat[0]\n",
    "    t0_ecg = ecg['Timestamp'].iat[0]\n",
    "\n",
    "    if t0_ecg < t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(after = t1_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0])\n",
    "        \n",
    "        \n",
    "    elif t0_ecg > t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(after = t1_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg < t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        offset_bvp = bvp\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "    \n",
    "    elif t0_ecg > t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "        offset_ecg = ecg\n",
    "    \n",
    "#     offset = {'offset_ecg':offset_ecg, 'offset_bvp':offset_bvp}\n",
    "#     return offset\n",
    "    \n",
    "    offset_ecg_cross = offset_ecg\n",
    "    offset_bvp_cross= offset_bvp\n",
    "    \n",
    "    ecg2 = offset_ecg.iloc[:,1]\n",
    "    ecg2 = np.expand_dims(ecg2.values, axis = 1)\n",
    "    bvp2 = offset_bvp.iloc[:,1]\n",
    "    bvp2 = np.expand_dims(bvp2.values, axis = 1)\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    E4_to_dic[\"ECG\"] = ecg2\n",
    "    E4_to_dic[\"BVP\"] = bvp2\n",
    "\n",
    "    if False:\n",
    "        string3 = fhex+'_ECG&BVP.pkl'\n",
    "        with open(string3, 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return offset_ecg,offset_bvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offset_ecg_cross, offset_bvp_cross = doublesync_offset(ecg, a['BVP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling for ECG & BVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization for ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accsync_offset(ecg, bvp):\n",
    "    \n",
    "    t1_ecg = ecg['Timestamp'].iat[-1]\n",
    "    t1_bvp = bvp['Timestamp'].iat[-1]\n",
    "    t0_bvp = bvp['Timestamp'].iat[0]\n",
    "    t0_ecg = ecg['Timestamp'].iat[0]\n",
    "\n",
    "    if t0_ecg < t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "\n",
    "        offset_bvp = bvp.truncate(after = t1_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg > t0_bvp and t1_ecg > t1_bvp:\n",
    "\n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg,1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp,1)].head(1)\n",
    "\n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0])\n",
    "        offset_ecg = ecg.truncate(after = t1_loc.index[0])\n",
    "        \n",
    "    elif t0_ecg < t0_bvp and t1_ecg > t1_bvp:\n",
    "        \n",
    "        t0_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t0_bvp, 1)].head(1)\n",
    "        t1_loc = ecg.loc[round(ecg['Timestamp'],1) == round(t1_bvp, 1)].head(1)\n",
    "        \n",
    "        offset_bvp = bvp\n",
    "        offset_ecg = ecg.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "    \n",
    "    elif t0_ecg > t0_bvp and t1_ecg < t1_bvp:\n",
    "        \n",
    "        t0_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t0_ecg, 1)].head(1)\n",
    "        t1_loc = bvp.loc[round(bvp['Timestamp'],1) == round(t1_ecg, 1)].head(1)\n",
    "        \n",
    "        offset_bvp = bvp.truncate(before = t0_loc.index[0], after = t1_loc.index[0])\n",
    "        offset_ecg = ecg\n",
    "    \n",
    "#     offset = {'offset_ecg':offset_ecg, 'offset_bvp':offset_bvp}\n",
    "#     return offset\n",
    "\n",
    "\n",
    "    offset_acc2 = offset_ecg\n",
    "    offset_ACC2 = offset_bvp\n",
    "    offset_ACC2.iloc[:,2]\n",
    "\n",
    "    l3 = offset_acc2.iloc[:,0:1]\n",
    "    l4 = offset_acc2.iloc[:,4]\n",
    "\n",
    "    Acceleration_x2 = offset_acc2.iloc[:,1]\n",
    "    Acceleration_X12 = pd.concat([l3,Acceleration_x2], axis=1, join='outer')\n",
    "    Acceleration_X2 = pd.concat([Acceleration_X12,l4], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y2 = offset_acc2.iloc[:,2]\n",
    "    Acceleration_Y12 = pd.concat([l3,Acceleration_y2], axis=1, join='outer')\n",
    "    Acceleration_Y2 = pd.concat([Acceleration_Y12,l4], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z2 = offset_acc2.iloc[:,3]\n",
    "    Acceleration_Z12 = pd.concat([l3,Acceleration_z2], axis=1, join='outer')\n",
    "    Acceleration_Z2 = pd.concat([Acceleration_Z12,l4], axis=1, join='outer')\n",
    "\n",
    "    l5 = offset_ACC2.iloc[:,0:1]\n",
    "    l6 = offset_ACC2.iloc[:,4]\n",
    "\n",
    "    Acceleration_x3 = offset_ACC2.iloc[:,1]\n",
    "    Acceleration_X13 = pd.concat([l5,Acceleration_x3], axis=1, join='outer')\n",
    "    Acceleration_X3 = pd.concat([Acceleration_X13,l6], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_y3 = offset_ACC2.iloc[:,2]\n",
    "    Acceleration_Y13 = pd.concat([l5,Acceleration_y3], axis=1, join='outer')\n",
    "    Acceleration_Y3 = pd.concat([Acceleration_Y13,l6], axis=1, join='outer')\n",
    "\n",
    "    Acceleration_z3 = offset_ACC2.iloc[:,3]\n",
    "    Acceleration_Z13 = pd.concat([l5,Acceleration_z3], axis=1, join='outer')\n",
    "    Acceleration_Z3 = pd.concat([Acceleration_Z13,l6], axis=1, join='outer')\n",
    "\n",
    "\n",
    "\n",
    "    acc22 = offset_acc2.iloc[:,1:4]\n",
    "    acc22 = acc22.values\n",
    "    ACC22 = offset_ACC2.iloc[:,1:4]\n",
    "    ACC22 = ACC22.values\n",
    "\n",
    "    ax = Acceleration_X2.iloc[:,1]\n",
    "    ax = np.expand_dims(ax.values, axis = 1)\n",
    "    ay = Acceleration_Y2.iloc[:,1]\n",
    "    ay = np.expand_dims(ay.values, axis = 1)\n",
    "    az = Acceleration_Z2.iloc[:,1]\n",
    "    az = np.expand_dims(az.values, axis = 1)\n",
    "\n",
    "    Ax = Acceleration_X3.iloc[:,1]\n",
    "    Ax = np.expand_dims(Ax.values, axis = 1)\n",
    "    Ay = Acceleration_Y3.iloc[:,1]\n",
    "    Ay = np.expand_dims(Ay.values, axis = 1)\n",
    "    Az = Acceleration_Z3.iloc[:,1]\n",
    "    Az = np.expand_dims(Az.values, axis = 1)\n",
    "\n",
    "    E4_to_dic = {}\n",
    "    E4_to_dic[\"accx_e4\"] = ax\n",
    "    E4_to_dic[\"accy_e4\"] = ay\n",
    "    E4_to_dic[\"accz_e4\"] = az\n",
    "\n",
    "    E4_to_dic[\"accx_hex\"] = Ax\n",
    "    E4_to_dic[\"accy_hex\"] = Ay\n",
    "    E4_to_dic[\"accz_hex\"] = Az\n",
    "\n",
    "    E4_to_dic[\"acc_e4\"] = acc22\n",
    "    E4_to_dic[\"acc_hex\"] = ACC22\n",
    "\n",
    "    E4_to_dic\n",
    "    \n",
    "    acc_dic = E4_to_dic\n",
    "\n",
    "    if False:\n",
    "        string4 = fhex+'_accE4&accHex.pkl'\n",
    "        with open(string4, 'wb') as handle:\n",
    "            pickle.dump(E4_to_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return acc_dic\n",
    "acc_dic = accsync_offset(e4sync_offset['Acceleration'],hxsync_offset['ACC_n'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
