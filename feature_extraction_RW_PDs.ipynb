{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d91f138",
   "metadata": {},
   "source": [
    "most recent as of 10/23/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea50ce1",
   "metadata": {},
   "source": [
    "# init packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20604081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Link: https://github.com/WJMatthew/WESAD/blob/master/data_wrangling.py\n",
    "import os\n",
    "import re\n",
    "import graphviz\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "import dask as dd\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "#import seaborn as sns\n",
    "import scipy.signal as scisig\n",
    "#from xgboost import XGBClassifier\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import heartpy as hp\n",
    "import biosppy\n",
    "import neurokit2 as nk\n",
    "from heartpy.datautils import *\n",
    "from heartpy.peakdetection import *\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d5538f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8af7f1",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdc347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed758cf9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24314e2f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cd49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 64, 'BVP': 64, 'EDA': 64, 'TEMP': 64, 'label': 64, 'Resp': 64, 'ECG': 64, \n",
    "           'chest': 64}\n",
    "# Window size\n",
    "WINDOW_IN_SECONDS = 60\n",
    "#rot_anx_dict = {'calibration': 0, 'LA': 1, 'HA': 2}\n",
    "#calib_label = {'not calibration':0, 'calibration':1, 'cpt':2,'meditation':3}\n",
    "# Labels\n",
    "#label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0} # WESAD labels\n",
    "label_dict = {'calibration': 0, 'low anxiety': 1, 'high anxiety': 2} # RADWear labels \n",
    "# Int to label mappings\n",
    "#int_to_label = {1: 'low anxiety', 2: 'high anxiety', 0: 'calibration'} WESAD labels\n",
    "# Feature names\n",
    "feat_names = None\n",
    "# Where to save the data\n",
    "savePath = '/projects/bbnp/Vansh/data/RADWear/output'\n",
    "\n",
    "# Where to get the data\n",
    "subject_feature_path = '/subject_feats'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8087bc7",
   "metadata": {},
   "source": [
    "## Class to Store Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bec7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the data for each subject\n",
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'ACC_C': self.data['signal']['chest']['ACC'],\n",
    "                     'ECG_C': self.data['signal']['chest']['ECG'],\n",
    "                     'EDA_C': self.data['signal']['chest']['EDA'],\n",
    "                     'EMG_C': self.data['signal']['chest']['EMG'],\n",
    "                     'Resp_C': self.data['signal']['chest']['Resp'],\n",
    "                     'Temp_C': self.data['signal']['chest']['Temp']\n",
    "                     })\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "    \n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.wrist_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7962d",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a6269",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f0149",
   "metadata": {},
   "source": [
    "### Lowpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f48211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/load_files.py\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183600cb",
   "metadata": {},
   "source": [
    "### Lowpass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = scisig.lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2654d",
   "metadata": {},
   "source": [
    "### Slope Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04115363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(series):\n",
    "    linreg = scipy.stats.linregress(np.arange(len(series)), series )\n",
    "    slope = linreg[0]\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ac3d5",
   "metadata": {},
   "source": [
    "### Mean/Std/Min/Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a883ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2db9b9",
   "metadata": {},
   "source": [
    "### Absolute Integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd96c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_integral(x):\n",
    "    return np.sum(np.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4119bd",
   "metadata": {},
   "source": [
    "### Dynamic Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ff05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_range(x):\n",
    "    return np.max(x) / np.min(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b8431",
   "metadata": {},
   "source": [
    "### Peak Frequency (Periodogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3ab744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=64)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358976c7",
   "metadata": {},
   "source": [
    "### Respiration Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75316f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resp_features(resp_data):\n",
    "    resp_rate, filtered, zeros, resp_rate_ts, resp_rate = biosppy.signals.resp.resp(resp_data, sampling_rate=64, show=False)\n",
    "    extremas, values = biosppy.signals.tools.find_extrema(signal=filtered, mode='both')\n",
    "    inhal_durations = []\n",
    "    exhal_durations = []\n",
    "    last_index = 0\n",
    "    for i in range(len(extremas)):\n",
    "        if values[i] * values[last_index] < 0:\n",
    "            if values[last_index] < 0:\n",
    "                inhal_durations.append((extremas[i] - extremas[last_index]) / 64)\n",
    "            else:\n",
    "                exhal_durations.append((extremas[i] - extremas[last_index]) / 64)\n",
    "            last_index = i\n",
    "    return np.mean(resp_rate), np.mean(inhal_durations), np.std(inhal_durations), np.mean(exhal_durations), np.std(exhal_durations), np.sum(inhal_durations) / np.sum(exhal_durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c7863",
   "metadata": {},
   "source": [
    "### FIR Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd1882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43894c7",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a973f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating window statistics for ECGs\n",
    "def get_window_stats_ecg(data, label=-1):\n",
    "    wd, m = hp.process(data['ECG'].dropna().reset_index(drop=True), 64)\n",
    "    return {'bpm': m['bpm'], 'ibi': m['ibi'], 'sdnn': m['sdnn'], 'sdsd': m['sdsd'], \n",
    "            'rmssd': m['rmssd'], 'pnn20': m['pnn20'], 'pnn50': m['pnn50']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b56005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_chest(ch_data_dict, labels, norm_type=None):\n",
    "    ecg_df = pd_old.DataFrame(ch_data_dict['ECG'], columns=['ECG'])\n",
    "    \n",
    "    # Adding index for combination due to different sampling frequencies\n",
    "    ecg_df.index = [(1 / fs_dict['ECG']) * i for i in range(len(ecg_df))]\n",
    "    \n",
    "    # Change index to datetime\n",
    "    ecg_df.index = pd_old.to_datetime(ecg_df.index, unit='s')\n",
    "    \n",
    "    return ecg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fbcde",
   "metadata": {},
   "source": [
    "## Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7064a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes features for wrist\n",
    "def compute_features(e4_data_dict, ch_data_dict, labels, norm_type=None):\n",
    "    # Dataframes for each sensor type\n",
    "    eda_df = pd_old.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "    bvp_df = pd_old.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "    acc_df = pd_old.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    temp_df = pd_old.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "    label_df = pd_old.DataFrame(labels, columns=['label'])\n",
    "    resp_df = pd_old.DataFrame(e4_data_dict['Resp_C'], columns=['Resp_C'])\n",
    "    acc_c_df = pd_old.DataFrame(e4_data_dict['ACC_C'], columns=['ACC_x_C', 'ACC_y_C', 'ACC_z_C'])\n",
    "    ecg_c_df = pd_old.DataFrame(e4_data_dict['ECG_C'], columns=['ECG_C'])\n",
    "    eda_c_df = pd_old.DataFrame(e4_data_dict['EDA_C'], columns=['EDA_C'])\n",
    "    emg_c_df = pd_old.DataFrame(e4_data_dict['EMG_C'], columns=['EMG_C'])\n",
    "    resp_c_df = pd_old.DataFrame(e4_data_dict['Resp_C'], columns=['Resp_C'])\n",
    "    temp_c_df = pd_old.DataFrame(e4_data_dict['Temp_C'], columns=['Temp_C'])\n",
    "\n",
    "    # Filter EDA\n",
    "    eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "    eda_c_df['EDA_C'] = butter_lowpass_filter(eda_c_df['EDA_C'], 1.0, fs_dict['chest'], 6)\n",
    "\n",
    "    \n",
    "    eda_data = nk.eda_phasic(nk.standardize(eda_df['EDA']), sampling_rate=fs_dict['EDA'])\n",
    "    eda_df['EDA_SCR'] = eda_data['EDA_Phasic']\n",
    "    eda_df['EDA_SCL'] = eda_data['EDA_Tonic']\n",
    "    eda_data_c = nk.eda_phasic(nk.standardize(eda_c_df['EDA_C']), sampling_rate=fs_dict['chest'])\n",
    "    eda_c_df['EDA_SCR_C'] = eda_data_c['EDA_Phasic']\n",
    "    eda_c_df['EDA_SCL_C'] = eda_data_c['EDA_Tonic']\n",
    "    \n",
    "    # Filter ACM\n",
    "    for _ in acc_df.columns:\n",
    "        acc_df[_] = filterSignalFIR(acc_df.values)\n",
    "    for _ in acc_c_df.columns:\n",
    "        acc_c_df[_] = filterSignalFIR(acc_c_df.values)\n",
    "\n",
    "    # Adding indices for combination due to differing sampling frequencies\n",
    "    eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "    bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "    acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "    temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "    label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "    resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]\n",
    "    acc_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(acc_c_df))]\n",
    "    ecg_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(ecg_c_df))]\n",
    "    eda_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(eda_c_df))]\n",
    "    emg_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(emg_c_df))]\n",
    "    resp_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(resp_c_df))]\n",
    "    temp_c_df.index = [(1 / fs_dict['chest']) * i for i in range(len(temp_c_df))]\n",
    "\n",
    "    # Change indices to datetime\n",
    "    eda_df.index = pd_old.to_datetime(eda_df.index, unit='s')\n",
    "    bvp_df.index = pd_old.to_datetime(bvp_df.index, unit='s')\n",
    "    temp_df.index = pd_old.to_datetime(temp_df.index, unit='s')\n",
    "    acc_df.index = pd_old.to_datetime(acc_df.index, unit='s')\n",
    "    label_df.index = pd_old.to_datetime(label_df.index, unit='s')\n",
    "    resp_df.index = pd_old.to_datetime(resp_df.index, unit='s')\n",
    "    acc_c_df.index = pd_old.to_datetime(acc_c_df.index, unit='s')\n",
    "    ecg_c_df.index = pd_old.to_datetime(ecg_c_df.index, unit='s')\n",
    "    eda_c_df.index = pd_old.to_datetime(eda_c_df.index, unit='s')\n",
    "    emg_c_df.index = pd_old.to_datetime(emg_c_df.index, unit='s')\n",
    "    resp_c_df.index = pd_old.to_datetime(resp_c_df.index, unit='s')\n",
    "    temp_c_df.index = pd_old.to_datetime(temp_c_df.index, unit='s')\n",
    "\n",
    "    # New EDA features\n",
    "#     r, p, t, l, d, e, obj = eda_stats(eda_df['EDA'])\n",
    "#     eda_df['EDA_phasic'] = r\n",
    "#     eda_df['EDA_smna'] = p\n",
    "#     eda_df['EDA_tonic'] = t\n",
    "\n",
    "    # Getting ECG features\n",
    "    ecg_df = compute_features_chest(ch_data_dict, labels, norm_type=None)\n",
    "        \n",
    "    # Combined dataframe - not used yet\n",
    "    df = eda_df.join(bvp_df, how='outer')\n",
    "    df = df.join(temp_df, how='outer')\n",
    "    df = df.join(acc_df, how='outer')\n",
    "    df = df.join(label_df, how='outer')\n",
    "    df = df.join(ecg_df, how='outer')\n",
    "    df = df.join(eda_c_df, how='outer')\n",
    "    df = df.join(acc_c_df, how='outer')\n",
    "    df = df.join(emg_c_df, how='outer')\n",
    "    df = df.join(resp_c_df, how='outer')\n",
    "    df = df.join(temp_c_df, how='outer')\n",
    "    df['label'] = df['label'].fillna(method='bfill')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.isna().sum())\n",
    "    if norm_type == 'std':\n",
    "        # std norm\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type == 'minmax':\n",
    "        # minmax norm\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # Groupby\n",
    "    grouped = df.groupby('label')\n",
    "    baseline = grouped.get_group(1)\n",
    "    stress = grouped.get_group(2)\n",
    "    amusement = grouped.get_group(3)\n",
    "    return grouped, baseline, stress, amusement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0585d3",
   "metadata": {},
   "source": [
    "## Get Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "628ed513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data, n_windows, label):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    # Using label freq (64 Hz) as our reference frequency due to it being the largest\n",
    "    # and thus encompassing the lesser ones in its resolution.\n",
    "    window_len = fs_dict['label'] * WINDOW_IN_SECONDS\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Get window of data\n",
    "        w = data[window_len * i: window_len * (i + 1)]\n",
    "\n",
    "        # Add/Calc rms acc\n",
    "        w = pd_old.concat([get_net_accel(w), w])\n",
    "        cols = list(w.columns)\n",
    "        cols[0] = 'net_acc'\n",
    "        w.columns = cols\n",
    "        \n",
    "        w = pd_old.concat([get_net_accel_C(w), w])\n",
    "        cols = list(w.columns)\n",
    "        cols[0] = 'net_acc_C'\n",
    "        w.columns = cols\n",
    "        \n",
    "        # Calculate stats for window\n",
    "        wstats = get_window_stats(data=w, label=label)\n",
    "        \n",
    "        # Calculate stats for window (ECG)\n",
    "        wstats_ecg = get_window_stats_ecg(data=w, label=label)\n",
    "        \n",
    "        # Seperating sample and label\n",
    "        x = pd_old.DataFrame(wstats).drop('label', axis=0)\n",
    "        y = x['label'][0]\n",
    "        x.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        feat_names = None\n",
    "        if feat_names is None:\n",
    "            feat_names = []\n",
    "            for row in x.index:\n",
    "                for col in x.columns:\n",
    "                    feat_names.append('_'.join([str(row), str(col)]))\n",
    "\n",
    "        # sample df\n",
    "        wdf = pd_old.DataFrame(x.values.flatten()).T\n",
    "        wdf.columns = feat_names\n",
    "        wdf = pd_old.concat([wdf, pd_old.DataFrame({'label': y}, index=[0])], axis=1)\n",
    "        \n",
    "        # More feats\n",
    "        wdf['BVP_peak_freq'] = get_peak_freq(w['BVP'].dropna())\n",
    "        \n",
    "        # Add more features here\n",
    "        # ACC (w and c)\n",
    "        wdf['net_acc_abs_integral'] = get_absolute_integral(w['net_acc'].dropna())\n",
    "        wdf['ACC_x_abs_integral'] = get_absolute_integral(w['ACC_x'].dropna())\n",
    "        wdf['ACC_y_abs_integral'] = get_absolute_integral(w['ACC_y'].dropna())\n",
    "        wdf['ACC_z_abs_integral'] = get_absolute_integral(w['ACC_z'].dropna())\n",
    "        wdf['net_acc_C_abs_integral'] = get_absolute_integral(w['net_acc_C'].dropna())\n",
    "        wdf['ACC_x_C_abs_integral'] = get_absolute_integral(w['ACC_x_C'].dropna())\n",
    "        wdf['ACC_y_C_abs_integral'] = get_absolute_integral(w['ACC_y_C'].dropna())\n",
    "        wdf['ACC_z_C_abs_integral'] = get_absolute_integral(w['ACC_z_C'].dropna())\n",
    "        wdf['ACC_x_peak_freq'] = get_peak_freq(w['ACC_x'].dropna())\n",
    "        wdf['ACC_y_peak_freq'] = get_peak_freq(w['ACC_y'].dropna())\n",
    "        wdf['ACC_z_peak_freq'] = get_peak_freq(w['ACC_z'].dropna())\n",
    "        wdf['ACC_x_C_peak_freq'] = None if len(w['ACC_x_C'].dropna()) == 0 else get_peak_freq(w['ACC_x_C'].dropna())\n",
    "        wdf['ACC_y_C_peak_freq'] = None if len(w['ACC_y_C'].dropna()) == 0 else get_peak_freq(w['ACC_y_C'].dropna())\n",
    "        wdf['ACC_z_C_peak_freq'] = None if len(w['ACC_z_C'].dropna()) == 0 else get_peak_freq(w['ACC_z_C'].dropna())\n",
    "        \n",
    "        # ECG\n",
    "        for key in wstats_ecg.keys():\n",
    "            wdf['ECG_'+key] = wstats_ecg[key]\n",
    "        \n",
    "        # EDA(w and c)\n",
    "        wdf['EDA_slope'] = get_slope(w['EDA'].dropna())\n",
    "        wdf['EDA_C_slope'] = None if len(w['ACC_z_C'].dropna()) == 0 else get_slope(w['EDA_C'].dropna())\n",
    "        wdf['EDA_drange'] = get_dynamic_range(w['EDA'].dropna())\n",
    "        wdf['EDA_C_drange'] = None if len(w['EDA_C'].dropna()) == 0 else get_dynamic_range(w['EDA_C'].dropna())\n",
    "\n",
    "        # EMG(c)\n",
    "        wdf['EMG_drange'] = get_dynamic_range(w['EMG_C'].dropna())\n",
    "        wdf['EMG_abs_integral'] = get_absolute_integral(w['EMG_C'].dropna())\n",
    "        # RESP(c)\n",
    "        \n",
    "        if len(w['Resp_C'].dropna()) > 0:\n",
    "            wdf['Resp_C_rate'], wdf['Resp_C_Inhal_mean'], wdf['Resp_C_Inhal_std'], wdf['Resp_C_Exhal_mean'], wdf['Resp_C_Exhal_std'], wdf['Resp_C_I/E'] = get_resp_features(w['Resp_C'].dropna())\n",
    "\n",
    "        # TEMP(w and c)\n",
    "        wdf['TEMP_drange'] = get_dynamic_range(w['TEMP'].dropna())\n",
    "        wdf['TEMP_C_drange'] = None if len(w['Temp_C'].dropna()) == 0 else get_dynamic_range(w['Temp_C'].dropna())\n",
    "        wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
    "        wdf['TEMP_C_slope'] = None if len(w['Temp_C'].dropna()) == 0 else get_slope(w['Temp_C'].dropna())\n",
    "        \n",
    "        samples.append(wdf)\n",
    "\n",
    "    return pd_old.concat(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444dd7bb",
   "metadata": {},
   "source": [
    "## Make Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6821e412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path='data/WESAD', subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    e4_data_dict = subject.get_wrist_data()\n",
    "\n",
    "    # Chest data\n",
    "    ch_data_dict = subject.get_chest_data()\n",
    "    \n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped, baseline, stress, amusement = compute_features(e4_data_dict, ch_data_dict, subject.labels, norm_type)\n",
    "\n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "    #\n",
    "    baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
    "#     for col in baseline_samples.columns:\n",
    "#         print(col)\n",
    "     # Downsampling\n",
    "    # baseline_samples = baseline_samples[::2]\n",
    "    stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
    "    amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
    "\n",
    "    all_samples = pd_old.concat([baseline_samples, stress_samples, amusement_samples])\n",
    "    all_samples = pd_old.concat([all_samples.drop('label', axis=1), pd_old.get_dummies(all_samples['label'])], axis=1)\n",
    "    # Save file as csv\n",
    "    all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats.csv')\n",
    "\n",
    "    subject = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a131f",
   "metadata": {},
   "source": [
    "## Combine Patient Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53da46c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd_old.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "    df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{savePath}/may14_feats4.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17] # WESAD\n",
    "     # part of RADWear. others like 5, 4, 5, 16 18 19 20 etc\n",
    "    '''\n",
    "    for patient in subject_ids:\n",
    "        print(f'Processing data for S{patient}...')\n",
    "        make_patient_data(patient)\n",
    "    '''\n",
    "    #combine_files(subject_ids)\n",
    "    print('Processing complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cd5aa",
   "metadata": {},
   "source": [
    "## Add Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b79b4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rparser:\n",
    "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
    "    VALUE_EXTRACT_KEYS = {\n",
    "        \"age\": {\n",
    "            'search_key': 'Age',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"height\": {\n",
    "            'search_key': 'Height',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"weight\": {\n",
    "            'search_key': 'Weight',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"gender\": {\n",
    "            'search_key': 'Gender',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"dominant_hand\": {\n",
    "            'search_key': 'Dominant',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"coffee_today\": {\n",
    "            'search_key': 'Did you drink coffee today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"coffee_last_hour\": {\n",
    "            'search_key': 'Did you drink coffee within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"sport_today\": {\n",
    "            'search_key': 'Did you do any sports today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoker\": {\n",
    "            'search_key': 'Are you a smoker',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoke_last_hour\": {\n",
    "            'search_key': 'Did you smoke within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"feel_ill_today\": {\n",
    "            'search_key': 'Do you feel ill today',\n",
    "            'delimiter': '? '\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #DATA_PATH = 'data/WESAD/'\n",
    "    DATA_PATH = '/mnt/c/Users/alkurdi/Desktop/Vansh/data/RADWear'\n",
    "\n",
    "    parse_file_suffix = '_readme.txt'\n",
    "        \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
    "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
    "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
    "        \n",
    "        # Check if parsed readme file is available ( should be as it is saved above )\n",
    "        if not os.path.isfile('data/readmes.csv'):\n",
    "            print('Parsing Readme files')\n",
    "            self.parse_all_readmes()\n",
    "        else:\n",
    "            print('Files already parsed.')\n",
    "            \n",
    "        self.merge_with_feature_data()\n",
    "        \n",
    "        \n",
    "    def parse_readme(self, subject_id):\n",
    "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
    "\n",
    "            x = f.read().split('\\n')\n",
    "\n",
    "        readme_dict = {}\n",
    "\n",
    "        for item in x:\n",
    "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
    "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
    "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
    "                if item.startswith(search_key):\n",
    "                    d, v = item.split(delimiter)\n",
    "                    readme_dict.update({key: v})\n",
    "                    break\n",
    "        return readme_dict\n",
    "\n",
    "\n",
    "    def parse_all_readmes(self):\n",
    "        \n",
    "        dframes = []\n",
    "\n",
    "        for subject_id, path in self.readme_locations.items():\n",
    "            readme_dict = self.parse_readme(subject_id)\n",
    "            df = pd_old.DataFrame(readme_dict, index=[subject_id])\n",
    "            dframes.append(df)\n",
    "\n",
    "        df = pd_old.concat(dframes)\n",
    "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
    "\n",
    "        \n",
    "    def merge_with_feature_data(self):\n",
    "        # Confirm feature files are available\n",
    "        if os.path.isfile('data/may14_feats4.csv'):\n",
    "            feat_df = pd.read_csv('data/may14_feats4.csv', index_col=0)\n",
    "            print(feat_df.info())\n",
    "        else:\n",
    "            print('No feature data available. Exiting...')\n",
    "            return\n",
    "           \n",
    "        # Combine data and save\n",
    "        df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
    "\n",
    "        dummy_df = pd_old.get_dummies(df)\n",
    "        \n",
    "        dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
    "\n",
    "        dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
    "                           'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
    "                           'feel_ill_today_YES', 'subject']]\n",
    "\n",
    "        merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
    "\n",
    "        merged_df.to_csv('data/no_noise.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5c59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c12d804",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f72177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4c30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#client = Client()\n",
    "\n",
    "# compute features\n",
    "\n",
    "# # for ecg\n",
    "#ddf['ECG'] \n",
    "# # for eda\n",
    "\n",
    "# # for emg\n",
    "\n",
    "# # for resp\n",
    "\n",
    "# # for temp\n",
    "\n",
    "# # for acc\n",
    "\n",
    "# # for bvp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# correlation matrix\n",
    "#correlation_value = ddf['column1'].corr(ddf['column2']).compute()\n",
    "\n",
    "\n",
    "\n",
    "# Initializing the client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd1570a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecg_nans():\n",
    "    x = 0\n",
    "    return {'bpm': np.nan , 'ibi': np.nan, 'sdnn': np.nan, 'sdsd': np.nan, \n",
    "            'rmssd': np.nan, 'pnn20': np.nan, 'pnn50': np.nan}\n",
    "\n",
    "def get_net_accel(data):\n",
    "    accn_e4 = (data['ACCx_hx'] ** 2 + data['ACCy_hx'] ** 2 + data['ACCz_hx'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "    accn_hx = (data['ACCx_hx'] ** 2 + data['ACCy_hx'] ** 2 + data['ACCz_hx'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "    acc = {'hx': accn_hx, 'e4':accn_e4}\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57afeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9058631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from fractions import Fraction\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "import pandas as pd\n",
    "import heartpy as hp\n",
    "import numpy as np\n",
    "\n",
    "Path = '/projects/bbnp/Vansh/data/RADWear/'\n",
    "\n",
    "ddf = dd.read_parquet(Path+'combined_participants.parquet/*.parquet', engine='pyarrow')\n",
    "#display(ddf)\n",
    "pdf = ddf.compute()\n",
    "#display(pdf)\n",
    "\n",
    "subject_ids = [7, 9, 12, 14, 16, 17, 21]\n",
    "n_rand_samples = 10\n",
    "list(pdf.index)[-1]\n",
    "m2df = pd.DataFrame()\n",
    "feat_df = pdf.copy(deep=True)\n",
    "feat_df.columns[0:13]\n",
    "feat_df = feat_df.drop(feat_df.columns[0:13], axis=1)\n",
    "ecg_columns = ['bpm', 'ibi', 'sdnn', 'sdsd',\n",
    "               'rmssd', 'pnn20', 'pnn50']\n",
    "m2df, m4df = pd.DataFrame(), pd.DataFrame()\n",
    "for i in ecg_columns:\n",
    "    m2df['ECG_'+i], m4df['ECG_'+i]= [], []\n",
    "m1 = {}\n",
    "m2 = {}\n",
    "fs_re = 64\n",
    "    \n",
    "def get_more_stats(df, sig_name):\n",
    "    ops_list = ['max', 'min','mean','corr', 'abs', 'cumsum', 'kurt', 'skew']\n",
    "    f_df = pd.DataFrame()\n",
    "    for op in ops_list:\n",
    "        f_df[sig_name+'_max'] = df[sig_name].max()\n",
    "        f_df[sig_name+'_min'] = df[sig_name].min()\n",
    "        f_df[sig_name+'_mean'] = df[sig_name].mean()\n",
    "        f_df[sig_name+'corr'] = df[sig_name].mean() #df.corr()\n",
    "        f_df[sig_name+'abs'] = df[sig_name].abs()\n",
    "        f_df[sig_name+'cumsum'] = df[sig_name].cumsum()\n",
    "        f_df[sig_name+'kurt'] = df[sig_name].kurt()\n",
    "        f_df[sig_name+'skew'] = df[sig_name].skew()\n",
    "        f_df[sig_name+'std'] = df[sig_name].std()\n",
    "    return f_df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e89a46c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1643587067.py, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 51\u001b[0;36m\u001b[0m\n\u001b[0;31m    ''''EDA':['min', 'max', 'mean', 'std', 'mean_phasic', 'std_phasic', 'mean_tonic', 'std_tonic', 'corr(scl, t)', 'scr_max_rise', 'scr_avg_recovery', 'scr_max_recovery', 'scr_avg_halfrecovery', 'scr_max_halfrecovery', 'scr_avg_scl', 'scr_max_scl', 'scr_avg_phasic', 'scr_max_phasic', 'scr_avg_tonic', 'scr_max_tonic'],'''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "seg_f_df = pd.DataFrame()\n",
    "for p in subject_ids[2:4]:\n",
    "    #pdf[pdf['participant']==p]\n",
    "    #print('display pd for p,', p)\n",
    "    #display(pdf[pdf['participant']==p].head(2))\n",
    "    p_pdf = pdf[pdf['participant']==p]\n",
    "    win_ind = random.sample(range(0,len(p_pdf),16), n_rand_samples)\n",
    "\n",
    "    print('participant p', p,'has n = ', len(p_pdf),' data points')\n",
    "    \n",
    "    for i in win_ind:\n",
    "        \n",
    "        print(f'progress: {win_ind.index(i)/n_rand_samples*100:.1f} %')\n",
    "        print\n",
    "        seg_df = p_pdf[i:3840+i]\n",
    "        seg_len = len(seg_df)\n",
    "        # computing ECG features\n",
    "        ''' 'ECG':['meanHR', 'stdHR', 'meanHRV', 'stdHRV', 'rmsHRV', 'NN50', 'pNN50', 'lf/hf', 'lfnorm', 'hfnorm'],'''\n",
    "        try: \n",
    "            _, m2 = hp.process(seg_df['ECG'].values, 64)\n",
    "        except:\n",
    "            _, m2 = np.nan, get_ecg_nans()\n",
    "\n",
    "        #seg_f_df = pd.concat([seg_f_df, m2])\n",
    "        \n",
    "        for key in m2.keys():\n",
    "            seg_f_df['ECG_'+key] = m2[key]\n",
    "        del m2\n",
    "\n",
    "        seg_f_df['ECG_peak_freq'] = get_peak_freq(seg_df['ECG'])\n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ECG_bpm')],axis=1) \n",
    "\n",
    "\n",
    "        # computing BVP features\n",
    "        ''' 'BVP':['meanHR', 'stdHR', 'meanHRV', 'stdHRV', 'rmsHRV', 'NN50', 'pNN50', 'lf/hf', 'lfnorm', 'hfnorm']}'''\n",
    "        try: \n",
    "            _, m4 = hp.process(seg_df['BVP'].values, 64)\n",
    "        except:\n",
    "            _, m4 = np.nan, get_ecg_nans()\n",
    "        #seg_f_df = pd.concat([seg_f_df, m4])\n",
    "        for key in m4.keys():\n",
    "            seg_f_df['BVP_'+key] = m4[key]\n",
    "        del m4\n",
    "\n",
    "        seg_f_df['BVP_peak_freq'] = get_peak_freq(seg_df['BVP'])\n",
    "        \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'BVP_bpm')],axis=1) \n",
    "\n",
    "        '''    \n",
    "        hp.process(data['ECG'].dropna().reset_index(drop=True), 700)\n",
    "        return {'bpm': m['bpm'], 'ibi': m['ibi'], 'sdnn': m['sdnn'], 'sdsd': m['sdsd'], \n",
    "                'rmssd': m['rmssd'], 'pnn20': m['pnn20'], 'pnn50': m['pnn50']}\n",
    "        ''' \n",
    "        \n",
    "        # # for eda \n",
    "        ''''EDA':['min', 'max', 'mean', 'std', 'mean_phasic', 'std_phasic', 'mean_tonic', 'std_tonic', 'corr(scl, t)', 'scr_max_rise', 'scr_avg_recovery', 'scr_max_recovery', 'scr_avg_halfrecovery', 'scr_max_halfrecovery', 'scr_avg_scl', 'scr_max_scl', 'scr_avg_phasic', 'scr_max_phasic', 'scr_avg_tonic', 'scr_max_tonic'],'''\n",
    "        seg_df['EDA'].values\n",
    "        seg_f_df['EDA_slope'] = get_slope(seg_df['EDA'])\n",
    "        seg_f_df['EDA_drange'] = get_dynamic_range(seg_df['EDA'])\n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'EDA')],axis=1) \n",
    "        \n",
    "        eda_data = nk.eda_phasic(nk.standardize(seg_df['EDA']), sampling_rate=fs_re)\n",
    "        seg_f_df['EDA_SCR'] = eda_data['EDA_Phasic']\n",
    "        seg_f_df['EDA_SCL'] = eda_data['EDA_Tonic']\n",
    "        \n",
    "        \n",
    "        #################### RESP IS MISSING FOR NOW, WILL PROCESS LATER ############################\n",
    "        # # for RESP\n",
    "        #''''RESP':['br', 'mean_inhal', 'std_inhal', 'mean_exhal', 'std_exhal', 'mean_ie', 'std_ie', 'mean_inhal_duration', 'std_inhal_duration', 'mean_exhal_duration', 'std_exhal_duration', 'mean_inhal_exhal_duration', 'std_inhal_exhal_duration'], '''\n",
    "        #seg_df['RESP_rate'], seg_df['RESP_Inhal_mean'], seg_df['RESP_Inhal_std'], seg_df['RESP_Exhal_mean'], seg_df['RESP_Exhal_std'], seg_df['RESP_I/E'] = get_resp_features(seg_df['RESP'])\n",
    "        #feat_df['RESP_rate'], feat_df['RESP_Inhal_mean'], feat_df['RESP_Inhal_std'], feat_df['RESP_Exhal_mean'], feat_df['RESP_Exhal_std'], feat_df['RESP_I/E'] = get_resp_features(seg_df['RESP'])\n",
    "        #if len(seg_len) > 0:\n",
    "        #    seg_f_df['RESP_rate'], seg_f_df['RESP_Inhal_mean'], seg_f_df['RESP_Inhal_std'], seg_f_df['RESP_Exhal_mean'], seg_f_df['RESP_Exhal_std'], seg_f_df['RESP_I/E'] = get_resp_features(seg_df['RESP'])\n",
    "        seg_f_df['BR'] = seg_df['BR']\n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'BR')],axis=1) \n",
             
    "\n",
    "        # # for temp\n",
    "        ''''TEMP':['mean', 'std', 'mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange']'''\n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'TEMP')],axis=1) \n",
    "        seg_f_df['TEMP_drange'] = get_dynamic_range(seg_df['TEMP'])\n",
    "        seg_f_df['TEMP_slope'] = get_slope(seg_df['TEMP'])\n",
    "        \n",
    "        \n",
    "        # # for acc\n",
    "        # # # acc hx\n",
    "        \n",
    "        seg_f_df['ACCx_hx_abs_integral'] = get_absolute_integral(seg_df['ACCx_hx'])\n",
    "        seg_f_df['ACCy_hx_abs_integral'] = get_absolute_integral(seg_df['ACCy_hx'])\n",
    "        seg_f_df['ACCz_hx_abs_integral'] = get_absolute_integral(seg_df['ACCz_hx'])\n",
    "        seg_f_df['ACCn_hx_abs_integral'] = get_absolute_integral(get_net_accel(seg_df)['hx'])\n",
    "        seg_f_df['ACCx_hx_peak_freq'] = get_peak_freq(seg_df['ACCx_hx'])\n",
    "        seg_f_df['ACCy_hx_peak_freq'] = get_peak_freq(seg_df['ACCy_hx'])\n",
    "        seg_f_df['ACCz_hx_peak_freq'] = get_peak_freq(seg_df['ACCz_hx'])\n",
    "        \n",
    "        seg_f_df['ACCx_hx_mean'] = seg_df['ACCx_hx'].mean()\n",
    "        seg_f_df['ACCy_hx_mean'] = seg_df['ACCy_hx'].mean()\n",
    "        seg_f_df['ACCz_hx_mean'] = seg_df['ACCz_hx'].mean()\n",
    "        seg_f_df['ACCn_hx_mean'] = get_net_accel(seg_df)['hx'].mean()\n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCx_hx')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCy_hx')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCz_hx')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCn_hx')],axis=1) \n",
    "\n",
    "        \n",
    "        # # # acc e4\n",
    "        seg_f_df['ACCx_e4_abs_integral'] = get_absolute_integral(seg_df['ACCx_e4'])\n",
    "        seg_f_df['ACCy_e4_abs_integral'] = get_absolute_integral(seg_df['ACCy_e4'])\n",
    "        seg_f_df['ACCz_e4_abs_integral'] = get_absolute_integral(seg_df['ACCz_e4'])\n",
    "        seg_f_df['ACCn_e4_abs_integral'] = get_absolute_integral(get_net_accel(seg_df)['e4'])\n",
    "        seg_f_df['ACCx_e4_peak_freq'] = None if seg_len == 0 else get_peak_freq(seg_df['ACCx_e4'])\n",
    "        seg_f_df['ACCy_e4_peak_freq'] = None if seg_len == 0 else get_peak_freq(seg_df['ACCy_e4'])\n",
    "        seg_f_df['ACCz_e4_peak_freq'] = None if seg_len == 0 else get_peak_freq(seg_df['ACCz_e4'])\n",
    "        \n",
    "        seg_f_df['ACCx_e4_mean'] = seg_df['ACCx_e4'].mean()\n",
    "        seg_f_df['ACCy_e4_mean'] = seg_df['ACCy_e4'].mean()\n",
    "        seg_f_df['ACCz_e4_mean'] = seg_df['ACCz_e4'].mean()\n",
    "        seg_f_df['ACCn_e4_mean'] = get_net_accel(seg_df)['e4'].mean()        \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCx_e4')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCy_e4')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCz_e4')],axis=1) \n",
    "        seg_f_df = pd.concat([seg_f_df, get_more_stats(seg_df,'ACCn_e4')],axis=1) \n",
    "\n",
    "        ''''ACC':['mean', 'std', 'abs integral',' mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange', 'peak frequency'], # for x, y, z, and norm'''\n",
    "  \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        feat_df.loc[i] = seg_f_df\n",
    "\n",
    "'''\n",
    "features_list =  'TEMP':['mean', 'std', 'mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange'],\n",
    "                 'ACC':['mean', 'std', 'abs integral',' mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange', 'peak frequency'], # for x, y, z, and norm\n",
    "             \n",
    "'BVP', 'ECG', 'EDA',\n",
    "'TEMP', 'BR',\n",
    "'ACCx_hx', 'ACCy_hx', 'ACCz_hx', \n",
    "'ACCx_e4', 'ACCy_e4', 'ACCz_e4',\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51446098",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_df.drop('daily_check_in_date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f850727",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(type(seg_df))\n",
    "#display(seg_df)\n",
    "#print('seg_df mean: ',seg_df.mean())\n",
    "#display('seg_df types',seg_df.dtypes)  \n",
    "display(seg_df.drop('daily_check_in_date',axis=1))\n",
    "display('seg_df types',seg_df.drop('daily_check_in_date',axis=1).dtypes)  \n",
    "display(type(love))\n",
    "love = seg_df.drop('daily_check_in_date',axis=1)\n",
    "seg_df.max(), seg_df.min(), seg_df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_pdf['ECG'][i:3840+i].index-97177717)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ad83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pdf['ECG'][i:3840+i]\n",
    "\n",
    "plt.figure();\n",
    "plt.plot((p_pdf['ECG'][i:3840+i].index-97177717), p_pdf['ECG'][i:3840+i].values);\n",
    "plt.title('ECG', fontsize=15)\n",
    "plt.xlabel('index');\n",
    "plt.xticks(fontsize=10);\n",
    "plt.yticks(fontsize=10);\n",
    "#plt.xscale('log');\n",
    "plt.ylabel('Accuracy', fontsize=20);\n",
    "\n",
    "\n",
    "_, m2 = hp.process(p_pdf['ECG'][i:3840+i].values, sample_rate=64, windowsize= 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024af0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.repartition(npartitions=150)\n",
    "print(ddf.columns)\n",
    "display(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00be5d8",
   "metadata": {},
   "source": [
    "EDA          \n",
    "EDA_SCR      \n",
    "EDA_SCL      \n",
    "BVP          \n",
    "TEMP         \n",
    "ACC_x        \n",
    "ACC_y        \n",
    "ACC_z        \n",
    "label        \n",
    "ECG          \n",
    "EDA_C        \n",
    "EDA_SCR_C    \n",
    "EDA_SCL_C    \n",
    "ACC_x_C      \n",
    "ACC_y_C      \n",
    "ACC_z_C      \n",
    "EMG_C        \n",
    "Resp_C       \n",
    "Temp_C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = {'ECG':['meanHR', 'stdHR', 'meanHRV', 'stdHRV', 'rmsHRV', 'NN50', 'pNN50', 'lf/hf', 'lfnorm', 'hfnorm'],\n",
    "                 'EDA':['min', 'max', 'mean', 'std', 'mean_phasic', 'std_phasic', 'mean_tonic', 'std_tonic', 'corr(scl, t)', 'scr_max_rise', 'scr_avg_recovery', 'scr_max_recovery', 'scr_avg_halfrecovery', 'scr_max_halfrecovery', 'scr_avg_scl', 'scr_max_scl', 'scr_avg_phasic', 'scr_max_phasic', 'scr_avg_tonic', 'scr_max_tonic'],\n",
    "                 'RESP':['br', 'mean_inhal', 'std_inhal', 'mean_exhal', 'std_exhal', 'mean_ie', 'std_ie', 'mean_inhal_duration', 'std_inhal_duration', 'mean_exhal_duration', 'std_exhal_duration', 'mean_inhal_exhal_duration', 'std_inhal_exhal_duration'],\n",
    "                 'Temp':['mean', 'std', 'mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange'],\n",
    "                 'ACC':['mean', 'std', 'abs integral',' mean_slope', 'std_slope', 'mean_drange', 'std_drange', 'min', 'max', 'slope', 'drange', 'peak frequency'], # for x, y, z, and norm\n",
    "                 'BVP':['meanHR', 'stdHR', 'meanHRV', 'stdHRV', 'rmsHRV', 'NN50', 'pNN50', 'lf/hf', 'lfnorm', 'hfnorm']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995106e",
   "metadata": {},
   "source": [
    "## ECG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760dfe13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a335105",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf.partitions[0]['ECG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0]['ECG'].compute()\n",
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[i]['ECG'].compute()[0:3840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for j in range(561,540,-1):\n",
    "    #print('[i*16:3840+16*i]:  [', j*16,', ', 3840+16*j,']')\n",
    "    \n",
    "    ddf.partitions[i]['ECG'].compute()[j*16:3840+16*j]\n",
    "    \n",
    "    _, m2 = hp.process(ddf.partitions[i]['ECG'].compute()[start:end].values, sample_rate=64, windowsize= 60)\n",
    "    m2df = pd.DataFrame([m2])\n",
    "    del m2\n",
    "    m2ddf = dd.from_pandas(m2df, npartitions=100)\n",
    "    del m2df\n",
    "    m2dddf = dd.multi.concat([m2ddf, m2ddf])\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbf97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hp_results = dd.from_pandas(pd.DataFrame(m2), npartitions=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.concat([ddf,ddf_i],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d814769",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(wd2))\n",
    "print(type(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0].rolling(window=64, min_periods=1).mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8872f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 64\n",
    "wd200, m200 = hp.process(ddf.partitions[0]['ECG'].compute(), sample_rate=64,calc_freq = True)\n",
    "n_window = WINDOW_IN_SECONDS*sample_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10decd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c98886",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd2['hr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09751200",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(150):\n",
    "    print(len(ddf.partitions[i]))\n",
    "#ddf.partitions[0].iloc[:, len_ddf_part//100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b1603",
   "metadata": {},
   "source": [
    "## EDA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b187a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_data = nk.eda_phasic(nk.standardize(eda_df['EDA']), sampling_rate=fs_dict['EDA'])\n",
    "eda_df['EDA_SCR'] = eda_data['EDA_Phasic']\n",
    "eda_df['EDA_SCL'] = eda_data['EDA_Tonic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb7c21",
   "metadata": {},
   "source": [
    "## BVP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_data = nk.eda_phasic(nk.standardize(eda_df['EDA']), sampling_rate=fs_dict['EDA'])\n",
    "    eda_df['EDA_SCR'] = eda_data['EDA_Phasic']\n",
    "    eda_df['EDA_SCL'] = eda_data['EDA_Tonic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb12ec",
   "metadata": {},
   "source": [
    "## RESP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1873e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a31007",
   "metadata": {},
   "source": [
    "## TEMP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24947319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bced44f",
   "metadata": {},
   "source": [
    "## ACC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35aa78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_stats_ecg(data, label=-1):\n",
    "    wd, m = hp.process(data['ECG'].dropna().reset_index(drop=True), 64)\n",
    "    return {'bpm': m['bpm'], 'ibi': m['ibi'], 'sdnn': m['sdnn'], 'sdsd': m['sdsd'], \n",
    "            'rmssd': m['rmssd'], 'pnn20': m['pnn20'], 'pnn50': m['pnn50']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subject_ids[5]\n",
    "\n",
    "print(Path +  '/p_'+str(p)+'.pkl: ', os.path.isfile(Path +  '/p_'+str(p)+'.pkl'))\n",
    "#with open(Path +  '/p_'+str(p)+'.pkl', 'rb') as f:\n",
    "#    df = pickle.load(f)\n",
    "#df = pd_old.read_pickle('data/RADWear/p_'+str(p)+'.pkl')\n",
    "\n",
    "\n",
    "## Correlation of Each Feature to Target\n",
    "#vals = abs(df.corr()['label']).sort_values(ascending=False)\n",
    "#for i in range(len(vals)):\n",
    "#    print(vals.index[i], vals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a138010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf39dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:, df.columns != 'label'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft_idx in range(len(features)):\n",
    "    print(features[ft_idx], ft_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6898be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ee52",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0b4a2",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f7df5",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10675fc1",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9b03e",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cf325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "lda_baseline_acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b05e36",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ac10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = np.geomspace(1, 1000, 100)\n",
    "xg_accs = []\n",
    "for est in ests:\n",
    "    xg = XGBClassifier(n_estimators=int(est), n_jobs=-1)\n",
    "    xg.fit(X_train, y_train)\n",
    "    y_pred = xg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    xg_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "    xg_accs.append(xg_baseline_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12));\n",
    "plt.plot(ests, xg_accs);\n",
    "plt.title('XGB Accuracy vs. Num Estimators', fontsize=25)\n",
    "plt.xlabel('Number Estimators', fontsize=20);\n",
    "plt.xticks(fontsize=15);\n",
    "plt.yticks(fontsize=15);\n",
    "plt.xscale('log');\n",
    "plt.ylabel('Accuracy', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630814ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XGBClassifier(n_estimators=100, n_jobs=-1)\n",
    "xg.fit(X_train, y_train)\n",
    "y_pred = xg.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "xg_baseline_acc = accuracy_score(y_test, y_pred)\n",
    "xg_accs.append(xg_baseline_acc)\n",
    "importances = xg.feature_importances_\n",
    "\n",
    "xgb_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "xgb_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58bc21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed821",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  \n",
    "rf_baseline_acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44af091",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d21bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = classifier.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af563",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5e3e0",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)\n",
    "svm_baseline_acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7be4d",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(abs(clf.coef_[0]), index=features).nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e46db",
   "metadata": {},
   "source": [
    "### Linear SVM (lambda 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=0.9, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)\n",
    "svm_baseline_acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b61486",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(abs(clf.coef_[0]), index=features).nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b94aa",
   "metadata": {},
   "source": [
    "## Single Decision Tree Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c314a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "\n",
    "feat_importance = dt.feature_importances_\n",
    "plt.figure(figsize=(40,20));\n",
    "tree.plot_tree(dt,fontsize=10, feature_names=features);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b6c7f",
   "metadata": {},
   "source": [
    "### Top 5 Most Improtant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip(feat_importance, features)), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b3084",
   "metadata": {},
   "source": [
    "# Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737390",
   "metadata": {},
   "source": [
    "## Signal to Noise Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099bbe0",
   "metadata": {},
   "source": [
    "For a non-constant signal $S$ and noise $N$, the signal to noise ratio is defined as the following:\n",
    "$$ SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70b232",
   "metadata": {},
   "source": [
    "The expected value $\\mathbb{E}[X]$ of any continuous random variable $X$ is $\\int_{-\\infty}^{\\infty} x p(x) dx $, where $p(x)$ is its associated probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d01d",
   "metadata": {},
   "source": [
    "For homoskedastic noise, we can use closed form expressions to compute $E[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$.\n",
    "\n",
    "- For uniformly distributed noise $N$ ~ $u(\\alpha, \\beta)$, by the same logic as above $\\mathbb{E}[N^2] = \\left(\\frac{\\alpha - \\beta}{2}\\right)^2$.\n",
    "\n",
    "- For frequency-domain noise $N$ of the form $A\\sin(2\\pi x \\frac{1}{f}) + y, \\mathbb{E}[N^2] \\approx y^2 + \\frac{A^2}{2}$. Note the $\\approx$ since we cannot guarantee that the signal will end precisely on the end of the sin wave.\n",
    "\n",
    "For heteroskedastic noise, because there is no closed form expression, we simply take `N.mean()` where $N$ is our noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_noise_ratio(signal, noise_type, noise_dist, noise=None, sigma=None, alpha=None, beta=None, \n",
    "                          vertical_shift=None, amplitude=None):\n",
    "    \"\"\"\n",
    "    Function: Computes the signal to noise ratio for a given signal and its corresponding noise.\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we are evaluating\n",
    "        noise_type (string): 'Heteroskedastic' or 'Homoskedastic'\n",
    "        noise_dist (string): 'Uniform', 'Gaussian', or 'Frequency' for now\n",
    "        noise (array or ndarray): Only passed in if we have heteroskedastic noise\n",
    "        sigma (float): Sigma parameter of the gaussian\n",
    "        alpha (float): Alpha parameter of the uniform\n",
    "        beta (float): Beta parameter of the uniform\n",
    "        vertical_shift (float): Vertical shift parameter of the frequency\n",
    "        amplitude (float): Amplitude parameter of the frequency\n",
    "        \n",
    "    :return\n",
    "        signal_to_noise_ratio (float): Signal to noise ratio... E[S^2]/E[N^2]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate E[S^2]\n",
    "    e_s2 = (signal**2).mean()\n",
    "    e_n2 = None\n",
    "    \n",
    "    if noise_type == 'Homoskedastic':\n",
    "        # Calculate E[N^2] for the pertinent case\n",
    "        if noise_dist == 'Uniform':\n",
    "            e_n2 = (0.5*(alpha - beta))**2\n",
    "        elif noise_dist == 'Gaussian':\n",
    "            e_n2 = sigma**2\n",
    "        elif noise_dist == 'Frequency':\n",
    "            e_n2 = vertical_shift**2 + (amplitude**2)/2\n",
    "    elif noise_type == 'Heteroskedastic':\n",
    "        e_n2 = (noise**2).mean()\n",
    "    \n",
    "    # Return the signal to noise ratio\n",
    "    return e_s2/e_n2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542fdb4",
   "metadata": {},
   "source": [
    "## Calculate Distribution Parameters from SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb7d2c",
   "metadata": {},
   "source": [
    "Given a signal $S$, we can specify a signal to noise ratio $SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]}$ and use this to calculate $\\mathbb{E}[N^2]$ because $SNR$ and $\\mathbb{E}[S^2]$ are known. So $\\mathbb{E}[N^2] = \\frac{\\mathbb{E}[S^2]}{SNR}$.\n",
    "\n",
    "Then, for any homoskedastic noise following a well-defined probability density function (PDF), we can solve for the parameters of the PDF using the known value $\\mathbb{E}[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma^2)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$. Thus, $\\sigma^2 = \\frac{\\mathbb{E}[S^2]}{SNR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42057019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_param(signal, noise_type, signal_to_noise_ratio):\n",
    "    \"\"\"\n",
    "    Function: Calculates the parameters of a continuous probability density function given\n",
    "    our desired signal to noise ratio and signal.\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): Our signal\n",
    "        noise_type (string): Probability distribution of our noise (i.e., Gaussian)\n",
    "        signal_to_noise_ratio (float): Desired signal to noise ratio\n",
    "    \n",
    "    :return\n",
    "    (for now, just)\n",
    "        sigma (float): sigma of the gaussian\n",
    "    \"\"\"\n",
    "    \n",
    "    return (signal**2).mean()/signal_to_noise_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819be01",
   "metadata": {},
   "source": [
    "## Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99319416",
   "metadata": {},
   "source": [
    "The Gaussian probability density function is of the following form:\n",
    "\\begin{equation}\\label{eq:}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4198cb0",
   "metadata": {},
   "source": [
    "### Estimating $\\mu$ and $\\sigma$ of the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330a231",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential with Homoskedasticity Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4116e",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with homoskedasticity approach constructs a Gaussian distribution such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_i - S_{i+n}|)$, where $max(|S_i - S_{i+n}|)$ denotes the maximum absolute difference of the signal between index $i$ and $i+n$ in the entire signal, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "In conclusion, we randomly sample from the following probability density function:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\alpha \\cdot max(|S_i - S_{i+n}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_i - S_{i+n}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "This noise exhbits homoskedasticity because it does not vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_homoskedastic(signal_name, signal, signal_to_noise_ratio=None):\n",
    "    \"\"\"\n",
    "    Constructs a homoskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal_name (string): The name of the signal (i.e., ECG)\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        signal_to_noise_ratio (float): [default: None] If specified, our desired SNR.\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    x_new = None\n",
    "\n",
    "    if signal_name == 'ACC':\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        # Noise X Axis\n",
    "        x_axis = signal[:,0]\n",
    "        sigma = calculate_param(x_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        x_axis_new = np.copy(x_axis)\n",
    "        for i in range(len(x_axis_new)):\n",
    "            x_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Y Axis\n",
    "        y_axis = signal[:,1]\n",
    "        sigma = calculate_param(y_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        y_axis_new = np.copy(y_axis)\n",
    "        for i in range(len(y_axis_new)):\n",
    "            y_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Z Axis\n",
    "        z_axis = signal[:,2]\n",
    "        sigma = calculate_param(z_axis, 'Gaussian', signal_to_noise_ratio)\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        z_axis_new = np.copy(z_axis)\n",
    "        for i in range(len(z_axis_new)):\n",
    "            z_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "\n",
    "        # Put together noisy signal\n",
    "        x_new = np.zeros((len(signal), 3))\n",
    "        x_new[:,0] = x_axis_new\n",
    "        x_new[:,1] = y_axis_new\n",
    "        x_new[:,2] = z_axis_new\n",
    "\n",
    "        return (x_new, sigma)\n",
    "    else: \n",
    "        # Store original shape\n",
    "        original_shape = signal.shape\n",
    "\n",
    "        # Caveat: some signals like ACC have three axes\n",
    "        # Flatten signal to be 1d\n",
    "        x = np.ravel(signal)\n",
    "\n",
    "        # Calculate mean and Standard deviation\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        sigma = calculate_param(x, 'Gaussian', signal_to_noise_ratio)\n",
    "\n",
    "        # test that this works\n",
    "        x_new = x + np.random.normal(mu, sigma, (len(x),))\n",
    "\n",
    "#         for i in range(len(x_new)):\n",
    "#             x_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "\n",
    "        return (np.array(x_new).reshape(original_shape), sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17482202",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential Approach with Heteroskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87306268",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with heteroskedasticity approach constructs a Gaussian probability density function such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)$, where $max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)$ denotes the maximum absolute difference of the signal between any index $i$ and $i+n$ in the last $100$ samples from index $t$, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "At each fixed time value $t \\in [S_{start}, S_{end}]$, we randomly sample from the following probability density function:\n",
    "$$\n",
    "\\epsilon_t = f(x, t) = \\frac{1}{\\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "We add each $\\epsilon_t$ to its respective value of $S_t$ to add noise to the signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_heteroskedastic(signal):\n",
    "    \"\"\"\n",
    "    Constructs a heteroskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = signal.shape\n",
    "    \n",
    "    # Caveat: some signals like ACC have three axes\n",
    "    # Flatten signal to be 1d\n",
    "    x = np.ravel(signal)\n",
    "    \n",
    "    # Calculate mean and Standard deviation\n",
    "    alpha = 0.5\n",
    "    mu = 0\n",
    "    sigma = None\n",
    "    \n",
    "    # Add heteroskedastic noise\n",
    "    x_new = np.copy(x)\n",
    "    noise = np.array([])\n",
    "    window_len = 100\n",
    "    for i in range(100, len(x)):\n",
    "        x_rolling = x[i-100:i]\n",
    "        sigma = alpha*abs(x_rolling - np.roll(x_rolling, 5)).max()\n",
    "        noise_i = float(np.random.normal(mu, sigma, 1))\n",
    "        np.append(noise, noise_i)\n",
    "        x_new[i] += noise_i\n",
    "    for i in range(len(x)-100, len(x)):\n",
    "        x_new[len(x)-i] += float(np.random.normal(mu, sigma, 1))  \n",
    "\n",
    "    return (np.array(x_new).reshape(original_shape), signal_to_noise_ratio(signal=x, noise=noise, noise_type='Heteroskedastic', \n",
    "                                        noise_dist='Gaussian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe612ffe",
   "metadata": {},
   "source": [
    "## Frequency Domain Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f99a38",
   "metadata": {},
   "source": [
    "A sinusoidal wave is of the form:\n",
    "\n",
    "\\begin{equation}\\label{eq:1}\n",
    "f(x) = Asin(2 \\pi T*x) + y\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is the amplitude of the wave, $T = \\frac{1}{f}$ is the period, and $y$ is the vertical shift. Note that we have omitted the phase shift parameter since we will overlay the noise to the data at pre-determined locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a sinusoidal wave\n",
    "def sinusoidal_wave(amplitude, frequency, vertical_shift, x):\n",
    "    return amplitude * np.sin(2*np.pi*(1/frequency)*x) + vertical_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bbf9a",
   "metadata": {},
   "source": [
    "Each physiological signal has a different sampling frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dict = {'ACC': 64, 'BVP': 64, 'EDA': 64, 'TEMP': 64, 'label': 64, 'Resp': 64, 'ECG': 64, 'chest': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658acac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(signal, hz):\n",
    "    \"\"\"\n",
    "    Constructs a heteroskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        hz (float): The hertz (frequency) at which we wish to add noise)\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = signal.shape\n",
    "    \n",
    "    # Caveat: some signals like ACC have three axes\n",
    "    # Flatten signal to be 1d\n",
    "    x = np.ravel(signal)\n",
    "    \n",
    "    # Caveat: each signal has a different base sampling frequency\n",
    "    # Add frequency noise\n",
    "    sinusoidal_wave(amplitude, 1/hz, vertical_shift, x)\n",
    "    x_new = np.copy(x)\n",
    "\n",
    "    return np.array(x_new).reshape(original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71592233",
   "metadata": {},
   "source": [
    "### Estimating $A$, $T$ and $y$ of the Sinusoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eadcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28f5e839",
   "metadata": {},
   "source": [
    "## Exporting Noisy Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d546927",
   "metadata": {},
   "source": [
    "### Function to Write File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path, file_name, data):\n",
    "    \"\"\"\n",
    "    Function: Writes filename to its specified path and then dumps data in .pkl format to that file.\n",
    "    \n",
    "    :param:\n",
    "        path (string): Where you want to write (create) the file\n",
    "        file_name (string): What you want to name the file you're creating\n",
    "        data (array or ndarray or pd.DataFrame): What you want inside the file you're creating\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filename = file_name + '.pkl'\n",
    "    with open(os.path.join(path, filename), 'wb') as dest:\n",
    "        pickle.dump(data, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50265a",
   "metadata": {},
   "source": [
    "Root Directory (you will likely need to change, unless you are Sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66245ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = r'C:\\Users\\Vansh\\Desktop\\anxietyFB-1\\data\\WESAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783e4a1",
   "metadata": {},
   "source": [
    "### Function to Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snrs = [0.01, 0.05, 0.1, 0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23027610",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b56364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snrs = [0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(rootdir, snrs, body_parts=['wrist', 'chest'],\n",
    "              signals={'wrist': ['ACC', 'BVP', 'EDA', 'TEMP'],'chest': ['ECG', 'Temp', 'EMG', 'Resp', 'ACC']}, num_runs=10):\n",
    "    \"\"\"\n",
    "    Function: Adds noise to the WESAD data, stored in the specified root directory\n",
    "    \n",
    "    :param:\n",
    "        rootdir (string): The root directory from which to read the WESAD data.\n",
    "        body_parts (list): [Default: '[wrist', 'chest']] Body parts from which to read the WESAD data\n",
    "        signals (dict): [Default: see above] The physiological signals each body part has\n",
    "    :return\n",
    "        None\n",
    "    \"\"\"\n",
    "    snr=snrs\n",
    "    patients = []\n",
    "    patients_with_noise1 = []\n",
    "    patients_with_noise2 = []\n",
    "    patients_with_noise3 = []\n",
    "    patients_with_noise4 = []\n",
    "    patients_with_noise5 = []\n",
    "    #patient_idx = 0\n",
    "    # Iterate through each patient's folder and construct a df with all patient data\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        #output_dir = os.path.join(rootdir, f\"run_{run+1}\")\n",
    "        #os.makedirs(output_dir, exist_ok=True)\n",
    "        patient_idx = 0\n",
    "        for subdir, dirs, files in os.walk(rootdir):\n",
    "            for file in files:\n",
    "\n",
    "\n",
    "                # If this is a .pkl file then it's the synchronized features/labels\n",
    "                # and we want to serialize the file\n",
    "                if '.pkl' in file and 'gauss' not in file: \n",
    "                    output_dir = os.path.join(rootdir, f\"{file.split('.')[0]}\")\n",
    "                    output_dir = os.path.join(output_dir, f\"run_{run+1}\")\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    patient = pd.read_pickle(os.path.join(subdir, file))\n",
    "                    #print(patient)\n",
    "                    for snr in snrs: \n",
    "                        noisy_patient = patient.copy()\n",
    "                        for body_part in body_parts:\n",
    "                            # Add noise\n",
    "                            for sgl in signals[body_part]:\n",
    "                                # Get signal\n",
    "                                signal = patient['signal'][body_part][sgl]\n",
    "                                x_gaussian_homoskedastic, sigma = gaussian_homoskedastic(sgl, signal, snr)\n",
    "                                noisy_patient['signal'][body_part][sgl] = x_gaussian_homoskedastic\n",
    "\n",
    "                        #output_subdir = os.path.join(output_dir, f\"patient_{patient_idx + 1}\")\n",
    "                        #os.makedirs(output_subdir, exist_ok=True)\n",
    "                        output_filename = f\"{file.split('.')[0]}_gauss_homo_snr_{snr}.pkl\"\n",
    "                        write_file(output_dir, output_filename, noisy_patient)\n",
    "                        del noisy_patient\n",
    "                    patient_idx+=1\n",
    "                    del patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec21026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "#add_noise(rootdir, snrs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3f98b",
   "metadata": {},
   "source": [
    "# Feature Extraction (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab017de",
   "metadata": {},
   "source": [
    "Do feature extraction again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_new_noise1 = []\n",
    "patients_new_noise2 = []\n",
    "patients_new_noise3 = []\n",
    "patients_new_noise4 = []\n",
    "patients_new_noise5 = []\n",
    "# Iterate through each patient's folder and construct a\n",
    "# df with all patient data\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "\n",
    "        # If this is a .pkl file then it's the synchronized features/labels\n",
    "        # and we want to serialize the file\n",
    "        if '.pkl' in file and 'gauss' in file and 'snr_0.001' in file:\n",
    "            patients_new_noise1.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.15' in file:\n",
    "            patients_new_noise2.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.3' in file:\n",
    "            patients_new_noise3.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.4' in file:\n",
    "            patients_new_noise4.append(pd.read_pickle(subdir + '/' + file))\n",
    "        elif '.pkl' in file and 'gauss' in file and 'snr_0.6' in file:\n",
    "            patients_new_noise5.append(pd.read_pickle(subdir + '/' + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5a055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(16,4))\n",
    "#[4015000:4020000]\n",
    "axs[0].plot(np.ravel(patients_new_noise1[0]['signal']['chest']['Resp'][3000000 - 100 :3000000]))\n",
    "axs[0].set_title('SNR: 0.001')\n",
    "axs[1].plot(np.ravel(patients_new_noise2[0]['signal']['chest']['Resp'][3000000 - 100 :3000000]))\n",
    "axs[1].set_title('SNR: 0.15')\n",
    "axs[2].plot(np.ravel(patients_new_noise3[0]['signal']['chest']['Resp'][3000000 - 100 :3000000]))\n",
    "axs[2].set_title('SNR: 0.3')\n",
    "axs[3].plot(np.ravel(patients_new_noise4[0]['signal']['chest']['Resp'][3000000 - 100 :3000000]))\n",
    "axs[3].set_title('SNR: 0.4')\n",
    "axs[4].plot(np.ravel(patients_new_noise5[0]['signal']['chest']['Resp'][3000000 - 100 :3000000]))\n",
    "axs[4].set_title('SNR: 0.6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022334",
   "metadata": {},
   "source": [
    "# Data Preparation (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce48d1c",
   "metadata": {},
   "source": [
    "Prepare the data again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the data for each subject\n",
    "class SubjectDataWithNoise:\n",
    "\n",
    "    def __init__(self, main_path, subject_number, snr):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + \"/run_1/\" + self.name + f'_gauss_homo_snr_{snr}.pkl.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'ACC_C': self.data['signal']['chest']['ACC'],\n",
    "                     'ECG_C': self.data['signal']['chest']['ECG'],\n",
    "                     'EDA_C': self.data['signal']['chest']['EDA'],\n",
    "                     'EMG_C': self.data['signal']['chest']['EMG'],\n",
    "                     'Resp_C': self.data['signal']['chest']['Resp'],\n",
    "                     'Temp_C': self.data['signal']['chest']['Temp']\n",
    "                     })\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.wrist_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f442ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data_with_noise(subject_id):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    for snr in snrs: \n",
    "        print(snr)\n",
    "        subject = SubjectDataWithNoise(main_path='data/WESAD', subject_number=subject_id, snr=snr)\n",
    "\n",
    "        # Empatica E4 data - now with resp\n",
    "        e4_data_dict = subject.get_wrist_data()\n",
    "\n",
    "        # Chest data\n",
    "        ch_data_dict = subject.get_chest_data()\n",
    "        \n",
    "        # norm type\n",
    "        norm_type = None\n",
    "\n",
    "        # The 3 classes we are classifying\n",
    "        grouped, baseline, stress, amusement = compute_features(e4_data_dict, ch_data_dict, subject.labels, norm_type)\n",
    "\n",
    "        # print(f'Available windows for {subject.name}:')\n",
    "        n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "        n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "        n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "        # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "        #\n",
    "        baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
    "        #for col in baseline_samples.columns:\n",
    "        #    print(col)\n",
    "        # Downsampling\n",
    "        # baseline_samples = baseline_samples[::2]\n",
    "        stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
    "        amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
    "\n",
    "        all_samples = pd_old.concat([baseline_samples, stress_samples, amusement_samples])\n",
    "        all_samples = pd_old.concat([all_samples.drop('label', axis=1), pd_old.get_dummies(all_samples['label'])], axis=1)\n",
    "        # Save file as csv\n",
    "        all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats_with_noise_{snr}.csv')\n",
    "\n",
    "        subject = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_files_with_noise(subjects):\n",
    "    df_list = []\n",
    "    for snr in snrs: \n",
    "        for s in subjects:\n",
    "            df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_with_noise_{snr}.csv', index_col=0)\n",
    "            df['subject'] = s\n",
    "            df_list.append(df)\n",
    "\n",
    "        df = pd_old.concat(df_list)\n",
    "\n",
    "        df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "        df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print('here')\n",
    "        df.to_csv(f'{savePath}/{snr}_feats4_with_noise.csv')\n",
    "\n",
    "        counts = df['label'].value_counts()\n",
    "        print('Number of samples per class:')\n",
    "        for label, number in zip(counts.index, counts.values):\n",
    "            print(f'{int_to_label[label]}: {number}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rparser_with_noise:\n",
    "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
    "    VALUE_EXTRACT_KEYS = {\n",
    "        \"age\": {\n",
    "            'search_key': 'Age',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"height\": {\n",
    "            'search_key': 'Height',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"weight\": {\n",
    "            'search_key': 'Weight',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"gender\": {\n",
    "            'search_key': 'Gender',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"dominant_hand\": {\n",
    "            'search_key': 'Dominant',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"coffee_today\": {\n",
    "            'search_key': 'Did you drink coffee today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"coffee_last_hour\": {\n",
    "            'search_key': 'Did you drink coffee within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"sport_today\": {\n",
    "            'search_key': 'Did you do any sports today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoker\": {\n",
    "            'search_key': 'Are you a smoker',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoke_last_hour\": {\n",
    "            'search_key': 'Did you smoke within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"feel_ill_today\": {\n",
    "            'search_key': 'Do you feel ill today',\n",
    "            'delimiter': '? '\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DATA_PATH = 'data/WESAD/'\n",
    "    parse_file_suffix = '_readme.txt'\n",
    "    \n",
    "    \n",
    "    def __init__(self, snrs, run):\n",
    "        \n",
    "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
    "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
    "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
    "        \n",
    "        # Check if parsed readme file is available ( should be as it is saved above )\n",
    "        if not os.path.isfile('data/readmes.csv'):\n",
    "            print('Parsing Readme files')\n",
    "            self.parse_all_readmes()\n",
    "        else:\n",
    "            print('Files already parsed.')\n",
    "            \n",
    "        self.merge_with_feature_data_with_noise(snrs, run)\n",
    "        \n",
    "        \n",
    "    def parse_readme(self, subject_id):\n",
    "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
    "\n",
    "            x = f.read().split('\\n')\n",
    "\n",
    "        readme_dict = {}\n",
    "\n",
    "        for item in x:\n",
    "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
    "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
    "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
    "                if item.startswith(search_key):\n",
    "                    d, v = item.split(delimiter)\n",
    "                    readme_dict.update({key: v})\n",
    "                    break\n",
    "        return readme_dict\n",
    "\n",
    "\n",
    "    def parse_all_readmes(self):\n",
    "        \n",
    "        dframes = []\n",
    "\n",
    "        for subject_id, path in self.readme_locations.items():\n",
    "            readme_dict = self.parse_readme(subject_id)\n",
    "            df = pd.DataFrame(readme_dict, index=[subject_id])\n",
    "            dframes.append(df)\n",
    "\n",
    "        df = pd_old.concat(dframes)\n",
    "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
    "\n",
    "        \n",
    "    def merge_with_feature_data_with_noise(self, snrs, run):\n",
    "        # Confirm feature files are available\n",
    "        for snr in snrs: \n",
    "            if os.path.isfile(f'data/{snr}_feats4_with_noise.csv'):\n",
    "                feat_df = pd.read_csv(f'data/{snr}_feats4_with_noise.csv', index_col=0)\n",
    "                print(feat_df.info())\n",
    "            else:\n",
    "                print('No feature data available. Exiting...')\n",
    "                return\n",
    "            \n",
    "            # Combine data and save\n",
    "            df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
    "\n",
    "            dummy_df = pd_old.get_dummies(df)\n",
    "            \n",
    "            dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
    "\n",
    "            dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
    "                            'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
    "                            'feel_ill_today_YES', 'subject']]\n",
    "\n",
    "            merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
    "\n",
    "            merged_df.to_csv(f'data/{run}/noise_snr_{snr}.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "num_itr = 10\n",
    "snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "for i in range(6, num_itr):\n",
    "    if i != 6: \n",
    "        add_noise(rootdir, snrs, num_runs=1)\n",
    "    print(i)\n",
    "    for patient in subject_ids:\n",
    "        print(f'Processing data for S{patient}...')\n",
    "        make_patient_data_with_noise(patient)\n",
    "\n",
    "    combine_files_with_noise(subject_ids)\n",
    "    print('Processing complete.')\n",
    "    rp_with_noise = rparser_with_noise(snrs, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97427c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rp_with_noise = rparser_with_noise(snrs, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb3726",
   "metadata": {},
   "source": [
    "# Modeling (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99893b",
   "metadata": {},
   "source": [
    "Model again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f997c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/1/noise_snr_0.6.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.isna().sum()\n",
    "for i in range(len(s)):\n",
    "    if s[i] > 0:\n",
    "        print(s.index[i], s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.loc[:, df.columns != 'Resp_C_rate']\n",
    "#df\n",
    "df = df.dropna(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:, df.columns != 'label'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6559b2c",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63bf21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8aa91f",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = classifier.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "forest_importances[:20].plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d44d3",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973d694",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4551",
   "metadata": {},
   "source": [
    "Compare the results of the noisy data models and the clean data models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb53c5",
   "metadata": {},
   "source": [
    "- Plots of SNR (x-axis) vs. accuracy (y-axis)\n",
    "- Compare feature importances across different noise regimes\n",
    "    - Develop dynamic evaluation method based on original feature importance / added noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0de9",
   "metadata": {},
   "source": [
    "## Test Each Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10e7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "lda_accuracy = {key: [] for key in snrs}\n",
    "rf_accuracy = {key: [] for key in snrs}\n",
    "svm_accuracy = {key: [] for key in snrs}\n",
    "ft_imp_matrix = {key: [] for key in snrs}\n",
    "ft_imp_schmidt = {key: [] for key in snrs}\n",
    "# For each signal to noise ratio\n",
    "\n",
    "for j in range(1, 7):\n",
    "    for i in range(len(snrs)):\n",
    "            # Get data\n",
    "        df = pd.read_csv(f'data/{j}/noise_snr_{snrs[i]}.csv', index_col=0)\n",
    "            # Since Resp_C_rate is null for the first four, simply get rid of it\n",
    "        #df = df.loc[:, df.columns != 'Resp_C_rate']\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df = df.dropna(axis=1)\n",
    "        \n",
    "            # Get features, label\n",
    "        X = df.drop('label', axis=1).values\n",
    "        y = df['label'].values\n",
    "        \n",
    "            # Get train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "            \n",
    "            # Scale the data\n",
    "        sc = StandardScaler()  \n",
    "        X_train = sc.fit_transform(X_train)  \n",
    "        X_test = sc.transform(X_test)  \n",
    "            \n",
    "            # Test LDA\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "        y_pred = lda.predict(X_test)\n",
    "        lda_accuracy[snrs[i]].append(accuracy_score(y_test, y_pred))\n",
    "            \n",
    "            # Feature importance the same way Schmidt did\n",
    "        #clf = DecisionTreeClassifier()\n",
    "        #clf.fit(X, y)\n",
    "\n",
    "        #feat_importance = clf.tree_.compute_feature_importances(normalize=False)\n",
    "\n",
    "        #out = io.StringIO()\n",
    "        #out = export_graphviz(clf, out_file='no_noise.dot')\n",
    "        #ft_imp_schmidt[snrs[i]].append(sorted(list(zip(feat_importance, features)), reverse=True)[:5])\n",
    "            \n",
    "            # Test RF\n",
    "        classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "        classifier.fit(X_train, y_train)  \n",
    "        y_pred = classifier.predict(X_test)  \n",
    "        rf_accuracy[snrs[i]].append(accuracy_score(y_test, y_pred))\n",
    "        importances = classifier.feature_importances_\n",
    "        forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)[:10]\n",
    "        ft_imp_matrix[snrs[i]].append(forest_importances)\n",
    "\n",
    "        '''\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        forest_importances[:20].plot.barh(ax=ax)\n",
    "        ax.set_title(\"Feature importances using MDI with SNR: \" + str(snrs[i]))\n",
    "        ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "        fig.tight_layout()\n",
    "        plt.show();\n",
    "          '''  \n",
    "            # Test SVM\n",
    "        clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_out = clf.predict(X_test)    \n",
    "        svm_accuracy[snrs[i]].append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82befef3",
   "metadata": {},
   "source": [
    "## Plot SNR vs. Accuracy - Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_accuracy = min(min(min(lda_accuracy.values())), min(min(rf_accuracy.values())), min(min(svm_accuracy.values()))) - 0.01\n",
    "max_accuracy = max(lda_baseline_acc, rf_baseline_acc, svm_baseline_acc) + 0.01\n",
    "\n",
    "q25_lda_accuracy = {key: np.percentile(val, 25) for key, val in lda_accuracy.items()}\n",
    "q75_lda_accuracy = {key: np.percentile(val, 75) for key, val in lda_accuracy.items()}\n",
    "\n",
    "q25_rf_accuracy = {key: np.percentile(val, 25) for key, val in rf_accuracy.items()}\n",
    "q75_rf_accuracy = {key: np.percentile(val, 75) for key, val in rf_accuracy.items()}\n",
    "\n",
    "q25_svm_accuracy = {key: np.percentile(val, 25) for key, val in svm_accuracy.items()}\n",
    "q75_svm_accuracy = {key: np.percentile(val, 75) for key, val in svm_accuracy.items()}\n",
    "\n",
    "mean_lda_accuracy = {key: np.mean(val) for key, val in lda_accuracy.items()}\n",
    "mean_rf_accuracy = {key: np.mean(val) for key, val in rf_accuracy.items()}\n",
    "mean_svm_accuracy = {key: np.mean(val) for key, val in svm_accuracy.items()}\n",
    "\n",
    "# Plot accuracy comparison with 25th and 75th percentiles\n",
    "# Create subplots for each classifier\n",
    "#fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(8, 15))\n",
    "sns.set_style('dark')\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(25,7))\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "# LDA accuracy plot\n",
    "ax1.plot(snrs, mean_lda_accuracy.values(), '-o', label='Mean (n = 6)')\n",
    "ax1.plot(snrs, q25_lda_accuracy.values(), '--o', label='25th percentile', color='g')\n",
    "ax1.plot(snrs, q75_lda_accuracy.values(), '--o', label='75th percentile', color='r')\n",
    "ax1.plot(snrs, [lda_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "ax1.set_xlabel('Signal-to-Noise Ratio', fontsize=15)\n",
    "ax1.set_ylabel('Accuracy', fontsize=15)\n",
    "ax1.set_title('Linear Discriminant Analysis', fontsize=20)\n",
    "ax1.set_ylim([min_accuracy, max_accuracy]);\n",
    "ax1.legend()\n",
    "\n",
    "# RF accuracy plot\n",
    "ax2.plot(snrs, mean_rf_accuracy.values(), '-o', label='Mean (n = 6)')\n",
    "ax2.plot(snrs, q25_rf_accuracy.values(), '--o', label='25th percentile', color='g')\n",
    "ax2.plot(snrs, q75_rf_accuracy.values(), '--o', label='75th percentile', color='r')\n",
    "ax2.plot(snrs, [rf_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "ax2.set_xlabel('Signal-to-Noise Ratio', fontsize=15)\n",
    "ax2.set_ylabel('Accuracy', fontsize=15)\n",
    "ax2.set_title('Random Forest', fontsize=20)\n",
    "ax2.set_ylim([min_accuracy, max_accuracy]);\n",
    "ax2.legend()\n",
    "\n",
    "# SVM accuracy plot\n",
    "ax3.plot(snrs, mean_svm_accuracy.values(), '-o', label='Mean (n = 6)')\n",
    "ax3.plot(snrs, q25_svm_accuracy.values(), '--o', label='25th percentile', color='g')\n",
    "ax3.plot(snrs, q75_svm_accuracy.values(), '--o', label='75th percentile', color='r')\n",
    "ax3.plot(snrs, [svm_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "ax3.set_xlabel('Signal-to-Noise Ratio', fontsize=15)\n",
    "ax3.set_ylabel('Accuracy', fontsize=15)\n",
    "ax3.set_title('Support Vector Machine', fontsize=20)\n",
    "ax3.set_ylim([min_accuracy, max_accuracy]);\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"Mean accuracy for LDA:\", mean_lda_accuracy)\n",
    "print(\"Mean accuracy for RF:\", mean_rf_accuracy)\n",
    "print(\"Mean accuracy for SVM:\", mean_svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "for ft in range(0, len(ft_imp_matrix)):\n",
    "    ft_imp_matrix[ft] = ft_imp_matrix[ft].to_frame().rename(columns={0:'importance'})\n",
    "    ft_imp_matrix[ft]['rank'] = ft_imp_matrix[ft]['importance'].rank(ascending=False)\n",
    "    ft_imp_matrix[ft].drop(columns={'importance'}, inplace=True)\n",
    "    ft_imp_matrix[ft] = ft_imp_matrix[ft].iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ebc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df = pd.DataFrame(ft_imp_matrix, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d157b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.dropna(axis=1, how='all').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_df.columns.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d742139d",
   "metadata": {},
   "source": [
    "## Plot SNR vs. Accuracy - OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sns.set_style('dark')\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,7))\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "min_accuracy = min(min(lda_accuracy), min(rf_accuracy), min(svm_accuracy)) - 0.01\n",
    "max_accuracy = max(lda_baseline_acc, rf_baseline_acc, svm_baseline_acc) + 0.01\n",
    "\n",
    "axs[0].plot(snrs, lda_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[0].plot(snrs, [lda_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[0].set_title('Linear Discriminant Analysis', fontsize=20);\n",
    "axs[0].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[0].set_ylabel('Classification Accuracy', fontsize=15);\n",
    "axs[0].legend();\n",
    "axs[0].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[1].plot(snrs, rf_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[1].plot(snrs, [rf_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[1].set_title('Random Forest', fontsize=20);\n",
    "axs[1].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[1].legend();\n",
    "axs[1].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "axs[2].plot(snrs, svm_accuracy, label='Accuracy (with noise)', marker='o');\n",
    "axs[2].plot(snrs, [svm_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "axs[2].set_title('Support Vector Machine', fontsize=20);\n",
    "axs[2].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "axs[2].legend();\n",
    "axs[2].set_ylim([min_accuracy, max_accuracy]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c643cab",
   "metadata": {},
   "source": [
    "## Leave Feature Out Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e1066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_to_leave_out = ['EDA', 'Resp', 'ECG']\n",
    "lda_accuracies = []\n",
    "rf_accuracies = []\n",
    "svm_accuracies = []\n",
    "ft_imp_matrices = []\n",
    "ft_imp_schmidts = []\n",
    "for ftre in features_to_leave_out:\n",
    "    snrs = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "    lda_accuracy = []\n",
    "    rf_accuracy = []\n",
    "    svm_accuracy = []\n",
    "    ft_imp_matrix = []\n",
    "    ft_imp_schmidt = []\n",
    "    # For each signal to noise ratio\n",
    "    for i in range(len(snrs)):\n",
    "        # Get data\n",
    "        df = pd.read_csv('data/noise_snr_'+str(snrs[i])+'.csv', index_col=0)\n",
    "        \n",
    "        # Get columns to remove\n",
    "        cols = ['Resp_C_rate']\n",
    "        for cl in df.columns:\n",
    "            if ftre in cl: cols.append(cl)\n",
    "        \n",
    "        # Since Resp_C_rate is null for the first four, simply get rid of it\n",
    "        df = df.loc[:, ~df.columns.isin(cols)]\n",
    "\n",
    "        # Get features, label\n",
    "        features = df.drop('label', axis=1).columns\n",
    "        X = df.drop('label', axis=1).values\n",
    "        y = df['label'].values\n",
    "\n",
    "        # Get train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "\n",
    "        # Scale the data\n",
    "        sc = StandardScaler()  \n",
    "        X_train = sc.fit_transform(X_train)  \n",
    "        X_test = sc.transform(X_test)  \n",
    "\n",
    "        # Test LDA\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "        y_pred = lda.predict(X_test)\n",
    "        lda_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # Feature importance the same way Schmidt did\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(X, y)\n",
    "\n",
    "        feat_importance = dt.feature_importances_\n",
    "        plt.figure(figsize=(40,20));\n",
    "        tree.plot_tree(dt,fontsize=10, feature_names=features);\n",
    "        plt.show();\n",
    "\n",
    "        # Test RF\n",
    "        classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "        classifier.fit(X_train, y_train)  \n",
    "        y_pred = classifier.predict(X_test)  \n",
    "        rf_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        importances = classifier.feature_importances_\n",
    "        forest_importances = pd.Series(importances, index=features).sort_values(ascending=False)[:10]\n",
    "        ft_imp_matrix.append(forest_importances)\n",
    "\n",
    "\n",
    "        # Test SVM\n",
    "        clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_out = clf.predict(X_test)    \n",
    "        svm_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Append to outer lists\n",
    "    lda_accuracies.append(lda_accuracy)\n",
    "    rf_accuracies.append(rf_accuracy)\n",
    "    svm_accuracies.append(svm_accuracy)\n",
    "    ft_imp_matrices.append(ft_imp_matrix)\n",
    "    ft_imp_schmidts.append(ft_imp_schmidt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a4076",
   "metadata": {},
   "source": [
    "### Plot SNR vs. Accuracy for Leave Feature Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185ee1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(features_to_leave_out)):\n",
    "    sns.set_style('dark')\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,7))\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    \n",
    "    print('Models trained without: ' + features_to_leave_out[idx])\n",
    "    \n",
    "    min_accuracy = min(min(lda_accuracies[idx]), min(rf_accuracies[idx]), min(svm_accuracies[idx])) - 0.01\n",
    "    max_accuracy = max(lda_baseline_acc, rf_baseline_acc, svm_baseline_acc) + 0.01\n",
    "\n",
    "    axs[0].plot(snrs, lda_accuracies[idx], label='Accuracy (with noise)', marker='o');\n",
    "    axs[0].plot(snrs, [lda_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "    axs[0].set_title('Linear Discriminant Analysis', fontsize=20);\n",
    "    axs[0].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "    axs[0].set_ylabel('Classification Accuracy', fontsize=15);\n",
    "    axs[0].legend();\n",
    "    axs[0].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "    axs[1].plot(snrs, rf_accuracies[idx], label='Accuracy (with noise)', marker='o');\n",
    "    axs[1].plot(snrs, [rf_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "    axs[1].set_title('Random Forest', fontsize=20);\n",
    "    axs[1].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "    axs[1].legend();\n",
    "    axs[1].set_ylim([min_accuracy, max_accuracy]);\n",
    "\n",
    "    axs[2].plot(snrs, svm_accuracies[idx], label='Accuracy (with noise)', marker='o');\n",
    "    axs[2].plot(snrs, [svm_baseline_acc]*len(snrs), linestyle='dashed', label='Accuracy (without noise)')\n",
    "    axs[2].set_title('Support Vector Machine', fontsize=20);\n",
    "    axs[2].set_xlabel('Signal to Noise Ratio', fontsize=15);\n",
    "    axs[2].legend();\n",
    "    axs[2].set_ylim([min_accuracy, max_accuracy]);\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1055px",
    "left": "216px",
    "top": "111.12px",
    "width": "343.299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
