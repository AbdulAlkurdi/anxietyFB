{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8af7f1",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20604081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Link: https://github.com/WJMatthew/WESAD/blob/master/data_wrangling.py\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import heartpy as hp\n",
    "from heartpy.datautils import *\n",
    "from heartpy.peakdetection import *\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from scipy import stats\n",
    "import cvxEDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed758cf9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24314e2f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23cd49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700, 'ECG': 700}\n",
    "# Window size\n",
    "WINDOW_IN_SECONDS = 30\n",
    "\n",
    "# Labels\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "# Int to label mappings\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "# Feature names\n",
    "feat_names = None\n",
    "# Where to save the data\n",
    "savePath = 'data'\n",
    "# Where to get the data\n",
    "subject_feature_path = '/subject_feats'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8087bc7",
   "metadata": {},
   "source": [
    "## Class to Store Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bec7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the data for each subject\n",
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.wrist_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f31671",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0e85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvxEDA\n",
    "def eda_stats(y):\n",
    "    Fs = fs_dict['EDA']\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return [r, p, t, l, d, e, obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7962d",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533e4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net_accel(data):\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a6269",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f0149",
   "metadata": {},
   "source": [
    "### Lowpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f48211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/load_files.py\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183600cb",
   "metadata": {},
   "source": [
    "### Lowpass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = scisig.lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2654d",
   "metadata": {},
   "source": [
    "### Slope Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04115363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(series):\n",
    "    linreg = scipy.stats.linregress(np.arange(len(series)), series )\n",
    "    slope = linreg[0]\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ac3d5",
   "metadata": {},
   "source": [
    "### Mean/Std/Min/Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a883ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b8431",
   "metadata": {},
   "source": [
    "### Peak Frequency (Periodogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3ab744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c7863",
   "metadata": {},
   "source": [
    "### FIR Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dd1882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43894c7",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a973f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating window statistics for ECGs\n",
    "def get_window_stats_ecg(data, label=-1):\n",
    "    # Drop nulls\n",
    "    data2 = data['ECG'].dropna().reset_index(drop=True)\n",
    "    # Find R peaks\n",
    "    rr_list = find_peaks(data2, height=0.5, distance=25)[0]\n",
    "    # Find difference between successive R peaks\n",
    "    rr_diff = np.diff(rr_list)\n",
    "    # Find squared difference between successive R peaks\n",
    "    rr_sqdiff = np.diff(rr_list)**2\n",
    "    \n",
    "    # Build measures dictionary\n",
    "    measures = dict()\n",
    "    # Compute beats per minute\n",
    "    measures['bpm'] = (60*fs_dict['ECG'])/np.mean(rr_diff)\n",
    "    # Compute interbeat interval\n",
    "    measures['ibi'] = np.mean(fs_dict['ECG']/rr_diff)\n",
    "    measures['sdnn'] = np.std(fs_dict['ECG']/rr_list)\n",
    "    measures['sdsd'] = np.std(fs_dict['ECG']/rr_diff)\n",
    "    measures['rmssd'] = np.sqrt(np.mean(fs_dict['ECG']**2/rr_sqdiff))\n",
    "    nn20 = rr_diff[np.where(rr_diff > 20.0)]\n",
    "    nn50 = rr_diff[np.where(rr_diff > 50.0)]\n",
    "    try:\n",
    "        measures['pnn20'] = float(len(nn20)) / float(len(rr_diff))\n",
    "    except:\n",
    "        measures['pnn20'] = np.nan\n",
    "    try:\n",
    "        measures['pnn50'] = float(len(nn50)) / float(len(rr_diff))\n",
    "    except:\n",
    "        measures['pnn50'] = np.nan\n",
    "        \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b56005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_chest(ch_data_dict, labels, norm_type=None):\n",
    "    ecg_df = pd.DataFrame(ch_data_dict['ECG'], columns=['ECG'])\n",
    "    \n",
    "    # Adding index for combination due to different sampling frequencies\n",
    "    ecg_df.index = [(1 / fs_dict['ECG']) * i for i in range(len(ecg_df))]\n",
    "    \n",
    "    # Change index to datetime\n",
    "    ecg_df.index = pd.to_datetime(ecg_df.index, unit='s')\n",
    "    \n",
    "    return ecg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fbcde",
   "metadata": {},
   "source": [
    "## Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7064a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes features for wrist\n",
    "def compute_features(e4_data_dict, ch_data_dict, labels, norm_type=None):\n",
    "    # Dataframes for each sensor type\n",
    "    eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "    bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "    acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "    label_df = pd.DataFrame(labels, columns=['label'])\n",
    "    resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
    "\n",
    "    # Filter EDA\n",
    "    eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "\n",
    "    # Filter ACM\n",
    "    for _ in acc_df.columns:\n",
    "        acc_df[_] = filterSignalFIR(acc_df.values)\n",
    "\n",
    "    # Adding indices for combination due to differing sampling frequencies\n",
    "    eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "    bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "    acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "    temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "    label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "    resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]\n",
    "\n",
    "    # Change indices to datetime\n",
    "    eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "    bvp_df.index = pd.to_datetime(bvp_df.index, unit='s')\n",
    "    temp_df.index = pd.to_datetime(temp_df.index, unit='s')\n",
    "    acc_df.index = pd.to_datetime(acc_df.index, unit='s')\n",
    "    label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "    resp_df.index = pd.to_datetime(resp_df.index, unit='s')\n",
    "\n",
    "    # New EDA features\n",
    "    r, p, t, l, d, e, obj = eda_stats(eda_df['EDA'])\n",
    "    eda_df['EDA_phasic'] = r\n",
    "    eda_df['EDA_smna'] = p\n",
    "    eda_df['EDA_tonic'] = t\n",
    "\n",
    "    # Getting ECG features\n",
    "    ecg_df = compute_features_chest(ch_data_dict, labels, norm_type=None)\n",
    "        \n",
    "    # Combined dataframe - not used yet\n",
    "    df = eda_df.join(bvp_df, how='outer')\n",
    "    df = df.join(temp_df, how='outer')\n",
    "    df = df.join(acc_df, how='outer')\n",
    "    df = df.join(resp_df, how='outer')\n",
    "    df = df.join(label_df, how='outer')\n",
    "    df = df.join(ecg_df, how='outer')\n",
    "    df['label'] = df['label'].fillna(method='bfill')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if norm_type == 'std':\n",
    "        # std norm\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type == 'minmax':\n",
    "        # minmax norm\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # Groupby\n",
    "    grouped = df.groupby('label')\n",
    "    baseline = grouped.get_group(1)\n",
    "    stress = grouped.get_group(2)\n",
    "    amusement = grouped.get_group(3)\n",
    "    return grouped, baseline, stress, amusement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0585d3",
   "metadata": {},
   "source": [
    "## Get Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "628ed513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data, n_windows, label):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    # Using label freq (700 Hz) as our reference frequency due to it being the largest\n",
    "    # and thus encompassing the lesser ones in its resolution.\n",
    "    window_len = fs_dict['label'] * WINDOW_IN_SECONDS\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Get window of data\n",
    "        w = data[window_len * i: window_len * (i + 1)]\n",
    "\n",
    "        # Add/Calc rms acc\n",
    "        w = pd.concat([w, get_net_accel(w)])\n",
    "        cols = list(w.columns)\n",
    "        cols[0] = 'net_acc'\n",
    "        w.columns = cols\n",
    "        \n",
    "        # Calculate stats for window\n",
    "        wstats = get_window_stats(data=w, label=label)\n",
    "        \n",
    "        # Calculate stats for window (ECG)\n",
    "        wstats_ecg = get_window_stats_ecg(data=w, label=label)\n",
    "        # Merge features\n",
    "        wstats.update(wstats_ecg)\n",
    "        \n",
    "        # Seperating sample and label\n",
    "        x = pd.DataFrame(wstats).drop('label', axis=0)\n",
    "        y = x['label'][0]\n",
    "        x.drop('label', axis=1, inplace=True)\n",
    "\n",
    "        if feat_names is None:\n",
    "            feat_names = []\n",
    "            for row in x.index:\n",
    "                for col in x.columns:\n",
    "                    feat_names.append('_'.join([str(row), str(col)]))\n",
    "        # sample df\n",
    "        wdf = pd.DataFrame(x.values.flatten()).T\n",
    "        wdf.columns = feat_names\n",
    "        wdf = pd.concat([wdf, pd.DataFrame({'label': y}, index=[0])], axis=1)\n",
    "        \n",
    "        # More feats\n",
    "        wdf['BVP_peak_freq'] = get_peak_freq(w['BVP'].dropna())\n",
    "        # Add other peak frequencies here\n",
    "        wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
    "        samples.append(wdf)\n",
    "\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444dd7bb",
   "metadata": {},
   "source": [
    "## Make Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6821e412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path='data/WESAD', subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    e4_data_dict = subject.get_wrist_data()\n",
    "    \n",
    "    # Chest data\n",
    "    ch_data_dict = subject.get_chest_data()\n",
    "\n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped, baseline, stress, amusement = compute_features(e4_data_dict, ch_data_dict, subject.labels, norm_type)\n",
    "\n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "    #\n",
    "    baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
    "    for col in baseline_samples.columns:\n",
    "        print(col)\n",
    "    # Downsampling\n",
    "    # baseline_samples = baseline_samples[::2]\n",
    "    stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
    "    amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
    "\n",
    "    all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples])\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "    # Save file as csv\n",
    "    all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats_4.csv')\n",
    "\n",
    "    subject = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a131f",
   "metadata": {},
   "source": [
    "## Combine Patient Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53da46c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for S2...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.2093e+04 -1.2039e+04  5e+04  2e+02  2e-01\n",
      " 1: -1.2093e+04 -2.0218e+04  1e+04  4e+01  5e-02\n",
      " 2: -1.2100e+04 -1.5246e+04  3e+03  1e+01  1e-02\n",
      " 3: -1.2100e+04 -1.3551e+04  1e+03  4e+00  5e-03\n",
      " 4: -1.2097e+04 -1.2876e+04  8e+02  2e+00  2e-03\n",
      " 5: -1.2092e+04 -1.2599e+04  5e+02  8e-01  1e-03\n",
      " 6: -1.2089e+04 -1.2320e+04  2e+02  3e-01  4e-04\n",
      " 7: -1.2102e+04 -1.2158e+04  6e+01  2e-02  3e-05\n",
      " 8: -1.2127e+04 -1.2148e+04  2e+01  6e-03  8e-06\n",
      " 9: -1.2137e+04 -1.2146e+04  9e+00  2e-03  2e-06\n",
      "10: -1.2142e+04 -1.2146e+04  4e+00  5e-04  6e-07\n",
      "11: -1.2144e+04 -1.2145e+04  1e+00  1e-04  2e-07\n",
      "12: -1.2145e+04 -1.2145e+04  5e-01  2e-05  3e-08\n",
      "13: -1.2145e+04 -1.2145e+04  2e-01  5e-06  6e-09\n",
      "14: -1.2145e+04 -1.2145e+04  7e-02  1e-06  2e-09\n",
      "15: -1.2145e+04 -1.2145e+04  2e-02  3e-07  3e-10\n",
      "16: -1.2145e+04 -1.2145e+04  5e-03  3e-08  4e-11\n",
      "17: -1.2145e+04 -1.2145e+04  2e-03  6e-09  8e-12\n",
      "18: -1.2145e+04 -1.2145e+04  4e-04  7e-10  9e-13\n",
      "19: -1.2145e+04 -1.2145e+04  1e-04  2e-10  2e-13\n",
      "20: -1.2145e+04 -1.2145e+04  3e-05  2e-11  3e-14\n",
      "21: -1.2145e+04 -1.2145e+04  5e-06  4e-12  2e-14\n",
      "Optimal solution found.\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n",
      "net_acc\n",
      "EDA_phasic\n",
      "EDA_smna\n",
      "EDA_tonic\n",
      "BVP\n",
      "TEMP\n",
      "ACC_x\n",
      "ACC_y\n",
      "ACC_z\n",
      "Resp\n",
      "label\n",
      "ECG\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient \u001b[38;5;129;01min\u001b[39;00m subject_ids:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing data for S\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mmake_patient_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m combine_files(subject_ids)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mmake_patient_data\u001b[0;34m(subject_id)\u001b[0m\n\u001b[1;32m     23\u001b[0m n_amusement_wdws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(amusement) \u001b[38;5;241m/\u001b[39m (fs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m WINDOW_IN_SECONDS))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m baseline_samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_baseline_wdws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m baseline_samples\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(col)\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mget_samples\u001b[0;34m(data, n_windows, label)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(column)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate stats for window\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m wstats \u001b[38;5;241m=\u001b[39m \u001b[43mget_window_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate stats for window (ECG)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m wstats_ecg \u001b[38;5;241m=\u001b[39m get_window_stats_ecg(data\u001b[38;5;241m=\u001b[39mw, label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mget_window_stats\u001b[0;34m(data, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_window_stats\u001b[39m(data, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     mean_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     std_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(data)\n\u001b[1;32m      4\u001b[0m     min_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mamin(data)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3472\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3470\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3472\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   3475\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:11117\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11099\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m  11100\u001b[0m     _num_doc,\n\u001b[1;32m  11101\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11116\u001b[0m ):\n\u001b[0;32m> 11117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:10687\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  10680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10681\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  10686\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 10687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  10689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:10620\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10610\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m  10612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m  10613\u001b[0m     \u001b[38;5;66;03m# user must have explicitly passed axis=None\u001b[39;00m\n\u001b[1;32m  10614\u001b[0m     \u001b[38;5;66;03m# GH#21597\u001b[39;00m\n\u001b[1;32m  10615\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  10616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a future version, DataFrame.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(axis=None) will return a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  10617\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscalar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m over the entire DataFrame. To retain the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  10618\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehavior, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(axis=0)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or just \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m  10619\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m> 10620\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[43mfind_stack_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m  10621\u001b[0m     )\n\u001b[1;32m  10622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m  10623\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_stack_level\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    (tests notwithstanding).\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     pkg_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pd\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/inspect.py:1678\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/inspect.py:1655\u001b[0m, in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1653\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[0;32m-> 1655\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1656\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[1;32m   1657\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/inspect.py:1629\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1627\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1629\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/inspect.py:952\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 952\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[1;32m    954\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/inspect.py:869\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# Update the filename to module name cache and check yet again\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# Copy sys.modules in order to cope with changes while iterating\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modname, module \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ismodule(module) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__file__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    870\u001b[0m         f \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m _filesbymodname\u001b[38;5;241m.\u001b[39mget(modname, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    872\u001b[0m             \u001b[38;5;66;03m# Have already mapped this module, so skip it\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "    df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{savePath}/may14_feats4.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "    for patient in subject_ids:\n",
    "        print(f'Processing data for S{patient}...')\n",
    "        make_patient_data(patient)\n",
    "\n",
    "    combine_files(subject_ids)\n",
    "    print('Processing complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cd5aa",
   "metadata": {},
   "source": [
    "## Add Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b79b4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rparser:\n",
    "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
    "    VALUE_EXTRACT_KEYS = {\n",
    "        \"age\": {\n",
    "            'search_key': 'Age',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"height\": {\n",
    "            'search_key': 'Height',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"weight\": {\n",
    "            'search_key': 'Weight',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"gender\": {\n",
    "            'search_key': 'Gender',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"dominant_hand\": {\n",
    "            'search_key': 'Dominant',\n",
    "            'delimiter': ':'\n",
    "        },\n",
    "        \"coffee_today\": {\n",
    "            'search_key': 'Did you drink coffee today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"coffee_last_hour\": {\n",
    "            'search_key': 'Did you drink coffee within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"sport_today\": {\n",
    "            'search_key': 'Did you do any sports today',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoker\": {\n",
    "            'search_key': 'Are you a smoker',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"smoke_last_hour\": {\n",
    "            'search_key': 'Did you smoke within the last hour',\n",
    "            'delimiter': '? '\n",
    "        },\n",
    "        \"feel_ill_today\": {\n",
    "            'search_key': 'Do you feel ill today',\n",
    "            'delimiter': '? '\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DATA_PATH = 'data/WESAD/'\n",
    "    parse_file_suffix = '_readme.txt'\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
    "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
    "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
    "        \n",
    "        # Check if parsed readme file is available ( should be as it is saved above )\n",
    "        if not os.path.isfile('data/readmes.csv'):\n",
    "            print('Parsing Readme files')\n",
    "            self.parse_all_readmes()\n",
    "        else:\n",
    "            print('Files already parsed.')\n",
    "            \n",
    "        self.merge_with_feature_data()\n",
    "        \n",
    "        \n",
    "    def parse_readme(self, subject_id):\n",
    "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
    "\n",
    "            x = f.read().split('\\n')\n",
    "\n",
    "        readme_dict = {}\n",
    "\n",
    "        for item in x:\n",
    "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
    "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
    "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
    "                if item.startswith(search_key):\n",
    "                    d, v = item.split(delimiter)\n",
    "                    readme_dict.update({key: v})\n",
    "                    break\n",
    "        return readme_dict\n",
    "\n",
    "\n",
    "    def parse_all_readmes(self):\n",
    "        \n",
    "        dframes = []\n",
    "\n",
    "        for subject_id, path in self.readme_locations.items():\n",
    "            readme_dict = self.parse_readme(subject_id)\n",
    "            df = pd.DataFrame(readme_dict, index=[subject_id])\n",
    "            dframes.append(df)\n",
    "\n",
    "        df = pd.concat(dframes)\n",
    "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
    "\n",
    "        \n",
    "    def merge_with_feature_data(self):\n",
    "        # Confirm feature files are available\n",
    "        if os.path.isfile('data/may14_feats4.csv'):\n",
    "            feat_df = pd.read_csv('data/may14_feats4.csv', index_col=0)\n",
    "            print(feat_df.info())\n",
    "        else:\n",
    "            print('No feature data available. Exiting...')\n",
    "            return\n",
    "           \n",
    "        # Combine data and save\n",
    "        df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
    "\n",
    "        dummy_df = pd.get_dummies(df)\n",
    "        \n",
    "        dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
    "\n",
    "        dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
    "                           'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
    "                           'feel_ill_today_YES', 'subject']]\n",
    "\n",
    "        merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
    "\n",
    "        merged_df.to_csv('data/m14_merged.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0b5c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Readme files\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1178 entries, 0 to 1177\n",
      "Columns: 136 entries, net_acc_mean to label\n",
      "dtypes: float64(134), int64(2)\n",
      "memory usage: 1.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rp = rparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12d804",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3385104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/m14_merged.csv', index_col=0)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4de70d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG_bpm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ECG_bpm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.536791</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ECG_bpm     label\n",
       "ECG_bpm  1.000000  0.536791\n",
       "label    0.536791  1.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['ECG_bpm', 'label']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d019ea6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_acc_mean\n",
      "net_acc_std\n",
      "net_acc_min\n",
      "net_acc_max\n",
      "net_acc_bpm\n",
      "net_acc_ibi\n",
      "net_acc_sdnn\n",
      "net_acc_sdsd\n",
      "net_acc_rmssd\n",
      "net_acc_pnn20\n",
      "net_acc_pnn50\n",
      "ACC_x_mean\n",
      "ACC_x_std\n",
      "ACC_x_min\n",
      "ACC_x_max\n",
      "ACC_x_bpm\n",
      "ACC_x_ibi\n",
      "ACC_x_sdnn\n",
      "ACC_x_sdsd\n",
      "ACC_x_rmssd\n",
      "ACC_x_pnn20\n",
      "ACC_x_pnn50\n",
      "ACC_y_mean\n",
      "ACC_y_std\n",
      "ACC_y_min\n",
      "ACC_y_max\n",
      "ACC_y_bpm\n",
      "ACC_y_ibi\n",
      "ACC_y_sdnn\n",
      "ACC_y_sdsd\n",
      "ACC_y_rmssd\n",
      "ACC_y_pnn20\n",
      "ACC_y_pnn50\n",
      "ACC_z_mean\n",
      "ACC_z_std\n",
      "ACC_z_min\n",
      "ACC_z_max\n",
      "ACC_z_bpm\n",
      "ACC_z_ibi\n",
      "ACC_z_sdnn\n",
      "ACC_z_sdsd\n",
      "ACC_z_rmssd\n",
      "ACC_z_pnn20\n",
      "ACC_z_pnn50\n",
      "BVP_mean\n",
      "BVP_std\n",
      "BVP_min\n",
      "BVP_max\n",
      "BVP_bpm\n",
      "BVP_ibi\n",
      "BVP_sdnn\n",
      "BVP_sdsd\n",
      "BVP_rmssd\n",
      "BVP_pnn20\n",
      "BVP_pnn50\n",
      "EDA_mean\n",
      "EDA_std\n",
      "EDA_min\n",
      "EDA_max\n",
      "EDA_bpm\n",
      "EDA_ibi\n",
      "EDA_sdnn\n",
      "EDA_sdsd\n",
      "EDA_rmssd\n",
      "EDA_pnn20\n",
      "EDA_pnn50\n",
      "EDA_phasic_mean\n",
      "EDA_phasic_std\n",
      "EDA_phasic_min\n",
      "EDA_phasic_max\n",
      "EDA_phasic_bpm\n",
      "EDA_phasic_ibi\n",
      "EDA_phasic_sdnn\n",
      "EDA_phasic_sdsd\n",
      "EDA_phasic_rmssd\n",
      "EDA_phasic_pnn20\n",
      "EDA_phasic_pnn50\n",
      "EDA_smna_mean\n",
      "EDA_smna_std\n",
      "EDA_smna_min\n",
      "EDA_smna_max\n",
      "EDA_smna_bpm\n",
      "EDA_smna_ibi\n",
      "EDA_smna_sdnn\n",
      "EDA_smna_sdsd\n",
      "EDA_smna_rmssd\n",
      "EDA_smna_pnn20\n",
      "EDA_smna_pnn50\n",
      "EDA_tonic_mean\n",
      "EDA_tonic_std\n",
      "EDA_tonic_min\n",
      "EDA_tonic_max\n",
      "EDA_tonic_bpm\n",
      "EDA_tonic_ibi\n",
      "EDA_tonic_sdnn\n",
      "EDA_tonic_sdsd\n",
      "EDA_tonic_rmssd\n",
      "EDA_tonic_pnn20\n",
      "EDA_tonic_pnn50\n",
      "TEMP_mean\n",
      "TEMP_std\n",
      "TEMP_min\n",
      "TEMP_max\n",
      "TEMP_bpm\n",
      "TEMP_ibi\n",
      "TEMP_sdnn\n",
      "TEMP_sdsd\n",
      "TEMP_rmssd\n",
      "TEMP_pnn20\n",
      "TEMP_pnn50\n",
      "ECG_mean\n",
      "ECG_std\n",
      "ECG_min\n",
      "ECG_max\n",
      "ECG_bpm\n",
      "ECG_ibi\n",
      "ECG_sdnn\n",
      "ECG_sdsd\n",
      "ECG_rmssd\n",
      "ECG_pnn20\n",
      "ECG_pnn50\n",
      "0_mean\n",
      "0_std\n",
      "0_min\n",
      "0_max\n",
      "0_bpm\n",
      "0_ibi\n",
      "0_sdnn\n",
      "0_sdsd\n",
      "0_rmssd\n",
      "0_pnn20\n",
      "0_pnn50\n",
      "BVP_peak_freq\n",
      "TEMP_slope\n",
      "subject\n",
      "label\n",
      "age\n",
      "height\n",
      "weight\n",
      "gender_ female\n",
      "gender_ male\n",
      "coffee_today_YES\n",
      "sport_today_YES\n",
      "smoker_NO\n",
      "smoker_YES\n",
      "feel_ill_today_YES\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "308a8bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_mean</th>\n",
       "      <th>0_std</th>\n",
       "      <th>0_min</th>\n",
       "      <th>0_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029937</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021986</td>\n",
       "      <td>0.015845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034449</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.040595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028870</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.038531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.028898</td>\n",
       "      <td>0.038531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>0.032774</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.028898</td>\n",
       "      <td>0.041283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.031650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>0.027860</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.026146</td>\n",
       "      <td>0.028898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>0.027939</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.025458</td>\n",
       "      <td>0.035779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1178 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_mean     0_std     0_min     0_max\n",
       "0     0.029937  0.009942  0.000000  0.087383\n",
       "1     0.021986  0.015845  0.000000  0.071558\n",
       "2     0.020839  0.011034  0.002752  0.054356\n",
       "3     0.034449  0.003185  0.013761  0.040595\n",
       "4     0.028870  0.004379  0.013761  0.038531\n",
       "...        ...       ...       ...       ...\n",
       "1173  0.033900  0.000894  0.028898  0.038531\n",
       "1174  0.032774  0.002033  0.028898  0.041283\n",
       "1175  0.028112  0.001486  0.024770  0.031650\n",
       "1176  0.027860  0.000512  0.026146  0.028898\n",
       "1177  0.027939  0.000709  0.025458  0.035779\n",
       "\n",
       "[1178 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['0_mean', '0_std', '0_min', '0_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2ed895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max',\n",
    "           'BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max', \n",
    "            'EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max',\n",
    "            'EDA_phasic_mean', 'EDA_phasic_std', 'EDA_phasic_min', 'EDA_phasic_max',\n",
    "            'EDA_smna_mean', 'EDA_smna_std', 'EDA_smna_min', 'EDA_smna_max',\n",
    "            'EDA_tonic_mean', 'EDA_tonic_std', 'EDA_tonic_min', 'EDA_tonic_max',\n",
    "            'TEMP_mean', 'TEMP_std', 'TEMP_min', 'TEMP_max',\n",
    "           'ACC_x_mean', 'ACC_x_std', 'ACC_x_min', 'ACC_x_max',\n",
    "           'ACC_y_mean', 'ACC_y_std', 'ACC_y_min', 'ACC_y_max',\n",
    "           'ACC_z_mean', 'ACC_z_std', 'ACC_z_min', 'ACC_z_max',\n",
    "           'Resp_mean', 'Resp_std', 'Resp_min', 'Resp_max',\n",
    "           'ECG_bpm', 'ECG_ibi', 'ECG_sdnn', 'ECG_sdsd', 'ECG_rmssd',\n",
    "           'ECG_pnn20', 'ECG_pnn50', 'BVP_peak_freq', 'TEMP_slope',\n",
    "           'subject', 'age', 'height', 'weight', 'gender_ female',\n",
    "           'gender_ male', 'coffee_today_YES', 'sport_today_YES',\n",
    "           'smoker_NO', 'smoker_YES', 'feel_ill_today_YES', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01c2cd25",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Resp_mean', 'Resp_std', 'Resp_min', 'Resp_max'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Resp_mean', 'Resp_std', 'Resp_min', 'Resp_max'] not in index\""
     ]
    }
   ],
   "source": [
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963cd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(corr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a6898be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ee52",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9b03e",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "471cf325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "453f0a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 15  26   2]\n",
      " [  8 113   5]\n",
      " [  2   1  64]]\n",
      "Accuracy: 0.8135593220338984\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58bc21",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2410057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3eed821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14  28   1]\n",
      " [  1 124   1]\n",
      " [  0   9  58]]\n",
      "Accuracy: 0.8305084745762712\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af563",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0416733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.4186    0.5373        43\n",
      "           1     0.8345    0.9206    0.8755       126\n",
      "           2     0.8904    0.9701    0.9286        67\n",
      "\n",
      "    accuracy                         0.8432       236\n",
      "   macro avg     0.8250    0.7698    0.7805       236\n",
      "weighted avg     0.8350    0.8432    0.8289       236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear', C=1, random_state=0)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_out = clf.predict(X_test)\n",
    "lm_svc=(classification_report(y_test, y_out, digits=4))\n",
    "print(lm_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b3084",
   "metadata": {},
   "source": [
    "# Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737390",
   "metadata": {},
   "source": [
    "## Signal to Noise Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099bbe0",
   "metadata": {},
   "source": [
    "For a non-constant signal $S$ and noise $N$, the signal to noise ratio is defined as the following:\n",
    "$$ SNR = \\frac{\\mathbb{E}[S^2]}{\\mathbb{E}[N^2]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70b232",
   "metadata": {},
   "source": [
    "The expected value $\\mathbb{E}[X]$ of any continuous random variable $X$ is $\\int_{-\\infty}^{\\infty} x p(x) dx $, where $p(x)$ is its associated probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d01d",
   "metadata": {},
   "source": [
    "For homoskedastic noise, we can use closed form expressions to compute $E[N^2]$.\n",
    "\n",
    "- For Gaussian distributed noise $N$ ~ $n(\\mu, \\sigma)$, notice that $\\text{V}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2,$ so $\\mathbb{E}[N^2] = \\text{V}[N] + (\\mathbb{E}[N])^2 = \\sigma^2 + \\mu$. In our case $\\mu = 0$, so $\\mathbb{E}[N^2] = \\sigma^2$.\n",
    "\n",
    "- For uniformly distributed noise $N$ ~ $u(\\alpha, \\beta)$, by the same logic as above $\\mathbb{E}[N^2] = \\left(\\frac{\\alpha - \\beta}{2}\\right)^2$.\n",
    "\n",
    "- For frequency-domain noise $N$ of the form $A\\sin(2\\pi x \\frac{1}{f}) + y, \\mathbb{E}[N^2] \\approx y^2 + \\frac{A^2}{2}$. Note the $\\approx$ since we cannot guarantee that the signal will end precisely on the end of the sin wave.\n",
    "\n",
    "For heteroskedastic noise, because there is no closed form expression, we simply take `N.mean()` where $N$ is our noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "beccc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_noise_ratio(signal, noise_type, noise_dist, noise=None, sigma=None, alpha=None, beta=None, \n",
    "                          vertical_shift=None, amplitude=None):\n",
    "    \"\"\"\n",
    "    Computes the signal to noise ratio for a given signal and its corresponding noise.\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we are evaluating\n",
    "        noise_type (string): 'Heteroskedastic' or 'Homoskedastic'\n",
    "        noise_dist (string): 'Uniform', 'Gaussian', or 'Frequency' for now\n",
    "        noise (array or ndarray): Only passed in if we have heteroskedastic noise\n",
    "        sigma (float): Sigma parameter of the gaussian\n",
    "        alpha (float): Alpha parameter of the uniform\n",
    "        beta (float): Beta parameter of the uniform\n",
    "        vertical_shift (float): Vertical shift parameter of the frequency\n",
    "        amplitude (float): Amplitude parameter of the frequency\n",
    "        \n",
    "    :return\n",
    "        signal_to_noise_ratio (float): Signal to noise ratio... E[S^2]/E[N^2]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate E[S^2]\n",
    "    e_s2 = (signal**2).mean()\n",
    "    e_n2 = None\n",
    "    \n",
    "    if noise_type == 'Homoskedastic':\n",
    "        # Calculate E[N^2] for the pertinent case\n",
    "        if noise_dist == 'Uniform':\n",
    "            e_n2 = (0.5*(alpha - beta))**2\n",
    "        elif noise_dist == 'Gaussian':\n",
    "            e_n2 = sigma**2\n",
    "        elif noise_dist == 'Frequency':\n",
    "            e_n2 = vertical_shift**2 + (amplitude**2)/2\n",
    "    elif noise_type == 'Heteroskedastic':\n",
    "        e_n2 = (noise**2).mean()\n",
    "    \n",
    "    # Return the signal to noise ratio\n",
    "    return e_s2/e_n2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9eb5f",
   "metadata": {},
   "source": [
    "## Uniform Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3be96e",
   "metadata": {},
   "source": [
    "The uniform probability density function is of the following form:\n",
    "$$\n",
    "f(x) = \n",
    "\\left \\{\n",
    "    \\begin{array}{lr}\n",
    "        \\frac{1}{\\beta - \\alpha}, & \\text{if } \\alpha \\leq x \\leq \\beta \\\\\n",
    "        0, & \\text{otherwise }\n",
    "    \\end{array}\n",
    "\\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8aa479",
   "metadata": {},
   "source": [
    "### Estimating $\\alpha$ and $\\beta$ of the Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644f069",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential with Homoskedasticity Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f18e5",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with homoskedasticity approach constructs a Gaussian distribution such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_i - S_{i+n}|)$, where $max(|S_i - S_{i+n}|)$ denotes the maximum absolute difference of the signal between index $i$ and $i+n$ in the entire signal, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "In conclusion, we randomly sample from the following probability density function:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\alpha \\cdot max(|S_i - S_{i+n}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_i - S_{i+n}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "This noise exhbits homoskedasticity because it does not vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "74086785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_homoskedastic(signal):\n",
    "    \"\"\"\n",
    "    Constructs a uniform gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        \n",
    "    :return\n",
    "        noisy_signal (array or ndarray): The signal after we have added noise to it\n",
    "        signal_to_noise_ratio (float): The signal to noise ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = signal.shape\n",
    "    \n",
    "    # Calculate alpha and beta\n",
    "    alpha = 0.5\n",
    "    beta = 0\n",
    "    sigma = alpha*abs(signal - np.roll(signal, 5)).max()\n",
    "    \n",
    "    # Sample from the Gaussian\n",
    "    s = np.random.normal(mu, sigma, 1000)\n",
    "    \n",
    "    # Create new signal\n",
    "    signal_new = np.copy(signal)\n",
    "        \n",
    "    # Add noise to it\n",
    "    for i in range(len(signal)):\n",
    "        signal_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        \n",
    "    return np.array(x_new).reshape(original_shape), signal_to_noise_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819be01",
   "metadata": {},
   "source": [
    "## Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99319416",
   "metadata": {},
   "source": [
    "The Gaussian probability density function is of the following form:\n",
    "\\begin{equation}\\label{eq:}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4198cb0",
   "metadata": {},
   "source": [
    "### Estimating $\\mu$ and $\\sigma$ of the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330a231",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential with Homoskedasticity Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4116e",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with homoskedasticity approach constructs a Gaussian distribution such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_i - S_{i+n}|)$, where $max(|S_i - S_{i+n}|)$ denotes the maximum absolute difference of the signal between index $i$ and $i+n$ in the entire signal, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "In conclusion, we randomly sample from the following probability density function:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\alpha \\cdot max(|S_i - S_{i+n}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_i - S_{i+n}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "This noise exhbits homoskedasticity because it does not vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e2d44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_homoskedastic(signal_name, signal):\n",
    "    \"\"\"\n",
    "    Constructs a homoskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal_name (string): The name of the signal (i.e., ECG)\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    x_new = None\n",
    "    \n",
    "    if signal_name == 'ACC':\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        # Noise X Axis\n",
    "        x_axis = signal[:,0]\n",
    "        sigma = alpha*abs(x_axis - np.roll(x_axis, 5)).max()\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        x_axis_new = np.copy(x_axis)\n",
    "        for i in range(len(x_axis_new)):\n",
    "            x_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Y Axis\n",
    "        y_axis = signal[:,1]\n",
    "        sigma = alpha*abs(y_axis - np.roll(y_axis, 5)).max()\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        y_axis_new = np.copy(y_axis)\n",
    "        for i in range(len(y_axis_new)):\n",
    "            y_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        # Noise Z Axis\n",
    "        z_axis = signal[:,2]\n",
    "        sigma = alpha*abs(z_axis - np.roll(z_axis, 5)).max()\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "        z_axis_new = np.copy(z_axis)\n",
    "        for i in range(len(z_axis_new)):\n",
    "            z_axis_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "        \n",
    "        # Put together noisy signal\n",
    "        x_new = np.zeros((len(signal), 3))\n",
    "        x_new[:,0] = x_axis_new\n",
    "        x_new[:,1] = y_axis_new\n",
    "        x_new[:,2] = z_axis_new\n",
    "        \n",
    "        return (x_new, signal_to_noise_ratio(signal=signal, noise_type='Homoskedastic', noise_dist='Gaussian', \n",
    "                                             sigma=sigma))\n",
    "    else:\n",
    "        # Store original shape\n",
    "        original_shape = signal.shape\n",
    "\n",
    "        # Caveat: some signals like ACC have three axes\n",
    "        # Flatten signal to be 1d\n",
    "        x = np.ravel(signal)\n",
    "\n",
    "        # Calculate mean and Standard deviation\n",
    "        alpha = 0.5\n",
    "        mu = 0\n",
    "        sigma = alpha*abs(x - np.roll(x, 5)).max()\n",
    "\n",
    "        # Sample from the Gaussian\n",
    "        s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "        x_new = np.copy(x)\n",
    "\n",
    "        for i in range(len(x_new)):\n",
    "            x_new[i] += float(np.random.normal(mu, sigma, 1))\n",
    "\n",
    "        return (np.array(x_new).reshape(original_shape), signal_to_noise_ratio(signal=x, noise_type='Homoskedastic', \n",
    "                                            noise_dist='Gaussian', sigma=sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17482202",
   "metadata": {},
   "source": [
    "#### Greatest $n$-Differential Approach with Heteroskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87306268",
   "metadata": {},
   "source": [
    "For a signal $S$, the greatest $n$-differential with heteroskedasticity approach constructs a Gaussian probability density function such that $\\mu$ = 0 and $\\sigma = \\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)$, where $max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)$ denotes the maximum absolute difference of the signal between any index $i$ and $i+n$ in the last $100$ samples from index $t$, and $\\alpha$ is a parameter that multiplicatively scales the intensity of the added noise. We can choose to set $n$ to any value, although we have empirically found $n = 5$ to be the best. We set $\\mu$ to $0$ so we don't vertically shift the original signal after adding noise. \n",
    "\n",
    "At each fixed time value $t \\in [S_{start}, S_{end}]$, we randomly sample from the following probability density function:\n",
    "$$\n",
    "\\epsilon_t = f(x, t) = \\frac{1}{\\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|) \\sqrt{2 \\pi}}exp\\left(-\\frac{1}{2}\\left(\\frac{x}{\\alpha \\cdot max(|S_{t-100\\thinspace\\leq\\thinspace i \\thinspace\\leq\\thinspace t} - S_{t-100\\thinspace\\leq\\thinspace i+n \\thinspace\\leq\\thinspace t}|)}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "We add each $\\epsilon_t$ to its respective value of $S_t$ to add noise to the signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3f2cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_heteroskedastic(signal):\n",
    "    \"\"\"\n",
    "    Constructs a heteroskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = signal.shape\n",
    "    \n",
    "    # Caveat: some signals like ACC have three axes\n",
    "    # Flatten signal to be 1d\n",
    "    x = np.ravel(signal)\n",
    "    \n",
    "    # Calculate mean and Standard deviation\n",
    "    alpha = 0.5\n",
    "    mu = 0\n",
    "    sigma = None\n",
    "    \n",
    "    # Add heteroskedastic noise\n",
    "    x_new = np.copy(x)\n",
    "    noise = np.array([])\n",
    "    window_len = 100\n",
    "    for i in range(100, len(x)):\n",
    "        x_rolling = x[i-100:i]\n",
    "        sigma = alpha*abs(x_rolling - np.roll(x_rolling, 5)).max()\n",
    "        noise_i = float(np.random.normal(mu, sigma, 1))\n",
    "        np.append(noise, noise_i)\n",
    "        x_new[i] += noise_i\n",
    "    for i in range(len(x)-100, len(x)):\n",
    "        x_new[len(x)-i] += float(np.random.normal(mu, sigma, 1))  \n",
    "\n",
    "    return (np.array(x_new).reshape(original_shape), signal_to_noise_ratio(signal=x, noise=noise, noise_type='Heteroskedastic', \n",
    "                                        noise_dist='Gaussian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d643f1",
   "metadata": {},
   "source": [
    "## Exponential Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db46431",
   "metadata": {},
   "source": [
    "The exponential probability density function is of the following form:\n",
    "$$\n",
    "f(x) =\n",
    "\\left \\{\n",
    "    \\begin{array}{lr}\n",
    "        \\lambda e^{-\\lambda x}, & \\text{if } x \\geq 0 \\\\\n",
    "        0, & \\text{otherwise }\n",
    "    \\end{array}\n",
    "\\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211f660",
   "metadata": {},
   "source": [
    "### Estimating $\\lambda$ of the Exponential Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20018d54",
   "metadata": {},
   "source": [
    "Note that the expected value of an exponential probability density function $E[X] = \\int_{0}^{\\infty} x(\\lambda e^{-\\lambda x})dx = \\frac{1}{\\lambda}$, meaning whatever our estimate of $\\lambda$ is, it will be inversely proportional to the expected value of this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe612ffe",
   "metadata": {},
   "source": [
    "## Frequency Domain Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f99a38",
   "metadata": {},
   "source": [
    "A sinusoidal wave is of the form:\n",
    "\n",
    "\\begin{equation}\\label{eq:1}\n",
    "f(x) = Asin(2 \\pi T*x) + y\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is the amplitude of the wave, $T = \\frac{1}{f}$ is the period, and $y$ is the vertical shift. Note that we have omitted the phase shift parameter since we will overlay the noise to the data at pre-determined locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "469b832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a sinusoidal wave\n",
    "def sinusoidal_wave(amplitude, frequency, vertical_shift, x):\n",
    "    return amplitude * np.sin(2*np.pi*(1/frequency)*x) + vertical_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bbf9a",
   "metadata": {},
   "source": [
    "Each physiological signal has a different sampling frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700, 'ECG': 700}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658acac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(signal, hz):\n",
    "    \"\"\"\n",
    "    Constructs a heteroskedastic gaussian probability density function, samples noise from it,\n",
    "    then adds noise to the signal\n",
    "    \n",
    "    :param:\n",
    "        signal (array or ndarray): The signal we wish to add noise to\n",
    "        hz (float): The hertz (frequency) at which we wish to add noise)\n",
    "        \n",
    "    :return\n",
    "        noisy_signal: The signal after we have added noise to it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = signal.shape\n",
    "    \n",
    "    # Caveat: some signals like ACC have three axes\n",
    "    # Flatten signal to be 1d\n",
    "    x = np.ravel(signal)\n",
    "    \n",
    "    # Caveat: each signal has a different base sampling frequency\n",
    "    # Add frequency noise\n",
    "    sinusoidal_wave(amplitude, 1/hz, vertical_shift, x)\n",
    "    x_new = np.copy(x)\n",
    "\n",
    "    return np.array(x_new).reshape(original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71592233",
   "metadata": {},
   "source": [
    "### Estimating $A$, $T$ and $y$ of the Sinusoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eadcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28f5e839",
   "metadata": {},
   "source": [
    "## Exporting Noisy Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d546927",
   "metadata": {},
   "source": [
    "### Function to Write File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "806d8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path, file_name, data):\n",
    "    \"\"\"\n",
    "    Function: Writes filename to its specified path and then dumps data in .pkl format to that file.\n",
    "    \n",
    "    :param:\n",
    "        path (string): Where you want to write (create) the file\n",
    "        file_name (string): What you want to name the file you're creating\n",
    "        data (array or ndarray or pd.DataFrame): What you want inside the file you're creating\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filename = file_name + '.pkl'\n",
    "    with open(os.path.join(path, filename), 'wb') as dest:\n",
    "        pickle.dump(data, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50265a",
   "metadata": {},
   "source": [
    "Root Directory (you will likely need to change, unless you are Sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "66245ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/samuelschapiro/Desktop/Research/HCDL/data/WESAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783e4a1",
   "metadata": {},
   "source": [
    "### Function to Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5fc5dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(rootdir, body_parts=['wrist', 'chest'], signals={'wrist': ['ACC', 'BVP', 'EDA', 'TEMP'],\n",
    "                                                            'chest': ['ECG', 'Temp', 'EMG', 'Resp', 'ACC']}):\n",
    "    \"\"\"\n",
    "    Function: Adds noise to the WESAD data, stored in the specified root directory\n",
    "    \n",
    "    :param:\n",
    "        rootdir (string): The root directory from which to read the WESAD data.\n",
    "        body_parts (list): [Default: '[wrist', 'chest']] Body parts from which to read the WESAD data\n",
    "        signals (dict): [Default: see above] The physiological signals each body part has\n",
    "    :return\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    patients = []\n",
    "    patients_with_noise1 = []\n",
    "    patient_idx = 0\n",
    "    # Iterate through each patient's folder and construct a df with all patient data\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # If this is a .pkl file then it's the synchronized features/labels\n",
    "            # and we want to serialize the file\n",
    "            if '.pkl' in file and 'noise' not in file: \n",
    "                # Serialize\n",
    "                patients.append(pd.read_pickle(subdir + '/' + file))\n",
    "                patients_with_noise1.append(pd.read_pickle(subdir + '/' + file))\n",
    "                for body_part in body_parts:\n",
    "                    # Add noise\n",
    "                    for sgl in signals[body_part]:\n",
    "                        # Get signal\n",
    "                        signal = patients[patient_idx]['signal'][body_part][sgl]\n",
    "\n",
    "                        # Get gaussian homoskedastic noise\n",
    "                        x_gaussian_homoskedastic, snr_gaussian_homoskedastic = gaussian_homoskedastic(sgl, signal)\n",
    "                        patients_with_noise1[patient_idx]['signal'][body_part][sgl] = x_gaussian_homoskedastic\n",
    "\n",
    "                # Export each noise type\n",
    "                write_file(subdir, file.split('.')[0]+'_gaussian_homoskedastic_noise', \n",
    "                           patients_with_noise1[patient_idx])\n",
    "                # Increment patient index\n",
    "                patient_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0d104727",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise(rootdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3f98b",
   "metadata": {},
   "source": [
    "# Feature Extraction (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab017de",
   "metadata": {},
   "source": [
    "Do feature extraction again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bad13a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S5_gaussian_homoskedastic_noise.pkl\n",
      "S2_gaussian_homoskedastic_noise.pkl\n",
      "S3_gaussian_homoskedastic_noise.pkl\n",
      "S4_gaussian_homoskedastic_noise.pkl\n",
      "S17_gaussian_homoskedastic_noise.pkl\n",
      "S10_gaussian_homoskedastic_noise.pkl\n",
      "S11_gaussian_homoskedastic_noise.pkl\n",
      "S16_gaussian_homoskedastic_noise.pkl\n",
      "S8_gaussian_homoskedastic_noise.pkl\n",
      "S6_gaussian_homoskedastic_noise.pkl\n",
      "S7_gaussian_homoskedastic_noise.pkl\n",
      "S9_gaussian_homoskedastic_noise.pkl\n",
      "S13_gaussian_homoskedastic_noise.pkl\n",
      "S14_gaussian_homoskedastic_noise.pkl\n",
      "S15_gaussian_homoskedastic_noise.pkl\n"
     ]
    }
   ],
   "source": [
    "patients_new = []\n",
    "# Iterate through each patient's folder and construct a\n",
    "# df with all patient data\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        # If this is a .pkl file then it's the synchronized features/labels\n",
    "        # and we want to serialize the file\n",
    "        if '.pkl' in file and 'noise' in file:\n",
    "            print(file)\n",
    "            patients_new.append(pd.read_pickle(subdir + '/' + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9e3ed2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14e4a5d80>]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.ravel(patients[0]['signal']['chest']['ECG'])[15000:16000]\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022334",
   "metadata": {},
   "source": [
    "# Data Preparation (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce48d1c",
   "metadata": {},
   "source": [
    "Prepare the data again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb3726",
   "metadata": {},
   "source": [
    "# Modeling (pt. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99893b",
   "metadata": {},
   "source": [
    "Model again, this time with the noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973d694",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4551",
   "metadata": {},
   "source": [
    "Compare the results of the noisy data models and the clean data models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb53c5",
   "metadata": {},
   "source": [
    "- Plots of SNR (x-axis) vs. accuracy (y-axis)\n",
    "- Compare feature importances across different noise regimes\n",
    "    - Develop dynamic evaluation method based on original feature importance / added noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
